{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "\n",
    "from parser_RRL import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "batch_size = 64\n",
    "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False, file='data_RRL.csv')\n",
    "\n",
    "SUMMARIES_DIR = './summaries/convnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape, layer_name):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
    "                             initializer=tf.constant_initializer(0.05))\n",
    "    \n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    \n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, \n",
    "                       strides = [1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape, layer_name):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_input = tf.placeholder(tf.float32, name='model_input', \n",
    "                             shape=(batch_size,1,200,1))\n",
    "tf.summary.image('input', model_input, 10)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob', shape=())\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target', shape=(batch_size, 4))\n",
    "\n",
    "# First convolution layer\n",
    "layer_name = 'conv1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv1_out = conv_layer(model_input, [1, 3, 1, 20], layer_name)\n",
    "# First pooling layer\n",
    "with tf.name_scope('pool1'):\n",
    "    pool1_out = tf.nn.avg_pool(conv1_out, ksize=[1, 2, 2, 1],\n",
    "                strides=[1, 2, 2, 1], padding='SAME',\n",
    "                name='pool1')\n",
    "    \n",
    "\n",
    "# Second convolution layer\n",
    "layer_name = 'conv2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv2_out = conv_layer(pool1_out, [1, 3, 20, 20], layer_name)\n",
    "# Second pooling layer\n",
    "with tf.name_scope('pool2'):\n",
    "    pool2_out = tf.nn.avg_pool(conv2_out, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME',\n",
    "                            name='pool2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,dim3,dim4 = pool2_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2_out_flat = tf.reshape(pool2_out, [-1, 1*dim3.value*dim4.value], name='pool2_flat')\n",
    "# First fully connected layer\n",
    "layer_name = 'fc1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc1_out = fc_layer(pool2_out_flat, [1*dim3.value*dim4.value, 500], layer_name)\n",
    "fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "\n",
    "# Second fully connected layer\n",
    "layer_name = 'fc2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc2_out = fc_layer(fc1_out_drop, [500, 250], layer_name)\n",
    "fc2_out_drop = tf.nn.dropout(fc2_out, keep_prob)\n",
    "\n",
    "\n",
    "# Third fully connected layer\n",
    "layer_name = 'fc3'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc3_out = fc_layer(fc2_out_drop, [250,4], layer_name)\n",
    "model_output = fc3_out\n",
    "\n",
    "\n",
    "with tf.name_scope('loss_function'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n",
    "                                           name='cross_entropy'))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    grads_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(model_output, 1),\n",
    "                             tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Useful training functions\n",
    "def validate():\n",
    "    batches = cifar10.getValidationSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    summary = sess.run(merged,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "    return summary, mean_acc\n",
    "def test():\n",
    "    batches = cifar10.getTestSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable variables\n",
      "conv1/weights:0\n",
      "conv1/biases:0\n",
      "conv2/weights:0\n",
      "conv2/biases:0\n",
      "fc1/weights:0\n",
      "fc1/biases:0\n",
      "fc2/weights:0\n",
      "fc2/biases:0\n",
      "fc3/weights:0\n",
      "fc3/biases:0\n",
      "Epoch 0, training loss 1.277323, accuracy 0.437500\n",
      "Validation accuracy 0.459821\n",
      "Time elapsed 0.0029333074887593585 minutes\n",
      "Epoch 1, training loss 1.012498, accuracy 0.484375\n",
      "Validation accuracy 0.495536\n",
      "Time elapsed 0.01882139444351196 minutes\n",
      "Epoch 2, training loss 0.972573, accuracy 0.500000\n",
      "Validation accuracy 0.533482\n",
      "Time elapsed 0.03455427090326945 minutes\n",
      "Epoch 3, training loss 0.885118, accuracy 0.593750\n",
      "Validation accuracy 0.495536\n",
      "Time elapsed 0.05069129069646199 minutes\n",
      "Epoch 4, training loss 0.871374, accuracy 0.625000\n",
      "Validation accuracy 0.495536\n",
      "Time elapsed 0.06669963995615641 minutes\n",
      "Epoch 5, training loss 0.934203, accuracy 0.531250\n",
      "Validation accuracy 0.495536\n",
      "Time elapsed 0.08350015878677368 minutes\n",
      "Epoch 6, training loss 1.047979, accuracy 0.406250\n",
      "Validation accuracy 0.533482\n",
      "Time elapsed 0.10109916925430298 minutes\n",
      "Epoch 7, training loss 0.921524, accuracy 0.546875\n",
      "Validation accuracy 0.497768\n",
      "Time elapsed 0.11640461683273315 minutes\n",
      "Epoch 8, training loss 1.014368, accuracy 0.468750\n",
      "Validation accuracy 0.491071\n",
      "Time elapsed 0.13264143466949463 minutes\n",
      "Epoch 9, training loss 0.893786, accuracy 0.609375\n",
      "Validation accuracy 0.555804\n",
      "Time elapsed 0.14819752772649128 minutes\n",
      "Epoch 10, training loss 0.943714, accuracy 0.562500\n",
      "Validation accuracy 0.551339\n",
      "Time elapsed 0.16565463145573933 minutes\n",
      "Epoch 11, training loss 0.935147, accuracy 0.578125\n",
      "Validation accuracy 0.544643\n",
      "Time elapsed 0.18176870743433635 minutes\n",
      "Epoch 12, training loss 0.875448, accuracy 0.593750\n",
      "Validation accuracy 0.562500\n",
      "Time elapsed 0.19839574495951334 minutes\n",
      "Epoch 13, training loss 0.882695, accuracy 0.609375\n",
      "Validation accuracy 0.524554\n",
      "Time elapsed 0.21373845338821412 minutes\n",
      "Epoch 14, training loss 0.966774, accuracy 0.515625\n",
      "Validation accuracy 0.558036\n",
      "Time elapsed 0.22976452906926473 minutes\n",
      "Epoch 15, training loss 0.791248, accuracy 0.546875\n",
      "Validation accuracy 0.524554\n",
      "Time elapsed 0.24610714117685953 minutes\n",
      "Epoch 16, training loss 0.773006, accuracy 0.687500\n",
      "Validation accuracy 0.537946\n",
      "Time elapsed 0.2621090531349182 minutes\n",
      "Epoch 17, training loss 0.916004, accuracy 0.484375\n",
      "Validation accuracy 0.575893\n",
      "Time elapsed 0.27832540273666384 minutes\n",
      "Epoch 18, training loss 0.784041, accuracy 0.640625\n",
      "Validation accuracy 0.607143\n",
      "Time elapsed 0.2944148818651835 minutes\n",
      "Epoch 19, training loss 0.814429, accuracy 0.578125\n",
      "Validation accuracy 0.604911\n",
      "Time elapsed 0.3098597606023153 minutes\n",
      "Epoch 20, training loss 0.904931, accuracy 0.562500\n",
      "Validation accuracy 0.616071\n",
      "Time elapsed 0.3253398577372233 minutes\n",
      "Epoch 21, training loss 0.754151, accuracy 0.671875\n",
      "Validation accuracy 0.625000\n",
      "Time elapsed 0.34099980592727663 minutes\n",
      "Epoch 22, training loss 0.865199, accuracy 0.531250\n",
      "Validation accuracy 0.618304\n",
      "Time elapsed 0.3581185022989909 minutes\n",
      "Epoch 23, training loss 0.632038, accuracy 0.718750\n",
      "Validation accuracy 0.640625\n",
      "Time elapsed 0.3750573039054871 minutes\n",
      "Epoch 24, training loss 0.694490, accuracy 0.671875\n",
      "Validation accuracy 0.649554\n",
      "Time elapsed 0.3912535389264425 minutes\n",
      "Epoch 25, training loss 0.702922, accuracy 0.671875\n",
      "Validation accuracy 0.633929\n",
      "Time elapsed 0.407199768225352 minutes\n",
      "Epoch 26, training loss 0.738851, accuracy 0.625000\n",
      "Validation accuracy 0.638393\n",
      "Time elapsed 0.42303189436594646 minutes\n",
      "Epoch 27, training loss 0.726418, accuracy 0.718750\n",
      "Validation accuracy 0.647321\n",
      "Time elapsed 0.43868232568105064 minutes\n",
      "Epoch 28, training loss 0.721010, accuracy 0.625000\n",
      "Validation accuracy 0.640625\n",
      "Time elapsed 0.4546200195948283 minutes\n",
      "Epoch 29, training loss 0.634662, accuracy 0.718750\n",
      "Validation accuracy 0.642857\n",
      "Time elapsed 0.47034368515014646 minutes\n",
      "Epoch 30, training loss 0.668601, accuracy 0.687500\n",
      "Validation accuracy 0.645089\n",
      "Time elapsed 0.4868329405784607 minutes\n",
      "Epoch 31, training loss 0.706685, accuracy 0.671875\n",
      "Validation accuracy 0.645089\n",
      "Time elapsed 0.5041471282641093 minutes\n",
      "Epoch 32, training loss 0.623178, accuracy 0.765625\n",
      "Validation accuracy 0.656250\n",
      "Time elapsed 0.5203863541285197 minutes\n",
      "Epoch 33, training loss 0.623141, accuracy 0.703125\n",
      "Validation accuracy 0.696429\n",
      "Time elapsed 0.537631114323934 minutes\n",
      "Epoch 34, training loss 0.595735, accuracy 0.703125\n",
      "Validation accuracy 0.678571\n",
      "Time elapsed 0.5536840240160624 minutes\n",
      "Epoch 35, training loss 0.657763, accuracy 0.718750\n",
      "Validation accuracy 0.680804\n",
      "Time elapsed 0.5692200223604839 minutes\n",
      "Epoch 36, training loss 0.631746, accuracy 0.671875\n",
      "Validation accuracy 0.689732\n",
      "Time elapsed 0.5853840072949728 minutes\n",
      "Epoch 37, training loss 0.737941, accuracy 0.703125\n",
      "Validation accuracy 0.696429\n",
      "Time elapsed 0.6016199549039205 minutes\n",
      "Epoch 38, training loss 0.532910, accuracy 0.812500\n",
      "Validation accuracy 0.696429\n",
      "Time elapsed 0.6175854841868083 minutes\n",
      "Epoch 39, training loss 0.563523, accuracy 0.765625\n",
      "Validation accuracy 0.689732\n",
      "Time elapsed 0.6339666803677877 minutes\n",
      "Epoch 40, training loss 0.560671, accuracy 0.765625\n",
      "Validation accuracy 0.683036\n",
      "Time elapsed 0.6498721400896709 minutes\n",
      "Epoch 41, training loss 0.673993, accuracy 0.625000\n",
      "Validation accuracy 0.696429\n",
      "Time elapsed 0.6658930619557698 minutes\n",
      "Epoch 42, training loss 0.534462, accuracy 0.734375\n",
      "Validation accuracy 0.696429\n",
      "Time elapsed 0.6824611822764078 minutes\n",
      "Epoch 43, training loss 0.639942, accuracy 0.718750\n",
      "Validation accuracy 0.707589\n",
      "Time elapsed 0.6992303450902303 minutes\n",
      "Epoch 44, training loss 0.613004, accuracy 0.750000\n",
      "Validation accuracy 0.723214\n",
      "Time elapsed 0.7148132244745891 minutes\n",
      "Epoch 45, training loss 0.504626, accuracy 0.812500\n",
      "Validation accuracy 0.712054\n",
      "Time elapsed 0.7314247488975525 minutes\n",
      "Epoch 46, training loss 0.569252, accuracy 0.718750\n",
      "Validation accuracy 0.703125\n",
      "Time elapsed 0.7477698723475138 minutes\n",
      "Epoch 47, training loss 0.703452, accuracy 0.671875\n",
      "Validation accuracy 0.696429\n",
      "Time elapsed 0.7650140881538391 minutes\n",
      "Epoch 48, training loss 0.606696, accuracy 0.656250\n",
      "Validation accuracy 0.707589\n",
      "Time elapsed 0.7807255029678345 minutes\n",
      "Epoch 49, training loss 0.564477, accuracy 0.750000\n",
      "Validation accuracy 0.727679\n",
      "Time elapsed 0.7965684096018474 minutes\n",
      "Epoch 50, training loss 0.568572, accuracy 0.750000\n",
      "Validation accuracy 0.707589\n",
      "Time elapsed 0.8120560010274251 minutes\n",
      "Epoch 51, training loss 0.543336, accuracy 0.765625\n",
      "Validation accuracy 0.709821\n",
      "Time elapsed 0.8277671416600545 minutes\n",
      "Epoch 52, training loss 0.495196, accuracy 0.765625\n",
      "Validation accuracy 0.712054\n",
      "Time elapsed 0.8435490886370342 minutes\n",
      "Epoch 53, training loss 0.597477, accuracy 0.750000\n",
      "Validation accuracy 0.718750\n",
      "Time elapsed 0.8599897265434265 minutes\n",
      "Epoch 54, training loss 0.549554, accuracy 0.796875\n",
      "Validation accuracy 0.741071\n",
      "Time elapsed 0.8757986942927043 minutes\n",
      "Epoch 55, training loss 0.652144, accuracy 0.671875\n",
      "Validation accuracy 0.700893\n",
      "Time elapsed 0.8920225143432617 minutes\n",
      "Epoch 56, training loss 0.391644, accuracy 0.843750\n",
      "Validation accuracy 0.703125\n",
      "Time elapsed 0.908541735013326 minutes\n",
      "Epoch 57, training loss 0.398207, accuracy 0.843750\n",
      "Validation accuracy 0.694196\n",
      "Time elapsed 0.9241682012875875 minutes\n",
      "Epoch 58, training loss 0.504931, accuracy 0.718750\n",
      "Validation accuracy 0.718750\n",
      "Time elapsed 0.9414807240168254 minutes\n",
      "Epoch 59, training loss 0.608563, accuracy 0.734375\n",
      "Validation accuracy 0.705357\n",
      "Time elapsed 0.9586092154184978 minutes\n",
      "Epoch 60, training loss 0.493304, accuracy 0.765625\n",
      "Validation accuracy 0.725446\n",
      "Time elapsed 0.9743389089902242 minutes\n",
      "Epoch 61, training loss 0.522745, accuracy 0.796875\n",
      "Validation accuracy 0.725446\n",
      "Time elapsed 0.9896724462509155 minutes\n",
      "Epoch 62, training loss 0.731754, accuracy 0.625000\n",
      "Validation accuracy 0.707589\n",
      "Time elapsed 1.0064358234405517 minutes\n",
      "Epoch 63, training loss 0.512783, accuracy 0.734375\n",
      "Validation accuracy 0.718750\n",
      "Time elapsed 1.0231818914413453 minutes\n",
      "Epoch 64, training loss 0.503055, accuracy 0.812500\n",
      "Validation accuracy 0.723214\n",
      "Time elapsed 1.0396007855733236 minutes\n",
      "Epoch 65, training loss 0.568279, accuracy 0.812500\n",
      "Validation accuracy 0.714286\n",
      "Time elapsed 1.056430172920227 minutes\n",
      "Epoch 66, training loss 0.526597, accuracy 0.734375\n",
      "Validation accuracy 0.727679\n",
      "Time elapsed 1.0738972425460815 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, training loss 0.517029, accuracy 0.765625\n",
      "Validation accuracy 0.716518\n",
      "Time elapsed 1.09098774989446 minutes\n",
      "Epoch 68, training loss 0.521254, accuracy 0.765625\n",
      "Validation accuracy 0.700893\n",
      "Time elapsed 1.1070779760678608 minutes\n",
      "Epoch 69, training loss 0.489925, accuracy 0.750000\n",
      "Validation accuracy 0.736607\n",
      "Time elapsed 1.1239771684010824 minutes\n",
      "Epoch 70, training loss 0.476589, accuracy 0.781250\n",
      "Validation accuracy 0.712054\n",
      "Time elapsed 1.1405113220214844 minutes\n",
      "Epoch 71, training loss 0.363987, accuracy 0.828125\n",
      "Validation accuracy 0.729911\n",
      "Time elapsed 1.158814255396525 minutes\n",
      "Epoch 72, training loss 0.582809, accuracy 0.687500\n",
      "Validation accuracy 0.696429\n",
      "Time elapsed 1.1761221011479697 minutes\n",
      "Epoch 73, training loss 0.638729, accuracy 0.671875\n",
      "Validation accuracy 0.709821\n",
      "Time elapsed 1.1931470473607382 minutes\n",
      "Epoch 74, training loss 0.541676, accuracy 0.750000\n",
      "Validation accuracy 0.707589\n",
      "Time elapsed 1.2112495342890421 minutes\n",
      "Epoch 75, training loss 0.527653, accuracy 0.750000\n",
      "Validation accuracy 0.723214\n",
      "Time elapsed 1.2274634122848511 minutes\n",
      "Epoch 76, training loss 0.531838, accuracy 0.765625\n",
      "Validation accuracy 0.738839\n",
      "Time elapsed 1.2443300525347392 minutes\n",
      "Epoch 77, training loss 0.466554, accuracy 0.812500\n",
      "Validation accuracy 0.712054\n",
      "Time elapsed 1.2610103527704875 minutes\n",
      "Epoch 78, training loss 0.535692, accuracy 0.750000\n",
      "Validation accuracy 0.716518\n",
      "Time elapsed 1.2772498687108358 minutes\n",
      "Epoch 79, training loss 0.624758, accuracy 0.734375\n",
      "Validation accuracy 0.718750\n",
      "Time elapsed 1.294166084130605 minutes\n",
      "Epoch 80, training loss 0.529044, accuracy 0.750000\n",
      "Validation accuracy 0.725446\n",
      "Time elapsed 1.311772084236145 minutes\n",
      "Epoch 81, training loss 0.464562, accuracy 0.843750\n",
      "Validation accuracy 0.718750\n",
      "Time elapsed 1.3276050567626954 minutes\n",
      "Epoch 82, training loss 0.499660, accuracy 0.843750\n",
      "Validation accuracy 0.718750\n",
      "Time elapsed 1.343893547852834 minutes\n",
      "Epoch 83, training loss 0.495904, accuracy 0.765625\n",
      "Validation accuracy 0.725446\n",
      "Time elapsed 1.3598196307818096 minutes\n",
      "Epoch 84, training loss 0.400984, accuracy 0.843750\n",
      "Validation accuracy 0.705357\n",
      "Time elapsed 1.376557958126068 minutes\n",
      "Epoch 85, training loss 0.435278, accuracy 0.843750\n",
      "Validation accuracy 0.714286\n",
      "Time elapsed 1.3922178983688354 minutes\n",
      "Epoch 86, training loss 0.691521, accuracy 0.718750\n",
      "Validation accuracy 0.734375\n",
      "Time elapsed 1.40880495707194 minutes\n",
      "Epoch 87, training loss 0.408355, accuracy 0.828125\n",
      "Validation accuracy 0.732143\n",
      "Time elapsed 1.4256284435590107 minutes\n",
      "Epoch 88, training loss 0.422898, accuracy 0.843750\n",
      "Validation accuracy 0.736607\n",
      "Time elapsed 1.4418227791786193 minutes\n",
      "Epoch 89, training loss 0.552812, accuracy 0.703125\n",
      "Validation accuracy 0.725446\n",
      "Time elapsed 1.459071401755015 minutes\n",
      "Epoch 90, training loss 0.448128, accuracy 0.812500\n",
      "Validation accuracy 0.714286\n",
      "Time elapsed 1.475434386730194 minutes\n",
      "Epoch 91, training loss 0.480091, accuracy 0.781250\n",
      "Validation accuracy 0.716518\n",
      "Time elapsed 1.4913727482159933 minutes\n",
      "Epoch 92, training loss 0.517473, accuracy 0.734375\n",
      "Validation accuracy 0.716518\n",
      "Time elapsed 1.507404347260793 minutes\n",
      "Epoch 93, training loss 0.567651, accuracy 0.734375\n",
      "Validation accuracy 0.714286\n",
      "Time elapsed 1.5236914038658143 minutes\n",
      "Epoch 94, training loss 0.463586, accuracy 0.859375\n",
      "Validation accuracy 0.723214\n",
      "Time elapsed 1.5400750438372295 minutes\n",
      "Epoch 95, training loss 0.613252, accuracy 0.718750\n",
      "Validation accuracy 0.729911\n",
      "Time elapsed 1.5587591568628947 minutes\n",
      "Epoch 96, training loss 0.460059, accuracy 0.828125\n",
      "Validation accuracy 0.691964\n",
      "Time elapsed 1.5755802472432454 minutes\n",
      "Epoch 97, training loss 0.501768, accuracy 0.796875\n",
      "Validation accuracy 0.720982\n",
      "Time elapsed 1.5928651173909505 minutes\n",
      "Epoch 98, training loss 0.557384, accuracy 0.765625\n",
      "Validation accuracy 0.707589\n",
      "Time elapsed 1.6097376863161723 minutes\n",
      "Epoch 99, training loss 0.489349, accuracy 0.765625\n",
      "Validation accuracy 0.716518\n",
      "Time elapsed 1.6253733038902283 minutes\n",
      "Epoch 100, training loss 0.475498, accuracy 0.781250\n",
      "Validation accuracy 0.723214\n",
      "Time elapsed 1.6416671633720399 minutes\n",
      "Epoch 101, training loss 0.496139, accuracy 0.765625\n",
      "Validation accuracy 0.723214\n",
      "Time elapsed 1.6577647805213929 minutes\n",
      "Epoch 102, training loss 0.590078, accuracy 0.718750\n",
      "Validation accuracy 0.712054\n",
      "Time elapsed 1.673918080329895 minutes\n",
      "Epoch 103, training loss 0.594898, accuracy 0.687500\n",
      "Validation accuracy 0.741071\n",
      "Time elapsed 1.6917059858640036 minutes\n",
      "Epoch 104, training loss 0.599409, accuracy 0.750000\n",
      "Validation accuracy 0.732143\n",
      "Time elapsed 1.7089075048764546 minutes\n",
      "Epoch 105, training loss 0.424533, accuracy 0.750000\n",
      "Validation accuracy 0.720982\n",
      "Time elapsed 1.7254904667536417 minutes\n",
      "Epoch 106, training loss 0.548112, accuracy 0.765625\n",
      "Validation accuracy 0.723214\n",
      "Time elapsed 1.7430800596872966 minutes\n",
      "Epoch 107, training loss 0.395187, accuracy 0.859375\n",
      "Validation accuracy 0.709821\n",
      "Time elapsed 1.7585213383038838 minutes\n",
      "Epoch 108, training loss 0.469788, accuracy 0.750000\n",
      "Validation accuracy 0.720982\n",
      "Time elapsed 1.7740113695462545 minutes\n",
      "Epoch 109, training loss 0.472651, accuracy 0.765625\n",
      "Validation accuracy 0.720982\n",
      "Time elapsed 1.7902358373006184 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/train',\n",
    "                                     sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/validation')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "cifar10.reset()\n",
    "print(\"Trainable variables\")\n",
    "for n in tf.trainable_variables():\n",
    "    print(n.name)\n",
    "\n",
    "epochs = 200\n",
    "mean_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "std_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "\n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "while cifar10.getEpoch() < epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    # just a training iteration\n",
    "    _ = sess.run(train_step,\n",
    "                feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "    \n",
    "    step = batch_idx+epoch*n_batches\n",
    "    \n",
    "    # Write training summary\n",
    "    if step%50==0:\n",
    "        summary = sess.run(merged,\n",
    "                          feed_dict={\n",
    "                model_input: batch_data,\n",
    "                target: batch_labels,\n",
    "                keep_prob: 0.5 # set to 1.0 at inference time\n",
    "            })\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "    # gradient (by layer) statistics over last training batch & validation summary\n",
    "    if batch_idx==0:\n",
    "        loss, acc, grads = sess.run((cross_entropy, accuracy, grads_vars), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "        for layer in range(len(tf.trainable_variables())):\n",
    "            mean_gradients[layer, epoch] = np.mean(np.abs(grads[layer][0]))\n",
    "            std_gradients[layer, epoch] = np.std(np.abs(grads[layer][0]))\n",
    "        print(\"Epoch %d, training loss %f, accuracy %f\" % (epoch, loss, acc))\n",
    "        \n",
    "        summary, validation_accuracy = validate()\n",
    "        validation_writer.add_summary(summary, step)\n",
    "        print(\"Validation accuracy %f\" % validation_accuracy)\n",
    "        print(\"Time elapsed\", (time.time()-t_i)/60.0, \"minutes\")\n",
    "train_writer.flush()\n",
    "validation_writer.flush()\n",
    "test_acc = test()\n",
    "print(\"Testing set accuracy %f\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting gradients\n",
    "n_layers = len(tf.trainable_variables()) // 2\n",
    "x = np.arange(epochs)\n",
    "i = 0\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Weights Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()\n",
    "i = 1\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Biases Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(targets, outputs):\n",
    "    '''Returns a confusion matrix. Both targets and outputs\n",
    "    should be 1-D arrays of zeros and ones.'''\n",
    "    encoded_data = 2*targets+outputs  # Map targets and outputs to {0, 1, 2, 3}\n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return ((TP, FP), (FN, TN))\n",
    "\n",
    "def roc_curve(targets, outputs):\n",
    "    '''Returns a ROC curve. Outputs should be in range 0-1\n",
    "    in order to move the threshold.'''\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        outputs_with_threshold = (outputs > threshold).astype(np.float)\n",
    "        ((TP, FP), (FN, TN)) = confusion_matrix(\n",
    "            targets, \n",
    "            outputs_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "        fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "accs = sess.run(accuracy,\n",
    "             feed_dict={\n",
    "            model_input: cifar10.training_data,\n",
    "                       target: cifar10.training_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "accs = np.asarray(accs)\n",
    "[[VN,FP],[FN,VP]] = confusion_matrix(\n",
    "    accs,\n",
    "    (accs>0.5).astype(np.float))\n",
    "print('VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN))\n",
    "print('Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN)))\n",
    "print('Precision: %%%f' %(100.0*VP/(VP+FP)))\n",
    "print('Recall: %%%f' %(100.0*VP/(VP+FN)))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
