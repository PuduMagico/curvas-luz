{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "\n",
    "from cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "batch_size = 64\n",
    "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False, file='data_original.csv')\n",
    "\n",
    "SUMMARIES_DIR = './summaries/convnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape, layer_name):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
    "                             initializer=tf.constant_initializer(0.05))\n",
    "    \n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    \n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, \n",
    "                       strides = [1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape, layer_name):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_input = tf.placeholder(tf.float32, name='model_input', \n",
    "                             shape=(batch_size,1,200,1))\n",
    "tf.summary.image('input', model_input, 10)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob', shape=())\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target', shape=(batch_size, 9))\n",
    "\n",
    "# First convolution layer\n",
    "layer_name = 'conv1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv1_out = conv_layer(model_input, [1, 3, 1, 20], layer_name)\n",
    "# First pooling layer\n",
    "with tf.name_scope('pool1'):\n",
    "    pool1_out = tf.nn.avg_pool(conv1_out, ksize=[1, 2, 2, 1],\n",
    "                strides=[1, 2, 2, 1], padding='SAME',\n",
    "                name='pool1')\n",
    "    \n",
    "\n",
    "# Second convolution layer\n",
    "layer_name = 'conv2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv2_out = conv_layer(pool1_out, [1, 3, 20, 20], layer_name)\n",
    "# Second pooling layer\n",
    "with tf.name_scope('pool2'):\n",
    "    pool2_out = tf.nn.avg_pool(conv2_out, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME',\n",
    "                            name='pool2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,dim3,dim4 = pool2_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2_out_flat = tf.reshape(pool2_out, [-1, 1*dim3.value*dim4.value], name='pool2_flat')\n",
    "# First fully connected layer\n",
    "layer_name = 'fc1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc1_out = fc_layer(pool2_out_flat, [1*dim3.value*dim4.value, 500], layer_name)\n",
    "fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "\n",
    "# Second fully connected layer\n",
    "layer_name = 'fc2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc2_out = fc_layer(fc1_out_drop, [500, 250], layer_name)\n",
    "fc2_out_drop = tf.nn.dropout(fc2_out, keep_prob)\n",
    "\n",
    "\n",
    "# Third fully connected layer\n",
    "layer_name = 'fc3'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc3_out = fc_layer(fc2_out_drop, [250,9], layer_name)\n",
    "model_output = fc3_out\n",
    "\n",
    "\n",
    "with tf.name_scope('loss_function'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n",
    "                                           name='cross_entropy'))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    grads_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(model_output, 1),\n",
    "                             tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Useful training functions\n",
    "def validate():\n",
    "    batches = cifar10.getValidationSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    summary = sess.run(merged,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "    return summary, mean_acc\n",
    "def test():\n",
    "    batches = cifar10.getTestSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable variables\n",
      "conv1/weights:0\n",
      "conv1/biases:0\n",
      "conv2/weights:0\n",
      "conv2/biases:0\n",
      "fc1/weights:0\n",
      "fc1/biases:0\n",
      "fc2/weights:0\n",
      "fc2/biases:0\n",
      "fc3/weights:0\n",
      "fc3/biases:0\n",
      "Epoch 0, training loss 2.019264, accuracy 0.265625\n",
      "Validation accuracy 0.176042\n",
      "Time elapsed 0.009492011864980061 minutes\n",
      "Epoch 1, training loss 1.677456, accuracy 0.250000\n",
      "Validation accuracy 0.303125\n",
      "Time elapsed 0.0545529047648112 minutes\n",
      "Epoch 2, training loss 1.717083, accuracy 0.296875\n",
      "Validation accuracy 0.329167\n",
      "Time elapsed 0.09572997490564981 minutes\n",
      "Epoch 3, training loss 1.694378, accuracy 0.296875\n",
      "Validation accuracy 0.334375\n",
      "Time elapsed 0.13125569025675457 minutes\n",
      "Epoch 4, training loss 1.733480, accuracy 0.343750\n",
      "Validation accuracy 0.382292\n",
      "Time elapsed 0.17433609962463378 minutes\n",
      "Epoch 5, training loss 1.558351, accuracy 0.468750\n",
      "Validation accuracy 0.371875\n",
      "Time elapsed 0.21065903107325237 minutes\n",
      "Epoch 6, training loss 1.629451, accuracy 0.312500\n",
      "Validation accuracy 0.382292\n",
      "Time elapsed 0.25112797419230143 minutes\n",
      "Epoch 7, training loss 1.395401, accuracy 0.484375\n",
      "Validation accuracy 0.368750\n",
      "Time elapsed 0.2938002546628316 minutes\n",
      "Epoch 8, training loss 1.413963, accuracy 0.406250\n",
      "Validation accuracy 0.398958\n",
      "Time elapsed 0.33934275309244794 minutes\n",
      "Epoch 9, training loss 1.408274, accuracy 0.453125\n",
      "Validation accuracy 0.460417\n",
      "Time elapsed 0.37986395359039304 minutes\n",
      "Epoch 10, training loss 1.356080, accuracy 0.562500\n",
      "Validation accuracy 0.451042\n",
      "Time elapsed 0.42346442540486656 minutes\n",
      "Epoch 11, training loss 1.437764, accuracy 0.390625\n",
      "Validation accuracy 0.444792\n",
      "Time elapsed 0.47026466925938926 minutes\n",
      "Epoch 12, training loss 1.326016, accuracy 0.578125\n",
      "Validation accuracy 0.471875\n",
      "Time elapsed 0.5196250677108765 minutes\n",
      "Epoch 13, training loss 1.295576, accuracy 0.437500\n",
      "Validation accuracy 0.451042\n",
      "Time elapsed 0.559041690826416 minutes\n",
      "Epoch 14, training loss 1.309890, accuracy 0.531250\n",
      "Validation accuracy 0.454167\n",
      "Time elapsed 0.5887076099713643 minutes\n",
      "Epoch 15, training loss 1.301342, accuracy 0.531250\n",
      "Validation accuracy 0.480208\n",
      "Time elapsed 0.614759091536204 minutes\n",
      "Epoch 16, training loss 1.584449, accuracy 0.359375\n",
      "Validation accuracy 0.486458\n",
      "Time elapsed 0.6396610776583354 minutes\n",
      "Epoch 17, training loss 1.341383, accuracy 0.406250\n",
      "Validation accuracy 0.479167\n",
      "Time elapsed 0.6644159356753031 minutes\n",
      "Epoch 18, training loss 1.272822, accuracy 0.500000\n",
      "Validation accuracy 0.488542\n",
      "Time elapsed 0.690743625164032 minutes\n",
      "Epoch 19, training loss 1.194100, accuracy 0.515625\n",
      "Validation accuracy 0.472917\n",
      "Time elapsed 0.7192325115203857 minutes\n",
      "Epoch 20, training loss 1.280245, accuracy 0.421875\n",
      "Validation accuracy 0.483333\n",
      "Time elapsed 0.7455784916877747 minutes\n",
      "Epoch 21, training loss 1.285245, accuracy 0.453125\n",
      "Validation accuracy 0.478125\n",
      "Time elapsed 0.7753693501154582 minutes\n",
      "Epoch 22, training loss 1.426727, accuracy 0.531250\n",
      "Validation accuracy 0.478125\n",
      "Time elapsed 0.8206837217013041 minutes\n",
      "Epoch 23, training loss 1.215400, accuracy 0.500000\n",
      "Validation accuracy 0.476042\n",
      "Time elapsed 0.8630336960156758 minutes\n",
      "Epoch 24, training loss 1.055348, accuracy 0.593750\n",
      "Validation accuracy 0.495833\n",
      "Time elapsed 0.9025850216547648 minutes\n",
      "Epoch 25, training loss 1.263995, accuracy 0.484375\n",
      "Validation accuracy 0.502083\n",
      "Time elapsed 0.9462782859802246 minutes\n",
      "Epoch 26, training loss 1.397699, accuracy 0.500000\n",
      "Validation accuracy 0.484375\n",
      "Time elapsed 0.9883289019266764 minutes\n",
      "Epoch 27, training loss 1.329564, accuracy 0.437500\n",
      "Validation accuracy 0.498958\n",
      "Time elapsed 1.027042023340861 minutes\n",
      "Epoch 28, training loss 1.121989, accuracy 0.531250\n",
      "Validation accuracy 0.505208\n",
      "Time elapsed 1.0636297663052876 minutes\n",
      "Epoch 29, training loss 1.314792, accuracy 0.500000\n",
      "Validation accuracy 0.508333\n",
      "Time elapsed 1.1024105668067932 minutes\n",
      "Epoch 30, training loss 1.190648, accuracy 0.468750\n",
      "Validation accuracy 0.496875\n",
      "Time elapsed 1.1411516745885213 minutes\n",
      "Epoch 31, training loss 1.080073, accuracy 0.515625\n",
      "Validation accuracy 0.503125\n",
      "Time elapsed 1.1780807574590046 minutes\n",
      "Epoch 32, training loss 1.168245, accuracy 0.671875\n",
      "Validation accuracy 0.516667\n",
      "Time elapsed 1.2159272988637289 minutes\n",
      "Epoch 33, training loss 0.924487, accuracy 0.687500\n",
      "Validation accuracy 0.508333\n",
      "Time elapsed 1.2530433058738708 minutes\n",
      "Epoch 34, training loss 1.034858, accuracy 0.546875\n",
      "Validation accuracy 0.526042\n",
      "Time elapsed 1.2883917371431985 minutes\n",
      "Epoch 35, training loss 1.080334, accuracy 0.562500\n",
      "Validation accuracy 0.520833\n",
      "Time elapsed 1.3247401038805644 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/train',\n",
    "                                     sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/validation')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "cifar10.reset()\n",
    "print(\"Trainable variables\")\n",
    "for n in tf.trainable_variables():\n",
    "    print(n.name)\n",
    "\n",
    "epochs = 200\n",
    "mean_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "std_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "\n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "while cifar10.getEpoch() < epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    # just a training iteration\n",
    "    _ = sess.run(train_step,\n",
    "                feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "    \n",
    "    step = batch_idx+epoch*n_batches\n",
    "    \n",
    "    # Write training summary\n",
    "    if step%50==0:\n",
    "        summary = sess.run(merged,\n",
    "                          feed_dict={\n",
    "                model_input: batch_data,\n",
    "                target: batch_labels,\n",
    "                keep_prob: 0.5 # set to 1.0 at inference time\n",
    "            })\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "    # gradient (by layer) statistics over last training batch & validation summary\n",
    "    if batch_idx==0:\n",
    "        loss, acc, grads = sess.run((cross_entropy, accuracy, grads_vars), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "        for layer in range(len(tf.trainable_variables())):\n",
    "            mean_gradients[layer, epoch] = np.mean(np.abs(grads[layer][0]))\n",
    "            std_gradients[layer, epoch] = np.std(np.abs(grads[layer][0]))\n",
    "        print(\"Epoch %d, training loss %f, accuracy %f\" % (epoch, loss, acc))\n",
    "        \n",
    "        summary, validation_accuracy = validate()\n",
    "        validation_writer.add_summary(summary, step)\n",
    "        print(\"Validation accuracy %f\" % validation_accuracy)\n",
    "        print(\"Time elapsed\", (time.time()-t_i)/60.0, \"minutes\")\n",
    "train_writer.flush()\n",
    "validation_writer.flush()\n",
    "test_acc = test()\n",
    "print(\"Testing set accuracy %f\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting gradients\n",
    "n_layers = len(tf.trainable_variables()) // 2\n",
    "x = np.arange(epochs)\n",
    "i = 0\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Weights Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()\n",
    "i = 1\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Biases Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(targets, outputs):\n",
    "    '''Returns a confusion matrix. Both targets and outputs\n",
    "    should be 1-D arrays of zeros and ones.'''\n",
    "    encoded_data = 2*targets+outputs  # Map targets and outputs to {0, 1, 2, 3}\n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return ((TP, FP), (FN, TN))\n",
    "\n",
    "def roc_curve(targets, outputs):\n",
    "    '''Returns a ROC curve. Outputs should be in range 0-1\n",
    "    in order to move the threshold.'''\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        outputs_with_threshold = (outputs > threshold).astype(np.float)\n",
    "        ((TP, FP), (FN, TN)) = confusion_matrix(\n",
    "            targets, \n",
    "            outputs_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "        fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "accs = sess.run(accuracy,\n",
    "             feed_dict={\n",
    "            model_input: cifar10.training_data,\n",
    "                       target: cifar10.training_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "accs = np.asarray(accs)\n",
    "[[VN,FP],[FN,VP]] = confusion_matrix(\n",
    "    accs,\n",
    "    (accs>0.5).astype(np.float))\n",
    "print('VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN))\n",
    "print('Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN)))\n",
    "print('Precision: %%%f' %(100.0*VP/(VP+FP)))\n",
    "print('Recall: %%%f' %(100.0*VP/(VP+FN)))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
