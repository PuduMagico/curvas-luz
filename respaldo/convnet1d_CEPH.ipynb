{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "\n",
    "from parser_CEPH import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "batch_size = 64\n",
    "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False, file='data_CEPH.csv')\n",
    "\n",
    "SUMMARIES_DIR = './summaries/convnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape, layer_name):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
    "                             initializer=tf.constant_initializer(0.05))\n",
    "    \n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    \n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, \n",
    "                       strides = [1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape, layer_name):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_input = tf.placeholder(tf.float32, name='model_input', \n",
    "                             shape=(batch_size,1,200,1))\n",
    "tf.summary.image('input', model_input, 10)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob', shape=())\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target', shape=(batch_size, 1))\n",
    "\n",
    "# First convolution layer\n",
    "layer_name = 'conv1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv1_out = conv_layer(model_input, [1, 3, 1, 20], layer_name)\n",
    "# First pooling layer\n",
    "with tf.name_scope('pool1'):\n",
    "    pool1_out = tf.nn.avg_pool(conv1_out, ksize=[1, 2, 2, 1],\n",
    "                strides=[1, 2, 2, 1], padding='SAME',\n",
    "                name='pool1')\n",
    "    \n",
    "\n",
    "# Second convolution layer\n",
    "layer_name = 'conv2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv2_out = conv_layer(pool1_out, [1, 3, 20, 20], layer_name)\n",
    "# Second pooling layer\n",
    "with tf.name_scope('pool2'):\n",
    "    pool2_out = tf.nn.avg_pool(conv2_out, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME',\n",
    "                            name='pool2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,dim3,dim4 = pool2_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2_out_flat = tf.reshape(pool2_out, [-1, 1*dim3.value*dim4.value], name='pool2_flat')\n",
    "# First fully connected layer\n",
    "layer_name = 'fc1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc1_out = fc_layer(pool2_out_flat, [1*dim3.value*dim4.value, 500], layer_name)\n",
    "fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "\n",
    "# Second fully connected layer\n",
    "layer_name = 'fc2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc2_out = fc_layer(fc1_out_drop, [500, 250], layer_name)\n",
    "fc2_out_drop = tf.nn.dropout(fc2_out, keep_prob)\n",
    "\n",
    "\n",
    "# Third fully connected layer\n",
    "layer_name = 'fc3'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc3_out = fc_layer(fc2_out_drop, [250,1], layer_name)\n",
    "model_output = fc3_out\n",
    "\n",
    "\n",
    "with tf.name_scope('loss_function'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n",
    "                                           name='cross_entropy'))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    grads_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(model_output, 1),\n",
    "                             tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Useful training functions\n",
    "def validate():\n",
    "    batches = cifar10.getValidationSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    summary = sess.run(merged,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "    return summary, mean_acc\n",
    "def test():\n",
    "    batches = cifar10.getTestSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable variables\n",
      "conv1/weights:0\n",
      "conv1/biases:0\n",
      "conv2/weights:0\n",
      "conv2/biases:0\n",
      "fc1/weights:0\n",
      "fc1/biases:0\n",
      "fc2/weights:0\n",
      "fc2/biases:0\n",
      "fc3/weights:0\n",
      "fc3/biases:0\n",
      "Epoch 0, training loss 1.354920, accuracy 0.437500\n",
      "Validation accuracy 0.453125\n",
      "Time elapsed 0.002472476164499919 minutes\n",
      "Epoch 1, training loss 1.164054, accuracy 0.406250\n",
      "Validation accuracy 0.476562\n",
      "Time elapsed 0.016185828049977622 minutes\n",
      "Epoch 2, training loss 1.111566, accuracy 0.406250\n",
      "Validation accuracy 0.500000\n",
      "Time elapsed 0.035639576117197674 minutes\n",
      "Epoch 3, training loss 1.229489, accuracy 0.343750\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 0.05029478867848714 minutes\n",
      "Epoch 4, training loss 1.048360, accuracy 0.406250\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 0.06449681520462036 minutes\n",
      "Epoch 5, training loss 1.027171, accuracy 0.312500\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 0.07911235094070435 minutes\n",
      "Epoch 6, training loss 1.049444, accuracy 0.437500\n",
      "Validation accuracy 0.445312\n",
      "Time elapsed 0.09250821272532145 minutes\n",
      "Epoch 7, training loss 0.913418, accuracy 0.437500\n",
      "Validation accuracy 0.455729\n",
      "Time elapsed 0.10847955147425334 minutes\n",
      "Epoch 8, training loss 0.919623, accuracy 0.406250\n",
      "Validation accuracy 0.434896\n",
      "Time elapsed 0.12560770114262898 minutes\n",
      "Epoch 9, training loss 0.910835, accuracy 0.468750\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 0.14182397921880086 minutes\n",
      "Epoch 10, training loss 0.871284, accuracy 0.500000\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 0.15697442690531413 minutes\n",
      "Epoch 11, training loss 0.992690, accuracy 0.609375\n",
      "Validation accuracy 0.442708\n",
      "Time elapsed 0.17697384357452392 minutes\n",
      "Epoch 12, training loss 0.873276, accuracy 0.390625\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 0.1929884632428487 minutes\n",
      "Epoch 13, training loss 1.034789, accuracy 0.515625\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 0.2072396477063497 minutes\n",
      "Epoch 14, training loss 1.146954, accuracy 0.406250\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 0.2219542105992635 minutes\n",
      "Epoch 15, training loss 0.988100, accuracy 0.562500\n",
      "Validation accuracy 0.455729\n",
      "Time elapsed 0.2359266996383667 minutes\n",
      "Epoch 16, training loss 0.954843, accuracy 0.453125\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 0.2517492055892944 minutes\n",
      "Epoch 17, training loss 0.933801, accuracy 0.515625\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 0.26698350509007773 minutes\n",
      "Epoch 18, training loss 1.004880, accuracy 0.406250\n",
      "Validation accuracy 0.442708\n",
      "Time elapsed 0.28460056384404503 minutes\n",
      "Epoch 19, training loss 0.919606, accuracy 0.421875\n",
      "Validation accuracy 0.471354\n",
      "Time elapsed 0.3009813626607259 minutes\n",
      "Epoch 20, training loss 0.951638, accuracy 0.453125\n",
      "Validation accuracy 0.445312\n",
      "Time elapsed 0.3175079623858134 minutes\n",
      "Epoch 21, training loss 0.965209, accuracy 0.453125\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 0.3378130793571472 minutes\n",
      "Epoch 22, training loss 0.947440, accuracy 0.546875\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 0.35491252740224205 minutes\n",
      "Epoch 23, training loss 0.908342, accuracy 0.406250\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 0.3731783668200175 minutes\n",
      "Epoch 24, training loss 0.931290, accuracy 0.500000\n",
      "Validation accuracy 0.455729\n",
      "Time elapsed 0.3917951544125875 minutes\n",
      "Epoch 25, training loss 1.065307, accuracy 0.359375\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 0.41003691355387367 minutes\n",
      "Epoch 26, training loss 0.989568, accuracy 0.359375\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 0.43043201764424643 minutes\n",
      "Epoch 27, training loss 1.134669, accuracy 0.406250\n",
      "Validation accuracy 0.455729\n",
      "Time elapsed 0.4490096211433411 minutes\n",
      "Epoch 28, training loss 1.020630, accuracy 0.515625\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 0.4655309955279032 minutes\n",
      "Epoch 29, training loss 0.946115, accuracy 0.546875\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 0.4836071491241455 minutes\n",
      "Epoch 30, training loss 0.844403, accuracy 0.500000\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 0.5023009975751241 minutes\n",
      "Epoch 31, training loss 0.885957, accuracy 0.500000\n",
      "Validation accuracy 0.440104\n",
      "Time elapsed 0.522317890326182 minutes\n",
      "Epoch 32, training loss 1.116315, accuracy 0.453125\n",
      "Validation accuracy 0.445312\n",
      "Time elapsed 0.5432072480519613 minutes\n",
      "Epoch 33, training loss 0.965483, accuracy 0.406250\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 0.5641520619392395 minutes\n",
      "Epoch 34, training loss 0.780194, accuracy 0.593750\n",
      "Validation accuracy 0.471354\n",
      "Time elapsed 0.5848004976908366 minutes\n",
      "Epoch 35, training loss 0.955155, accuracy 0.500000\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 0.6161439100901286 minutes\n",
      "Epoch 36, training loss 0.923758, accuracy 0.578125\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 0.641800320148468 minutes\n",
      "Epoch 37, training loss 0.975689, accuracy 0.515625\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 0.666397221883138 minutes\n",
      "Epoch 38, training loss 1.156429, accuracy 0.515625\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 0.6867109735806783 minutes\n",
      "Epoch 39, training loss 0.844504, accuracy 0.562500\n",
      "Validation accuracy 0.453125\n",
      "Time elapsed 0.7081719040870667 minutes\n",
      "Epoch 40, training loss 0.879875, accuracy 0.437500\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 0.7291122198104858 minutes\n",
      "Epoch 41, training loss 0.825047, accuracy 0.625000\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 0.7474759459495545 minutes\n",
      "Epoch 42, training loss 1.006828, accuracy 0.390625\n",
      "Validation accuracy 0.453125\n",
      "Time elapsed 0.7681870579719543 minutes\n",
      "Epoch 43, training loss 1.013858, accuracy 0.390625\n",
      "Validation accuracy 0.471354\n",
      "Time elapsed 0.7877347111701966 minutes\n",
      "Epoch 44, training loss 0.895276, accuracy 0.453125\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 0.8037764112154643 minutes\n",
      "Epoch 45, training loss 0.869894, accuracy 0.453125\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 0.8211133241653442 minutes\n",
      "Epoch 46, training loss 1.036278, accuracy 0.453125\n",
      "Validation accuracy 0.486979\n",
      "Time elapsed 0.8392948547999064 minutes\n",
      "Epoch 47, training loss 0.928981, accuracy 0.468750\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 0.8568517525990804 minutes\n",
      "Epoch 48, training loss 0.860277, accuracy 0.546875\n",
      "Validation accuracy 0.476562\n",
      "Time elapsed 0.8733824213345845 minutes\n",
      "Epoch 49, training loss 0.891506, accuracy 0.531250\n",
      "Validation accuracy 0.453125\n",
      "Time elapsed 0.8926567117373149 minutes\n",
      "Epoch 50, training loss 0.844393, accuracy 0.562500\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 0.9101149757703145 minutes\n",
      "Epoch 51, training loss 0.957856, accuracy 0.546875\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 0.9377102812131246 minutes\n",
      "Epoch 52, training loss 1.043573, accuracy 0.593750\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 0.9642311930656433 minutes\n",
      "Epoch 53, training loss 1.013844, accuracy 0.500000\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 0.9888203501701355 minutes\n",
      "Epoch 54, training loss 0.944983, accuracy 0.500000\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 1.0105092604955037 minutes\n",
      "Epoch 55, training loss 1.031543, accuracy 0.531250\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 1.0322343111038208 minutes\n",
      "Epoch 56, training loss 1.096894, accuracy 0.468750\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 1.0530898491541545 minutes\n",
      "Epoch 57, training loss 1.162213, accuracy 0.406250\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 1.071968718369802 minutes\n",
      "Epoch 58, training loss 0.937926, accuracy 0.406250\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 1.0913117011388143 minutes\n",
      "Epoch 59, training loss 1.024232, accuracy 0.531250\n",
      "Validation accuracy 0.447917\n",
      "Time elapsed 1.111615526676178 minutes\n",
      "Epoch 60, training loss 0.813630, accuracy 0.531250\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 1.1291450063387554 minutes\n",
      "Epoch 61, training loss 1.045873, accuracy 0.406250\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 1.1474493781725565 minutes\n",
      "Epoch 62, training loss 0.802799, accuracy 0.562500\n",
      "Validation accuracy 0.476562\n",
      "Time elapsed 1.164479915301005 minutes\n",
      "Epoch 63, training loss 0.924093, accuracy 0.484375\n",
      "Validation accuracy 0.455729\n",
      "Time elapsed 1.1809212485949199 minutes\n",
      "Epoch 64, training loss 1.018934, accuracy 0.562500\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 1.1984396656354268 minutes\n",
      "Epoch 65, training loss 0.941638, accuracy 0.437500\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 1.2183026432991029 minutes\n",
      "Epoch 66, training loss 0.956988, accuracy 0.390625\n",
      "Validation accuracy 0.481771\n",
      "Time elapsed 1.2357578078905742 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, training loss 1.185069, accuracy 0.406250\n",
      "Validation accuracy 0.484375\n",
      "Time elapsed 1.257774794101715 minutes\n",
      "Epoch 68, training loss 1.025751, accuracy 0.468750\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 1.2842205127080282 minutes\n",
      "Epoch 69, training loss 0.932274, accuracy 0.562500\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 1.311620259284973 minutes\n",
      "Epoch 70, training loss 0.884207, accuracy 0.515625\n",
      "Validation accuracy 0.476562\n",
      "Time elapsed 1.333538051446279 minutes\n",
      "Epoch 71, training loss 1.001035, accuracy 0.468750\n",
      "Validation accuracy 0.450521\n",
      "Time elapsed 1.3562750140825908 minutes\n",
      "Epoch 72, training loss 0.929100, accuracy 0.453125\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 1.3772461930910747 minutes\n",
      "Epoch 73, training loss 0.814916, accuracy 0.515625\n",
      "Validation accuracy 0.471354\n",
      "Time elapsed 1.393930466969808 minutes\n",
      "Epoch 74, training loss 0.969209, accuracy 0.437500\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 1.412281612555186 minutes\n",
      "Epoch 75, training loss 1.034709, accuracy 0.437500\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 1.4333828409512839 minutes\n",
      "Epoch 76, training loss 0.913810, accuracy 0.531250\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 1.4515618205070495 minutes\n",
      "Epoch 77, training loss 1.012401, accuracy 0.546875\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 1.468389125665029 minutes\n",
      "Epoch 78, training loss 0.937883, accuracy 0.515625\n",
      "Validation accuracy 0.471354\n",
      "Time elapsed 1.4869614203770956 minutes\n",
      "Epoch 79, training loss 0.918941, accuracy 0.500000\n",
      "Validation accuracy 0.492188\n",
      "Time elapsed 1.5055797696113586 minutes\n",
      "Epoch 80, training loss 0.993232, accuracy 0.500000\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 1.525481641292572 minutes\n",
      "Epoch 81, training loss 0.907653, accuracy 0.437500\n",
      "Validation accuracy 0.484375\n",
      "Time elapsed 1.5434204419453939 minutes\n",
      "Epoch 82, training loss 0.951986, accuracy 0.406250\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 1.5615795612335206 minutes\n",
      "Epoch 83, training loss 0.810371, accuracy 0.578125\n",
      "Validation accuracy 0.445312\n",
      "Time elapsed 1.583867621421814 minutes\n",
      "Epoch 84, training loss 1.102687, accuracy 0.406250\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 1.611274242401123 minutes\n",
      "Epoch 85, training loss 1.064751, accuracy 0.484375\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 1.6411977291107178 minutes\n",
      "Epoch 86, training loss 1.076650, accuracy 0.421875\n",
      "Validation accuracy 0.445312\n",
      "Time elapsed 1.6619977752367656 minutes\n",
      "Epoch 87, training loss 0.971235, accuracy 0.484375\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 1.684093995889028 minutes\n",
      "Epoch 88, training loss 0.931273, accuracy 0.484375\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 1.705015198389689 minutes\n",
      "Epoch 89, training loss 1.042359, accuracy 0.453125\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 1.7256711840629577 minutes\n",
      "Epoch 90, training loss 1.007283, accuracy 0.437500\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 1.7443851153055827 minutes\n",
      "Epoch 91, training loss 1.118598, accuracy 0.359375\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 1.762194526195526 minutes\n",
      "Epoch 92, training loss 0.888856, accuracy 0.421875\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 1.7817149758338928 minutes\n",
      "Epoch 93, training loss 0.930874, accuracy 0.562500\n",
      "Validation accuracy 0.447917\n",
      "Time elapsed 1.7998441537221272 minutes\n",
      "Epoch 94, training loss 0.912078, accuracy 0.609375\n",
      "Validation accuracy 0.440104\n",
      "Time elapsed 1.8176398913065592 minutes\n",
      "Epoch 95, training loss 1.069620, accuracy 0.453125\n",
      "Validation accuracy 0.453125\n",
      "Time elapsed 1.8358832438786825 minutes\n",
      "Epoch 96, training loss 0.906015, accuracy 0.593750\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 1.850339901447296 minutes\n",
      "Epoch 97, training loss 0.949297, accuracy 0.531250\n",
      "Validation accuracy 0.471354\n",
      "Time elapsed 1.8689712007840475 minutes\n",
      "Epoch 98, training loss 0.833303, accuracy 0.500000\n",
      "Validation accuracy 0.479167\n",
      "Time elapsed 1.8864531914393108 minutes\n",
      "Epoch 99, training loss 0.947915, accuracy 0.484375\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 1.906974466641744 minutes\n",
      "Epoch 100, training loss 0.990025, accuracy 0.500000\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 1.9373706102371215 minutes\n",
      "Epoch 101, training loss 1.143336, accuracy 0.468750\n",
      "Validation accuracy 0.455729\n",
      "Time elapsed 1.965598460038503 minutes\n",
      "Epoch 102, training loss 1.041965, accuracy 0.562500\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 1.9892231305440267 minutes\n",
      "Epoch 103, training loss 0.969215, accuracy 0.515625\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 2.012758433818817 minutes\n",
      "Epoch 104, training loss 0.863944, accuracy 0.453125\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 2.0331074436505636 minutes\n",
      "Epoch 105, training loss 0.894955, accuracy 0.593750\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 2.053155736128489 minutes\n",
      "Epoch 106, training loss 1.066881, accuracy 0.500000\n",
      "Validation accuracy 0.453125\n",
      "Time elapsed 2.072180676460266 minutes\n",
      "Epoch 107, training loss 0.910834, accuracy 0.531250\n",
      "Validation accuracy 0.466146\n",
      "Time elapsed 2.0900444984436035 minutes\n",
      "Epoch 108, training loss 0.985825, accuracy 0.406250\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 2.1083709160486857 minutes\n",
      "Epoch 109, training loss 0.902490, accuracy 0.453125\n",
      "Validation accuracy 0.471354\n",
      "Time elapsed 2.125326093037923 minutes\n",
      "Epoch 110, training loss 0.924634, accuracy 0.484375\n",
      "Validation accuracy 0.476562\n",
      "Time elapsed 2.144520886739095 minutes\n",
      "Epoch 111, training loss 0.880449, accuracy 0.562500\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 2.161817904313405 minutes\n",
      "Epoch 112, training loss 0.952683, accuracy 0.546875\n",
      "Validation accuracy 0.481771\n",
      "Time elapsed 2.1802330136299135 minutes\n",
      "Epoch 113, training loss 0.934214, accuracy 0.437500\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 2.1968743522961933 minutes\n",
      "Epoch 114, training loss 0.910215, accuracy 0.562500\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 2.214806548754374 minutes\n",
      "Epoch 115, training loss 1.043739, accuracy 0.390625\n",
      "Validation accuracy 0.460938\n",
      "Time elapsed 2.232399626572927 minutes\n",
      "Epoch 116, training loss 0.860424, accuracy 0.484375\n",
      "Validation accuracy 0.479167\n",
      "Time elapsed 2.253809479872386 minutes\n",
      "Epoch 117, training loss 0.903719, accuracy 0.468750\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 2.279936198393504 minutes\n",
      "Epoch 118, training loss 1.014556, accuracy 0.421875\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 2.306445546944936 minutes\n",
      "Epoch 119, training loss 0.858558, accuracy 0.500000\n",
      "Validation accuracy 0.471354\n",
      "Time elapsed 2.3273431022961932 minutes\n",
      "Epoch 120, training loss 0.882933, accuracy 0.437500\n",
      "Validation accuracy 0.481771\n",
      "Time elapsed 2.3491835991541543 minutes\n",
      "Epoch 121, training loss 1.053131, accuracy 0.609375\n",
      "Validation accuracy 0.445312\n",
      "Time elapsed 2.3709522128105163 minutes\n",
      "Epoch 122, training loss 0.972616, accuracy 0.484375\n",
      "Validation accuracy 0.502604\n",
      "Time elapsed 2.3922909021377565 minutes\n",
      "Epoch 123, training loss 0.993178, accuracy 0.500000\n",
      "Validation accuracy 0.476562\n",
      "Time elapsed 2.410910411675771 minutes\n",
      "Epoch 124, training loss 0.923645, accuracy 0.500000\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 2.430523423353831 minutes\n",
      "Epoch 125, training loss 1.002745, accuracy 0.453125\n",
      "Validation accuracy 0.476562\n",
      "Time elapsed 2.4485825618108112 minutes\n",
      "Epoch 126, training loss 1.242355, accuracy 0.453125\n",
      "Validation accuracy 0.442708\n",
      "Time elapsed 2.466992684205373 minutes\n",
      "Epoch 127, training loss 1.042143, accuracy 0.484375\n",
      "Validation accuracy 0.497396\n",
      "Time elapsed 2.484898821512858 minutes\n",
      "Epoch 128, training loss 0.950513, accuracy 0.562500\n",
      "Validation accuracy 0.492188\n",
      "Time elapsed 2.503624598185221 minutes\n",
      "Epoch 129, training loss 1.101622, accuracy 0.421875\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 2.520889174938202 minutes\n",
      "Epoch 130, training loss 0.821182, accuracy 0.656250\n",
      "Validation accuracy 0.489583\n",
      "Time elapsed 2.5394249399503073 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/train',\n",
    "                                     sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/validation')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "cifar10.reset()\n",
    "print(\"Trainable variables\")\n",
    "for n in tf.trainable_variables():\n",
    "    print(n.name)\n",
    "\n",
    "epochs = 200\n",
    "mean_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "std_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "\n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "while cifar10.getEpoch() < epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    # just a training iteration\n",
    "    _ = sess.run(train_step,\n",
    "                feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "    \n",
    "    step = batch_idx+epoch*n_batches\n",
    "    \n",
    "    # Write training summary\n",
    "    if step%50==0:\n",
    "        summary = sess.run(merged,\n",
    "                          feed_dict={\n",
    "                model_input: batch_data,\n",
    "                target: batch_labels,\n",
    "                keep_prob: 0.5 # set to 1.0 at inference time\n",
    "            })\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "    # gradient (by layer) statistics over last training batch & validation summary\n",
    "    if batch_idx==0:\n",
    "        loss, acc, grads = sess.run((cross_entropy, accuracy, grads_vars), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "        for layer in range(len(tf.trainable_variables())):\n",
    "            mean_gradients[layer, epoch] = np.mean(np.abs(grads[layer][0]))\n",
    "            std_gradients[layer, epoch] = np.std(np.abs(grads[layer][0]))\n",
    "        print(\"Epoch %d, training loss %f, accuracy %f\" % (epoch, loss, acc))\n",
    "        \n",
    "        summary, validation_accuracy = validate()\n",
    "        validation_writer.add_summary(summary, step)\n",
    "        print(\"Validation accuracy %f\" % validation_accuracy)\n",
    "        print(\"Time elapsed\", (time.time()-t_i)/60.0, \"minutes\")\n",
    "train_writer.flush()\n",
    "validation_writer.flush()\n",
    "test_acc = test()\n",
    "print(\"Testing set accuracy %f\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting gradients\n",
    "n_layers = len(tf.trainable_variables()) // 2\n",
    "x = np.arange(epochs)\n",
    "i = 0\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Weights Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()\n",
    "i = 1\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Biases Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(targets, outputs):\n",
    "    '''Returns a confusion matrix. Both targets and outputs\n",
    "    should be 1-D arrays of zeros and ones.'''\n",
    "    encoded_data = 2*targets+outputs  # Map targets and outputs to {0, 1, 2, 3}\n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return ((TP, FP), (FN, TN))\n",
    "\n",
    "def roc_curve(targets, outputs):\n",
    "    '''Returns a ROC curve. Outputs should be in range 0-1\n",
    "    in order to move the threshold.'''\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        outputs_with_threshold = (outputs > threshold).astype(np.float)\n",
    "        ((TP, FP), (FN, TN)) = confusion_matrix(\n",
    "            targets, \n",
    "            outputs_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "        fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "accs = sess.run(accuracy,\n",
    "             feed_dict={\n",
    "            model_input: cifar10.training_data,\n",
    "                       target: cifar10.training_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "accs = np.asarray(accs)\n",
    "[[VN,FP],[FN,VP]] = confusion_matrix(\n",
    "    accs,\n",
    "    (accs>0.5).astype(np.float))\n",
    "print('VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN))\n",
    "print('Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN)))\n",
    "print('Precision: %%%f' %(100.0*VP/(VP+FP)))\n",
    "print('Recall: %%%f' %(100.0*VP/(VP+FN)))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
