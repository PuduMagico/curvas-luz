{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "\n",
    "from cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "batch_size = 64\n",
    "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False, file='data_original.csv')\n",
    "\n",
    "SUMMARIES_DIR = './summaries/convnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape, layer_name):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
    "                             initializer=tf.constant_initializer(0.05))\n",
    "    \n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    \n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, \n",
    "                       strides = [1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape, layer_name):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_input = tf.placeholder(tf.float32, name='model_input', \n",
    "                             shape=(batch_size,1,200,1))\n",
    "tf.summary.image('input', model_input, 10)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob', shape=())\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target', shape=(batch_size, 9))\n",
    "\n",
    " # Reshape tensor to MLP\n",
    "first_layer_input = tf.reshape(model_input, [-1,200], name='first_layer_input')\n",
    "# First layer\n",
    "layer_name = 'fc1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc1_out = fc_layer(first_layer_input, [200, 150], layer_name)\n",
    "fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "# Second layer\n",
    "layer_name = 'fc2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc2_out = fc_layer(fc1_out_drop, [150, 100],layer_name)\n",
    "fc2_out_drop = tf.nn.dropout(fc2_out, keep_prob)\n",
    "#Third Layer\n",
    "layer_name = 'fc3'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc3_out = fc_layer(fc2_out_drop, [100,50], layer_name)\n",
    "fc3_out_drop = tf.nn.dropout(fc3_out, keep_prob)\n",
    "\n",
    "layer_name = 'fc4'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc4_out = fc_layer(fc3_out_drop, [50,10], layer_name)\n",
    "fc4_out_drop = tf.nn.dropout(fc4_out, keep_prob)\n",
    "\n",
    "layer_name = 'fc5'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc5_out = fc_layer(fc4_out_drop, [10,9], layer_name)\n",
    "model_output = fc5_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with tf.name_scope('loss_function'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n",
    "                                           name='cross_entropy'))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    grads_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(model_output, 1),\n",
    "                             tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Useful training functions\n",
    "def validate():\n",
    "    batches = cifar10.getValidationSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    summary = sess.run(merged,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "    return summary, mean_acc\n",
    "def test():\n",
    "    batches = cifar10.getTestSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable variables\n",
      "fc1/weights:0\n",
      "fc1/biases:0\n",
      "fc2/weights:0\n",
      "fc2/biases:0\n",
      "fc3/weights:0\n",
      "fc3/biases:0\n",
      "fc4/weights:0\n",
      "fc4/biases:0\n",
      "fc5/weights:0\n",
      "fc5/biases:0\n",
      "Epoch 0, training loss 2.234843, accuracy 0.093750\n",
      "Validation accuracy 0.066667\n",
      "Time elapsed 0.0016490379969278971 minutes\n",
      "Epoch 1, training loss 2.076166, accuracy 0.234375\n",
      "Validation accuracy 0.282292\n",
      "Time elapsed 0.008875628312428793 minutes\n",
      "Epoch 2, training loss 1.985534, accuracy 0.187500\n",
      "Validation accuracy 0.272917\n",
      "Time elapsed 0.015715765953063964 minutes\n",
      "Epoch 3, training loss 1.901565, accuracy 0.296875\n",
      "Validation accuracy 0.263542\n",
      "Time elapsed 0.02236416737238566 minutes\n",
      "Epoch 4, training loss 1.922077, accuracy 0.265625\n",
      "Validation accuracy 0.271875\n",
      "Time elapsed 0.029246441523234048 minutes\n",
      "Epoch 5, training loss 1.912251, accuracy 0.312500\n",
      "Validation accuracy 0.280208\n",
      "Time elapsed 0.03582298755645752 minutes\n",
      "Epoch 6, training loss 1.794742, accuracy 0.312500\n",
      "Validation accuracy 0.277083\n",
      "Time elapsed 0.042800295352935794 minutes\n",
      "Epoch 7, training loss 1.853137, accuracy 0.328125\n",
      "Validation accuracy 0.280208\n",
      "Time elapsed 0.049603227774302164 minutes\n",
      "Epoch 8, training loss 1.907706, accuracy 0.171875\n",
      "Validation accuracy 0.276042\n",
      "Time elapsed 0.05594940185546875 minutes\n",
      "Epoch 9, training loss 1.796334, accuracy 0.218750\n",
      "Validation accuracy 0.278125\n",
      "Time elapsed 0.06363655726114908 minutes\n",
      "Epoch 10, training loss 1.844233, accuracy 0.281250\n",
      "Validation accuracy 0.287500\n",
      "Time elapsed 0.07233901023864746 minutes\n",
      "Epoch 11, training loss 1.833623, accuracy 0.281250\n",
      "Validation accuracy 0.277083\n",
      "Time elapsed 0.07990379333496093 minutes\n",
      "Epoch 12, training loss 1.820272, accuracy 0.250000\n",
      "Validation accuracy 0.280208\n",
      "Time elapsed 0.0878452181816101 minutes\n",
      "Epoch 13, training loss 1.844246, accuracy 0.312500\n",
      "Validation accuracy 0.335417\n",
      "Time elapsed 0.09509257872899374 minutes\n",
      "Epoch 14, training loss 1.710209, accuracy 0.312500\n",
      "Validation accuracy 0.353125\n",
      "Time elapsed 0.10117852687835693 minutes\n",
      "Epoch 15, training loss 1.674395, accuracy 0.343750\n",
      "Validation accuracy 0.303125\n",
      "Time elapsed 0.10736689567565919 minutes\n",
      "Epoch 16, training loss 1.624249, accuracy 0.453125\n",
      "Validation accuracy 0.352083\n",
      "Time elapsed 0.11340313752492269 minutes\n",
      "Epoch 17, training loss 1.649566, accuracy 0.218750\n",
      "Validation accuracy 0.355208\n",
      "Time elapsed 0.11924955844879151 minutes\n",
      "Epoch 18, training loss 1.696396, accuracy 0.437500\n",
      "Validation accuracy 0.391667\n",
      "Time elapsed 0.12571934461593628 minutes\n",
      "Epoch 19, training loss 1.590353, accuracy 0.406250\n",
      "Validation accuracy 0.377083\n",
      "Time elapsed 0.13636101484298707 minutes\n",
      "Epoch 20, training loss 1.637467, accuracy 0.375000\n",
      "Validation accuracy 0.372917\n",
      "Time elapsed 0.14428605635960898 minutes\n",
      "Epoch 21, training loss 1.521410, accuracy 0.312500\n",
      "Validation accuracy 0.405208\n",
      "Time elapsed 0.1503383199373881 minutes\n",
      "Epoch 22, training loss 1.489315, accuracy 0.359375\n",
      "Validation accuracy 0.381250\n",
      "Time elapsed 0.15635912418365477 minutes\n",
      "Epoch 23, training loss 1.553620, accuracy 0.343750\n",
      "Validation accuracy 0.391667\n",
      "Time elapsed 0.16233205795288086 minutes\n",
      "Epoch 24, training loss 1.447537, accuracy 0.468750\n",
      "Validation accuracy 0.379167\n",
      "Time elapsed 0.17090962727864584 minutes\n",
      "Epoch 25, training loss 1.518922, accuracy 0.312500\n",
      "Validation accuracy 0.384375\n",
      "Time elapsed 0.17878662347793578 minutes\n",
      "Epoch 26, training loss 1.759131, accuracy 0.250000\n",
      "Validation accuracy 0.375000\n",
      "Time elapsed 0.18650994698206583 minutes\n",
      "Epoch 27, training loss 1.509991, accuracy 0.437500\n",
      "Validation accuracy 0.393750\n",
      "Time elapsed 0.1939606467882792 minutes\n",
      "Epoch 28, training loss 1.644971, accuracy 0.343750\n",
      "Validation accuracy 0.396875\n",
      "Time elapsed 0.20086724758148194 minutes\n",
      "Epoch 29, training loss 1.390593, accuracy 0.390625\n",
      "Validation accuracy 0.415625\n",
      "Time elapsed 0.20860838492711384 minutes\n",
      "Epoch 30, training loss 1.541400, accuracy 0.343750\n",
      "Validation accuracy 0.413542\n",
      "Time elapsed 0.21613422234853108 minutes\n",
      "Epoch 31, training loss 1.388013, accuracy 0.437500\n",
      "Validation accuracy 0.420833\n",
      "Time elapsed 0.2233088771502177 minutes\n",
      "Epoch 32, training loss 1.395169, accuracy 0.437500\n",
      "Validation accuracy 0.417708\n",
      "Time elapsed 0.2305510361989339 minutes\n",
      "Epoch 33, training loss 1.404095, accuracy 0.500000\n",
      "Validation accuracy 0.411458\n",
      "Time elapsed 0.23732942740122479 minutes\n",
      "Epoch 34, training loss 1.314035, accuracy 0.375000\n",
      "Validation accuracy 0.425000\n",
      "Time elapsed 0.24382415215174358 minutes\n",
      "Epoch 35, training loss 1.345319, accuracy 0.500000\n",
      "Validation accuracy 0.408333\n",
      "Time elapsed 0.24998435974121094 minutes\n",
      "Epoch 36, training loss 1.567884, accuracy 0.453125\n",
      "Validation accuracy 0.422917\n",
      "Time elapsed 0.2561808784802755 minutes\n",
      "Epoch 37, training loss 1.365300, accuracy 0.375000\n",
      "Validation accuracy 0.432292\n",
      "Time elapsed 0.26280922889709474 minutes\n",
      "Epoch 38, training loss 1.482492, accuracy 0.484375\n",
      "Validation accuracy 0.451042\n",
      "Time elapsed 0.2705827236175537 minutes\n",
      "Epoch 39, training loss 1.427826, accuracy 0.375000\n",
      "Validation accuracy 0.423958\n",
      "Time elapsed 0.2770922581354777 minutes\n",
      "Epoch 40, training loss 1.448256, accuracy 0.437500\n",
      "Validation accuracy 0.403125\n",
      "Time elapsed 0.28458770910898845 minutes\n",
      "Epoch 41, training loss 1.549890, accuracy 0.421875\n",
      "Validation accuracy 0.416667\n",
      "Time elapsed 0.2911093632380168 minutes\n",
      "Epoch 42, training loss 1.402967, accuracy 0.375000\n",
      "Validation accuracy 0.428125\n",
      "Time elapsed 0.2998496611913045 minutes\n",
      "Epoch 43, training loss 1.337168, accuracy 0.437500\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 0.30761237144470216 minutes\n",
      "Epoch 44, training loss 1.504587, accuracy 0.406250\n",
      "Validation accuracy 0.400000\n",
      "Time elapsed 0.3149389425913493 minutes\n",
      "Epoch 45, training loss 1.388169, accuracy 0.468750\n",
      "Validation accuracy 0.470833\n",
      "Time elapsed 0.322090208530426 minutes\n",
      "Epoch 46, training loss 1.243955, accuracy 0.484375\n",
      "Validation accuracy 0.448958\n",
      "Time elapsed 0.32854698499043783 minutes\n",
      "Epoch 47, training loss 1.412243, accuracy 0.453125\n",
      "Validation accuracy 0.461458\n",
      "Time elapsed 0.33540791273117065 minutes\n",
      "Epoch 48, training loss 1.384957, accuracy 0.437500\n",
      "Validation accuracy 0.465625\n",
      "Time elapsed 0.34212621053059894 minutes\n",
      "Epoch 49, training loss 1.464512, accuracy 0.453125\n",
      "Validation accuracy 0.461458\n",
      "Time elapsed 0.3485415061314901 minutes\n",
      "Epoch 50, training loss 1.594145, accuracy 0.484375\n",
      "Validation accuracy 0.466667\n",
      "Time elapsed 0.3554266174634298 minutes\n",
      "Epoch 51, training loss 1.603685, accuracy 0.421875\n",
      "Validation accuracy 0.477083\n",
      "Time elapsed 0.3624510049819946 minutes\n",
      "Epoch 52, training loss 1.359867, accuracy 0.562500\n",
      "Validation accuracy 0.469792\n",
      "Time elapsed 0.3686185797055562 minutes\n",
      "Epoch 53, training loss 1.374229, accuracy 0.484375\n",
      "Validation accuracy 0.437500\n",
      "Time elapsed 0.3750276247660319 minutes\n",
      "Epoch 54, training loss 1.400155, accuracy 0.593750\n",
      "Validation accuracy 0.472917\n",
      "Time elapsed 0.3810061613718669 minutes\n",
      "Epoch 55, training loss 1.382648, accuracy 0.468750\n",
      "Validation accuracy 0.455208\n",
      "Time elapsed 0.3890765945116679 minutes\n",
      "Epoch 56, training loss 1.508219, accuracy 0.453125\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 0.39579296509424844 minutes\n",
      "Epoch 57, training loss 1.504649, accuracy 0.437500\n",
      "Validation accuracy 0.467708\n",
      "Time elapsed 0.4033382534980774 minutes\n",
      "Epoch 58, training loss 1.450149, accuracy 0.390625\n",
      "Validation accuracy 0.460417\n",
      "Time elapsed 0.4108397841453552 minutes\n",
      "Epoch 59, training loss 1.444054, accuracy 0.484375\n",
      "Validation accuracy 0.457292\n",
      "Time elapsed 0.41826814810434976 minutes\n",
      "Epoch 60, training loss 1.317596, accuracy 0.390625\n",
      "Validation accuracy 0.458333\n",
      "Time elapsed 0.4255061109860738 minutes\n",
      "Epoch 61, training loss 1.500797, accuracy 0.375000\n",
      "Validation accuracy 0.476042\n",
      "Time elapsed 0.4322690486907959 minutes\n",
      "Epoch 62, training loss 1.406693, accuracy 0.500000\n",
      "Validation accuracy 0.486458\n",
      "Time elapsed 0.43922104040781657 minutes\n",
      "Epoch 63, training loss 1.438206, accuracy 0.390625\n",
      "Validation accuracy 0.478125\n",
      "Time elapsed 0.44550318717956544 minutes\n",
      "Epoch 64, training loss 1.207281, accuracy 0.484375\n",
      "Validation accuracy 0.439583\n",
      "Time elapsed 0.4518887718518575 minutes\n",
      "Epoch 65, training loss 1.300377, accuracy 0.609375\n",
      "Validation accuracy 0.483333\n",
      "Time elapsed 0.4582615613937378 minutes\n",
      "Epoch 66, training loss 1.384270, accuracy 0.484375\n",
      "Validation accuracy 0.481250\n",
      "Time elapsed 0.46487609545389813 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, training loss 1.322944, accuracy 0.515625\n",
      "Validation accuracy 0.485417\n",
      "Time elapsed 0.4709646900494893 minutes\n",
      "Epoch 68, training loss 1.460385, accuracy 0.453125\n",
      "Validation accuracy 0.481250\n",
      "Time elapsed 0.47767444054285685 minutes\n",
      "Epoch 69, training loss 1.337243, accuracy 0.453125\n",
      "Validation accuracy 0.467708\n",
      "Time elapsed 0.4840227007865906 minutes\n",
      "Epoch 70, training loss 1.331046, accuracy 0.500000\n",
      "Validation accuracy 0.483333\n",
      "Time elapsed 0.49065067768096926 minutes\n",
      "Epoch 71, training loss 1.435654, accuracy 0.468750\n",
      "Validation accuracy 0.490625\n",
      "Time elapsed 0.4970617810885111 minutes\n",
      "Epoch 72, training loss 1.300237, accuracy 0.531250\n",
      "Validation accuracy 0.460417\n",
      "Time elapsed 0.5044920603434245 minutes\n",
      "Epoch 73, training loss 1.383091, accuracy 0.500000\n",
      "Validation accuracy 0.490625\n",
      "Time elapsed 0.5122857411702474 minutes\n",
      "Epoch 74, training loss 1.111003, accuracy 0.609375\n",
      "Validation accuracy 0.456250\n",
      "Time elapsed 0.5204034805297851 minutes\n",
      "Epoch 75, training loss 1.452174, accuracy 0.421875\n",
      "Validation accuracy 0.482292\n",
      "Time elapsed 0.5278732617696126 minutes\n",
      "Epoch 76, training loss 1.414680, accuracy 0.453125\n",
      "Validation accuracy 0.464583\n",
      "Time elapsed 0.5344782312711079 minutes\n",
      "Epoch 77, training loss 1.332639, accuracy 0.437500\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 0.541091775894165 minutes\n",
      "Epoch 78, training loss 1.210567, accuracy 0.609375\n",
      "Validation accuracy 0.481250\n",
      "Time elapsed 0.54800386428833 minutes\n",
      "Epoch 79, training loss 1.337945, accuracy 0.453125\n",
      "Validation accuracy 0.471875\n",
      "Time elapsed 0.5547939896583557 minutes\n",
      "Epoch 80, training loss 1.239389, accuracy 0.531250\n",
      "Validation accuracy 0.481250\n",
      "Time elapsed 0.5613665143648784 minutes\n",
      "Epoch 81, training loss 1.285393, accuracy 0.500000\n",
      "Validation accuracy 0.496875\n",
      "Time elapsed 0.5677719831466674 minutes\n",
      "Epoch 82, training loss 1.437734, accuracy 0.546875\n",
      "Validation accuracy 0.463542\n",
      "Time elapsed 0.5740029215812683 minutes\n",
      "Epoch 83, training loss 1.439720, accuracy 0.500000\n",
      "Validation accuracy 0.470833\n",
      "Time elapsed 0.5804890354474386 minutes\n",
      "Epoch 84, training loss 1.301875, accuracy 0.546875\n",
      "Validation accuracy 0.481250\n",
      "Time elapsed 0.5866675933202108 minutes\n",
      "Epoch 85, training loss 1.353588, accuracy 0.453125\n",
      "Validation accuracy 0.497917\n",
      "Time elapsed 0.5927766362826029 minutes\n",
      "Epoch 86, training loss 1.495655, accuracy 0.468750\n",
      "Validation accuracy 0.484375\n",
      "Time elapsed 0.5992769638697306 minutes\n",
      "Epoch 87, training loss 1.279064, accuracy 0.468750\n",
      "Validation accuracy 0.490625\n",
      "Time elapsed 0.605685806274414 minutes\n",
      "Epoch 88, training loss 1.313598, accuracy 0.468750\n",
      "Validation accuracy 0.493750\n",
      "Time elapsed 0.613987390200297 minutes\n",
      "Epoch 89, training loss 1.390417, accuracy 0.437500\n",
      "Validation accuracy 0.473958\n",
      "Time elapsed 0.6205204645792644 minutes\n",
      "Epoch 90, training loss 1.395681, accuracy 0.468750\n",
      "Validation accuracy 0.464583\n",
      "Time elapsed 0.626569946606954 minutes\n",
      "Epoch 91, training loss 1.170035, accuracy 0.562500\n",
      "Validation accuracy 0.482292\n",
      "Time elapsed 0.6328928073247274 minutes\n",
      "Epoch 92, training loss 1.248409, accuracy 0.531250\n",
      "Validation accuracy 0.500000\n",
      "Time elapsed 0.6394287943840027 minutes\n",
      "Epoch 93, training loss 1.394267, accuracy 0.453125\n",
      "Validation accuracy 0.493750\n",
      "Time elapsed 0.6463184515635173 minutes\n",
      "Epoch 94, training loss 1.447246, accuracy 0.406250\n",
      "Validation accuracy 0.472917\n",
      "Time elapsed 0.652763311068217 minutes\n",
      "Epoch 95, training loss 1.515462, accuracy 0.437500\n",
      "Validation accuracy 0.497917\n",
      "Time elapsed 0.6602285941441853 minutes\n",
      "Epoch 96, training loss 1.328307, accuracy 0.546875\n",
      "Validation accuracy 0.468750\n",
      "Time elapsed 0.6673767288525899 minutes\n",
      "Epoch 97, training loss 1.331535, accuracy 0.609375\n",
      "Validation accuracy 0.472917\n",
      "Time elapsed 0.6788328727086385 minutes\n",
      "Epoch 98, training loss 1.297257, accuracy 0.453125\n",
      "Validation accuracy 0.489583\n",
      "Time elapsed 0.6859287659327189 minutes\n",
      "Epoch 99, training loss 1.434441, accuracy 0.468750\n",
      "Validation accuracy 0.477083\n",
      "Time elapsed 0.6923603812853495 minutes\n",
      "Epoch 100, training loss 1.189808, accuracy 0.562500\n",
      "Validation accuracy 0.481250\n",
      "Time elapsed 0.6992206255594889 minutes\n",
      "Epoch 101, training loss 1.417163, accuracy 0.562500\n",
      "Validation accuracy 0.500000\n",
      "Time elapsed 0.7068477272987366 minutes\n",
      "Epoch 102, training loss 1.312139, accuracy 0.421875\n",
      "Validation accuracy 0.492708\n",
      "Time elapsed 0.715472141901652 minutes\n",
      "Epoch 103, training loss 1.246357, accuracy 0.562500\n",
      "Validation accuracy 0.506250\n",
      "Time elapsed 0.7239262382189433 minutes\n",
      "Epoch 104, training loss 1.426147, accuracy 0.515625\n",
      "Validation accuracy 0.489583\n",
      "Time elapsed 0.7326700369517009 minutes\n",
      "Epoch 105, training loss 1.463163, accuracy 0.390625\n",
      "Validation accuracy 0.482292\n",
      "Time elapsed 0.7393215696016947 minutes\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/train',\n",
    "                                     sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/validation')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "cifar10.reset()\n",
    "print(\"Trainable variables\")\n",
    "for n in tf.trainable_variables():\n",
    "    print(n.name)\n",
    "\n",
    "epochs = 200\n",
    "mean_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "std_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "\n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "while cifar10.getEpoch() < epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    # just a training iteration\n",
    "    _ = sess.run(train_step,\n",
    "                feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "    \n",
    "    step = batch_idx+epoch*n_batches\n",
    "    \n",
    "    # Write training summary\n",
    "    if step%50==0:\n",
    "        summary = sess.run(merged,\n",
    "                          feed_dict={\n",
    "                model_input: batch_data,\n",
    "                target: batch_labels,\n",
    "                keep_prob: 0.5 # set to 1.0 at inference time\n",
    "            })\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "    # gradient (by layer) statistics over last training batch & validation summary\n",
    "    if batch_idx==0:\n",
    "        loss, acc, grads = sess.run((cross_entropy, accuracy, grads_vars), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "        for layer in range(len(tf.trainable_variables())):\n",
    "            mean_gradients[layer, epoch] = np.mean(np.abs(grads[layer][0]))\n",
    "            std_gradients[layer, epoch] = np.std(np.abs(grads[layer][0]))\n",
    "        print(\"Epoch %d, training loss %f, accuracy %f\" % (epoch, loss, acc))\n",
    "        \n",
    "        summary, validation_accuracy = validate()\n",
    "        validation_writer.add_summary(summary, step)\n",
    "        print(\"Validation accuracy %f\" % validation_accuracy)\n",
    "        print(\"Time elapsed\", (time.time()-t_i)/60.0, \"minutes\")\n",
    "train_writer.flush()\n",
    "validation_writer.flush()\n",
    "test_acc = test()\n",
    "print(\"Testing set accuracy %f\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting gradients\n",
    "n_layers = len(tf.trainable_variables()) // 2\n",
    "x = np.arange(epochs)\n",
    "i = 0\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Weights Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()\n",
    "i = 1\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Biases Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(targets, outputs):\n",
    "    '''Returns a confusion matrix. Both targets and outputs\n",
    "    should be 1-D arrays of zeros and ones.'''\n",
    "    encoded_data = 2*targets+outputs  # Map targets and outputs to {0, 1, 2, 3}\n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return ((TP, FP), (FN, TN))\n",
    "\n",
    "def roc_curve(targets, outputs):\n",
    "    '''Returns a ROC curve. Outputs should be in range 0-1\n",
    "    in order to move the threshold.'''\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        outputs_with_threshold = (outputs > threshold).astype(np.float)\n",
    "        ((TP, FP), (FN, TN)) = confusion_matrix(\n",
    "            targets, \n",
    "            outputs_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "        fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "accs = sess.run(accuracy,\n",
    "             feed_dict={\n",
    "            model_input: cifar10.training_data,\n",
    "                       target: cifar10.training_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "accs = np.asarray(accs)\n",
    "[[VN,FP],[FN,VP]] = confusion_matrix(\n",
    "    accs,\n",
    "    (accs>0.5).astype(np.float))\n",
    "print('VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN))\n",
    "print('Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN)))\n",
    "print('Precision: %%%f' %(100.0*VP/(VP+FP)))\n",
    "print('Recall: %%%f' %(100.0*VP/(VP+FN)))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
