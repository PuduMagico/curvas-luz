{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "\n",
    "from cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "batch_size = 64\n",
    "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False, file='data_original.csv')\n",
    "\n",
    "SUMMARIES_DIR = './summaries/convnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape, layer_name):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
    "                             initializer=tf.constant_initializer(0.05))\n",
    "    \n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    \n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, \n",
    "                       strides = [1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape, layer_name):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_input = tf.placeholder(tf.float32, name='model_input', \n",
    "                             shape=(batch_size,1,200,1))\n",
    "tf.summary.image('input', model_input, 10)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob', shape=())\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target', shape=(batch_size, 9))\n",
    "\n",
    " # Reshape tensor to MLP\n",
    "first_layer_input = tf.reshape(model_input, [-1,200], name='first_layer_input')\n",
    "# First layer\n",
    "layer_name = 'fc1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc1_out = fc_layer(first_layer_input, [200, 150], layer_name)\n",
    "fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "# Second layer\n",
    "layer_name = 'fc2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc2_out = fc_layer(fc1_out_drop, [150, 100],layer_name)\n",
    "fc2_out_drop = tf.nn.dropout(fc2_out, keep_prob)\n",
    "#Third Layer\n",
    "layer_name = 'fc3'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc3_out = fc_layer(fc2_out_drop, [100,50], layer_name)\n",
    "fc3_out_drop = tf.nn.dropout(fc3_out, keep_prob)\n",
    "\n",
    "layer_name = 'fc4'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc4_out = fc_layer(fc3_out_drop, [50,10], layer_name)\n",
    "fc4_out_drop = tf.nn.dropout(fc4_out, keep_prob)\n",
    "\n",
    "layer_name = 'fc5'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc5_out = fc_layer(fc4_out_drop, [10,9], layer_name)\n",
    "fc5_out_drop = tf.nn.dropout(fc5_out, keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d23b1eec0c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     cross_entropy = tf.reduce_mean(\n\u001b[0;32m----> 5\u001b[0;31m         tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                            name='cross_entropy'))\n\u001b[1;32m      7\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cross_entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_output' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.name_scope('loss_function'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n",
    "                                           name='cross_entropy'))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    grads_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(model_output, 1),\n",
    "                             tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Useful training functions\n",
    "def validate():\n",
    "    batches = cifar10.getValidationSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    summary = sess.run(merged,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "    return summary, mean_acc\n",
    "def test():\n",
    "    batches = cifar10.getTestSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/train',\n",
    "                                     sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/validation')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "cifar10.reset()\n",
    "print(\"Trainable variables\")\n",
    "for n in tf.trainable_variables():\n",
    "    print(n.name)\n",
    "\n",
    "epochs = 200\n",
    "mean_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "std_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "\n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "while cifar10.getEpoch() < epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    # just a training iteration\n",
    "    _ = sess.run(train_step,\n",
    "                feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "    \n",
    "    step = batch_idx+epoch*n_batches\n",
    "    \n",
    "    # Write training summary\n",
    "    if step%50==0:\n",
    "        summary = sess.run(merged,\n",
    "                          feed_dict={\n",
    "                model_input: batch_data,\n",
    "                target: batch_labels,\n",
    "                keep_prob: 0.5 # set to 1.0 at inference time\n",
    "            })\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "    # gradient (by layer) statistics over last training batch & validation summary\n",
    "    if batch_idx==0:\n",
    "        loss, acc, grads = sess.run((cross_entropy, accuracy, grads_vars), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "        for layer in range(len(tf.trainable_variables())):\n",
    "            mean_gradients[layer, epoch] = np.mean(np.abs(grads[layer][0]))\n",
    "            std_gradients[layer, epoch] = np.std(np.abs(grads[layer][0]))\n",
    "        print(\"Epoch %d, training loss %f, accuracy %f\" % (epoch, loss, acc))\n",
    "        \n",
    "        summary, validation_accuracy = validate()\n",
    "        validation_writer.add_summary(summary, step)\n",
    "        print(\"Validation accuracy %f\" % validation_accuracy)\n",
    "        print(\"Time elapsed\", (time.time()-t_i)/60.0, \"minutes\")\n",
    "train_writer.flush()\n",
    "validation_writer.flush()\n",
    "test_acc = test()\n",
    "print(\"Testing set accuracy %f\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting gradients\n",
    "n_layers = len(tf.trainable_variables()) // 2\n",
    "x = np.arange(epochs)\n",
    "i = 0\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Weights Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()\n",
    "i = 1\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Biases Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(targets, outputs):\n",
    "    '''Returns a confusion matrix. Both targets and outputs\n",
    "    should be 1-D arrays of zeros and ones.'''\n",
    "    encoded_data = 2*targets+outputs  # Map targets and outputs to {0, 1, 2, 3}\n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return ((TP, FP), (FN, TN))\n",
    "\n",
    "def roc_curve(targets, outputs):\n",
    "    '''Returns a ROC curve. Outputs should be in range 0-1\n",
    "    in order to move the threshold.'''\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        outputs_with_threshold = (outputs > threshold).astype(np.float)\n",
    "        ((TP, FP), (FN, TN)) = confusion_matrix(\n",
    "            targets, \n",
    "            outputs_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "        fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "accs = sess.run(accuracy,\n",
    "             feed_dict={\n",
    "            model_input: cifar10.training_data,\n",
    "                       target: cifar10.training_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "accs = np.asarray(accs)\n",
    "[[VN,FP],[FN,VP]] = confusion_matrix(\n",
    "    accs,\n",
    "    (accs>0.5).astype(np.float))\n",
    "print('VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN))\n",
    "print('Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN)))\n",
    "print('Precision: %%%f' %(100.0*VP/(VP+FP)))\n",
    "print('Recall: %%%f' %(100.0*VP/(VP+FN)))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
