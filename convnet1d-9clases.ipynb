{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "\n",
    "from cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "batch_size = 64\n",
    "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False, file='nuevos datos/data_original.csv')\n",
    "\n",
    "SUMMARIES_DIR = './summaries/convnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape, layer_name, stride = 1):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[2]],\n",
    "                             initializer=tf.constant_initializer(0.05))\n",
    "    \n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    \n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv1d(input_tensor, weights, \n",
    "                       stride = stride, padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape, layer_name):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_input = tf.placeholder(tf.float32, name='model_input', \n",
    "                             shape=(None,200,1))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob', shape=())\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target', shape=(None, 9))\n",
    "\n",
    "# First convolution layer\n",
    "layer_name = 'conv1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv1_out = conv_layer(model_input, [9, 1, 20], layer_name, 2)\n",
    "# First pooling layer\n",
    "with tf.name_scope('pool1'):\n",
    "    pool1_out = tf.nn.pool(conv1_out, window_shape=[2],\n",
    "                pooling_type=\"AVG\", padding='SAME', strides=[4],\n",
    "                name='pool1')\n",
    "    \n",
    "\n",
    "# Second convolution layer\n",
    "layer_name = 'conv2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv2_out = conv_layer(pool1_out, [5, 20, 20], layer_name)\n",
    "# Second pooling layer\n",
    "with tf.name_scope('pool2'):\n",
    "    pool2_out = tf.nn.pool(conv2_out, window_shape=[2],\n",
    "                pooling_type=\"AVG\", padding='SAME', strides=[1],\n",
    "                name='pool2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 20\n"
     ]
    }
   ],
   "source": [
    "_,dim3,dim4 = pool2_out.shape\n",
    "print(dim3,dim4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2_out_flat = tf.reshape(pool2_out, [-1, 1*dim3.value*dim4.value], name='pool2_flat')\n",
    "# First fully connected layer\n",
    "layer_name = 'fc4'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc1_out = fc_layer(pool2_out_flat, [1*dim3.value*dim4.value, 250], layer_name)\n",
    "fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "\n",
    "# Second fully connected layer\n",
    "layer_name = 'fc5'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc2_out = fc_layer(fc1_out_drop, [250, 100], layer_name)\n",
    "fc2_out_drop = tf.nn.dropout(fc2_out, keep_prob)\n",
    "\n",
    "\n",
    "# Third fully connected layer\n",
    "layer_name = 'fc6'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc3_out = fc_layer(fc2_out_drop, [100,9], layer_name)\n",
    "model_output = fc3_out\n",
    "\n",
    "\n",
    "with tf.name_scope('loss_function2'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n",
    "                                           name='cross_entropy'))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    grads_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(model_output, 1),\n",
    "                             tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Useful training functions\n",
    "def validate():\n",
    "    batches = cifar10.getValidationSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    summary = sess.run(merged,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "    return summary, mean_acc\n",
    "def test():\n",
    "    batches = cifar10.getTestSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: labels,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable variables\n",
      "conv1/weights:0\n",
      "conv1/biases:0\n",
      "conv2/weights:0\n",
      "conv2/biases:0\n",
      "fc4/weights:0\n",
      "fc4/biases:0\n",
      "fc5/weights:0\n",
      "fc5/biases:0\n",
      "fc6/weights:0\n",
      "fc6/biases:0\n",
      "Epoch 1200, training loss 0.461082, accuracy 0.843750\n",
      "Validation accuracy 0.724375\n",
      "Time elapsed 0.002430852254231771 minutes\n",
      "Epoch 1201, training loss 0.631895, accuracy 0.750000\n",
      "Validation accuracy 0.743750\n",
      "Time elapsed 0.06184319655100504 minutes\n",
      "Epoch 1202, training loss 0.725324, accuracy 0.718750\n",
      "Validation accuracy 0.726875\n",
      "Time elapsed 0.12970766226450603 minutes\n",
      "Epoch 1203, training loss 0.627038, accuracy 0.703125\n",
      "Validation accuracy 0.737500\n",
      "Time elapsed 0.17969552675882974 minutes\n",
      "Epoch 1204, training loss 0.607794, accuracy 0.765625\n",
      "Validation accuracy 0.731250\n",
      "Time elapsed 0.2412396510442098 minutes\n",
      "Epoch 1205, training loss 0.682555, accuracy 0.734375\n",
      "Validation accuracy 0.723750\n",
      "Time elapsed 0.29551306962966917 minutes\n",
      "Epoch 1206, training loss 0.507052, accuracy 0.812500\n",
      "Validation accuracy 0.715000\n",
      "Time elapsed 0.35877649784088134 minutes\n",
      "Epoch 1207, training loss 0.545541, accuracy 0.750000\n",
      "Validation accuracy 0.738125\n",
      "Time elapsed 0.42935507694880165 minutes\n",
      "Epoch 1208, training loss 0.693230, accuracy 0.734375\n",
      "Validation accuracy 0.739375\n",
      "Time elapsed 0.5076056996981303 minutes\n",
      "Epoch 1209, training loss 0.423828, accuracy 0.843750\n",
      "Validation accuracy 0.742500\n",
      "Time elapsed 0.5686016400655111 minutes\n",
      "Epoch 1210, training loss 0.518237, accuracy 0.812500\n",
      "Validation accuracy 0.733750\n",
      "Time elapsed 0.6360824664433797 minutes\n",
      "Epoch 1211, training loss 0.672968, accuracy 0.781250\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 0.6947774847348531 minutes\n",
      "Epoch 1212, training loss 0.609411, accuracy 0.812500\n",
      "Validation accuracy 0.721250\n",
      "Time elapsed 0.7670434236526489 minutes\n",
      "Epoch 1213, training loss 0.535719, accuracy 0.750000\n",
      "Validation accuracy 0.733750\n",
      "Time elapsed 0.8348095099131266 minutes\n",
      "Epoch 1214, training loss 0.568573, accuracy 0.687500\n",
      "Validation accuracy 0.723750\n",
      "Time elapsed 0.890435794989268 minutes\n",
      "Epoch 1215, training loss 0.700789, accuracy 0.687500\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 0.953231692314148 minutes\n",
      "Epoch 1216, training loss 0.577668, accuracy 0.812500\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 1.0074575940767925 minutes\n",
      "Epoch 1217, training loss 0.447809, accuracy 0.796875\n",
      "Validation accuracy 0.737500\n",
      "Time elapsed 1.0555083274841308 minutes\n",
      "Epoch 1218, training loss 0.568722, accuracy 0.703125\n",
      "Validation accuracy 0.728125\n",
      "Time elapsed 1.1062432448069255 minutes\n",
      "Epoch 1219, training loss 0.575776, accuracy 0.765625\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 1.1634084900220236 minutes\n",
      "Epoch 1220, training loss 0.675965, accuracy 0.640625\n",
      "Validation accuracy 0.742500\n",
      "Time elapsed 1.216917375723521 minutes\n",
      "Epoch 1221, training loss 0.702721, accuracy 0.687500\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 1.2714817881584168 minutes\n",
      "Epoch 1222, training loss 0.667029, accuracy 0.718750\n",
      "Validation accuracy 0.743125\n",
      "Time elapsed 1.3329113086064657 minutes\n",
      "Epoch 1223, training loss 0.488616, accuracy 0.812500\n",
      "Validation accuracy 0.742500\n",
      "Time elapsed 1.3928115487098693 minutes\n",
      "Epoch 1224, training loss 0.660477, accuracy 0.750000\n",
      "Validation accuracy 0.716250\n",
      "Time elapsed 1.4564239184061687 minutes\n",
      "Epoch 1225, training loss 0.551833, accuracy 0.734375\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 1.5130013306935628 minutes\n",
      "Epoch 1226, training loss 0.482775, accuracy 0.859375\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 1.5742249766985574 minutes\n",
      "Epoch 1227, training loss 0.618013, accuracy 0.703125\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 1.6378645022710165 minutes\n",
      "Epoch 1228, training loss 0.689579, accuracy 0.765625\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 1.6955190976460774 minutes\n",
      "Epoch 1229, training loss 0.624750, accuracy 0.750000\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 1.7554308573404949 minutes\n",
      "Epoch 1230, training loss 0.554980, accuracy 0.765625\n",
      "Validation accuracy 0.719375\n",
      "Time elapsed 1.8229365626970926 minutes\n",
      "Epoch 1231, training loss 0.557578, accuracy 0.781250\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 1.8897183934847515 minutes\n",
      "Epoch 1232, training loss 0.536554, accuracy 0.796875\n",
      "Validation accuracy 0.743125\n",
      "Time elapsed 1.9631120204925536 minutes\n",
      "Epoch 1233, training loss 0.616028, accuracy 0.765625\n",
      "Validation accuracy 0.721250\n",
      "Time elapsed 2.043140172958374 minutes\n",
      "Epoch 1234, training loss 0.747530, accuracy 0.750000\n",
      "Validation accuracy 0.737500\n",
      "Time elapsed 2.1096581975618998 minutes\n",
      "Epoch 1235, training loss 0.635269, accuracy 0.687500\n",
      "Validation accuracy 0.731250\n",
      "Time elapsed 2.1767226020495096 minutes\n",
      "Epoch 1236, training loss 0.624278, accuracy 0.656250\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 2.2393954833348593 minutes\n",
      "Epoch 1237, training loss 0.624279, accuracy 0.734375\n",
      "Validation accuracy 0.733750\n",
      "Time elapsed 2.2941402912139894 minutes\n",
      "Epoch 1238, training loss 0.606849, accuracy 0.734375\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 2.347988180319468 minutes\n",
      "Epoch 1239, training loss 0.497033, accuracy 0.828125\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 2.412873307863871 minutes\n",
      "Epoch 1240, training loss 0.493280, accuracy 0.750000\n",
      "Validation accuracy 0.741875\n",
      "Time elapsed 2.4677082896232605 minutes\n",
      "Epoch 1241, training loss 0.458117, accuracy 0.859375\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 2.527237804730733 minutes\n",
      "Epoch 1242, training loss 0.688007, accuracy 0.687500\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 2.581699852148692 minutes\n",
      "Epoch 1243, training loss 0.756920, accuracy 0.656250\n",
      "Validation accuracy 0.738125\n",
      "Time elapsed 2.6360604723294574 minutes\n",
      "Epoch 1244, training loss 0.538519, accuracy 0.765625\n",
      "Validation accuracy 0.736875\n",
      "Time elapsed 2.689481715361277 minutes\n",
      "Epoch 1245, training loss 0.493053, accuracy 0.796875\n",
      "Validation accuracy 0.733750\n",
      "Time elapsed 2.744187581539154 minutes\n",
      "Epoch 1246, training loss 0.648898, accuracy 0.781250\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 2.8056959390640257 minutes\n",
      "Epoch 1247, training loss 0.449928, accuracy 0.859375\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 2.8682432889938356 minutes\n",
      "Epoch 1248, training loss 0.476272, accuracy 0.796875\n",
      "Validation accuracy 0.738125\n",
      "Time elapsed 2.930190594991048 minutes\n",
      "Epoch 1249, training loss 0.506328, accuracy 0.812500\n",
      "Validation accuracy 0.731250\n",
      "Time elapsed 2.9835973143577577 minutes\n",
      "Epoch 1250, training loss 0.474595, accuracy 0.859375\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 3.035354538758596 minutes\n",
      "Epoch 1251, training loss 0.376342, accuracy 0.859375\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 3.08854710261027 minutes\n",
      "Epoch 1252, training loss 0.586159, accuracy 0.750000\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 3.140782117843628 minutes\n",
      "Epoch 1253, training loss 0.640302, accuracy 0.671875\n",
      "Validation accuracy 0.730000\n",
      "Time elapsed 3.1930029312769572 minutes\n",
      "Epoch 1254, training loss 0.506879, accuracy 0.812500\n",
      "Validation accuracy 0.730000\n",
      "Time elapsed 3.244344933827718 minutes\n",
      "Epoch 1255, training loss 0.476303, accuracy 0.890625\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 3.2963428338368734 minutes\n",
      "Epoch 1256, training loss 0.513360, accuracy 0.765625\n",
      "Validation accuracy 0.738125\n",
      "Time elapsed 3.3542876839637756 minutes\n",
      "Epoch 1257, training loss 0.585495, accuracy 0.734375\n",
      "Validation accuracy 0.740625\n",
      "Time elapsed 3.4081356287002564 minutes\n",
      "Epoch 1258, training loss 0.644883, accuracy 0.765625\n",
      "Validation accuracy 0.738750\n",
      "Time elapsed 3.457224937280019 minutes\n",
      "Epoch 1259, training loss 0.414742, accuracy 0.796875\n",
      "Validation accuracy 0.730000\n",
      "Time elapsed 3.5072451273600262 minutes\n",
      "Epoch 1260, training loss 0.358686, accuracy 0.890625\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 3.5623219927151997 minutes\n",
      "Epoch 1261, training loss 0.667839, accuracy 0.718750\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 3.613055427869161 minutes\n",
      "Epoch 1262, training loss 0.586217, accuracy 0.734375\n",
      "Validation accuracy 0.741250\n",
      "Time elapsed 3.6619938453038534 minutes\n",
      "Epoch 1263, training loss 0.534469, accuracy 0.828125\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 3.713447602589925 minutes\n",
      "Epoch 1264, training loss 0.551342, accuracy 0.796875\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 3.7618096113204955 minutes\n",
      "Epoch 1265, training loss 0.503913, accuracy 0.828125\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 3.8237383643786114 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1266, training loss 0.543616, accuracy 0.781250\n",
      "Validation accuracy 0.738750\n",
      "Time elapsed 3.879580601056417 minutes\n",
      "Epoch 1267, training loss 0.538819, accuracy 0.796875\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 3.9320542097091673 minutes\n",
      "Epoch 1268, training loss 0.682319, accuracy 0.687500\n",
      "Validation accuracy 0.743750\n",
      "Time elapsed 3.9854349295298257 minutes\n",
      "Epoch 1269, training loss 0.448792, accuracy 0.843750\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 4.037633621692658 minutes\n",
      "Epoch 1270, training loss 0.444351, accuracy 0.828125\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 4.087681686878204 minutes\n",
      "Epoch 1271, training loss 0.415542, accuracy 0.859375\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 4.137818086147308 minutes\n",
      "Epoch 1272, training loss 0.698322, accuracy 0.703125\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 4.189967699845632 minutes\n",
      "Epoch 1273, training loss 0.815866, accuracy 0.703125\n",
      "Validation accuracy 0.715625\n",
      "Time elapsed 4.244710969924927 minutes\n",
      "Epoch 1274, training loss 0.592062, accuracy 0.765625\n",
      "Validation accuracy 0.723125\n",
      "Time elapsed 4.302719004948934 minutes\n",
      "Epoch 1275, training loss 0.482353, accuracy 0.859375\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 4.3544052561124165 minutes\n",
      "Epoch 1276, training loss 0.564612, accuracy 0.750000\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 4.406928904851278 minutes\n",
      "Epoch 1277, training loss 0.596247, accuracy 0.750000\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 4.458473547299703 minutes\n",
      "Epoch 1278, training loss 0.432374, accuracy 0.828125\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 4.509670873483022 minutes\n",
      "Epoch 1279, training loss 0.496764, accuracy 0.781250\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 4.560339391231537 minutes\n",
      "Epoch 1280, training loss 0.565458, accuracy 0.781250\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 4.6108905156453455 minutes\n",
      "Epoch 1281, training loss 0.456455, accuracy 0.828125\n",
      "Validation accuracy 0.740000\n",
      "Time elapsed 4.661085573832194 minutes\n",
      "Epoch 1282, training loss 0.520964, accuracy 0.765625\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 4.711242802937826 minutes\n",
      "Epoch 1283, training loss 0.547837, accuracy 0.781250\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 4.761912520726522 minutes\n",
      "Epoch 1284, training loss 0.534370, accuracy 0.765625\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 4.813449116547902 minutes\n",
      "Epoch 1285, training loss 0.530881, accuracy 0.750000\n",
      "Validation accuracy 0.742500\n",
      "Time elapsed 4.8650848984718325 minutes\n",
      "Epoch 1286, training loss 0.603941, accuracy 0.734375\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 4.91793460448583 minutes\n",
      "Epoch 1287, training loss 0.690343, accuracy 0.687500\n",
      "Validation accuracy 0.737500\n",
      "Time elapsed 4.9688119451204935 minutes\n",
      "Epoch 1288, training loss 0.548356, accuracy 0.750000\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 5.027381086349488 minutes\n",
      "Epoch 1289, training loss 0.515614, accuracy 0.796875\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 5.081611835956574 minutes\n",
      "Epoch 1290, training loss 0.599666, accuracy 0.781250\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 5.146249969800313 minutes\n",
      "Epoch 1291, training loss 0.457697, accuracy 0.796875\n",
      "Validation accuracy 0.743750\n",
      "Time elapsed 5.203194801012675 minutes\n",
      "Epoch 1292, training loss 0.513884, accuracy 0.828125\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 5.2528422276179 minutes\n",
      "Epoch 1293, training loss 0.487423, accuracy 0.750000\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 5.308870279788971 minutes\n",
      "Epoch 1294, training loss 0.669859, accuracy 0.781250\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 5.367689768473308 minutes\n",
      "Epoch 1295, training loss 0.673175, accuracy 0.718750\n",
      "Validation accuracy 0.731875\n",
      "Time elapsed 5.434702396392822 minutes\n",
      "Epoch 1296, training loss 0.608748, accuracy 0.765625\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 5.48748406569163 minutes\n",
      "Epoch 1297, training loss 0.486464, accuracy 0.687500\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 5.542236014207204 minutes\n",
      "Epoch 1298, training loss 0.780840, accuracy 0.687500\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 5.594566563765208 minutes\n",
      "Epoch 1299, training loss 0.655236, accuracy 0.750000\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 5.648297739028931 minutes\n",
      "Epoch 1300, training loss 0.648958, accuracy 0.687500\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 5.702007015546163 minutes\n",
      "Epoch 1301, training loss 0.738831, accuracy 0.640625\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 5.7554376800855005 minutes\n",
      "Epoch 1302, training loss 0.585536, accuracy 0.718750\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 5.805970219771067 minutes\n",
      "Epoch 1303, training loss 0.530324, accuracy 0.796875\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 5.85567075808843 minutes\n",
      "Epoch 1304, training loss 0.619890, accuracy 0.703125\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 5.911698730786641 minutes\n",
      "Epoch 1305, training loss 0.471126, accuracy 0.765625\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 5.962522264321645 minutes\n",
      "Epoch 1306, training loss 0.488724, accuracy 0.796875\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 6.013722240924835 minutes\n",
      "Epoch 1307, training loss 0.624201, accuracy 0.781250\n",
      "Validation accuracy 0.738750\n",
      "Time elapsed 6.064877518018087 minutes\n",
      "Epoch 1308, training loss 0.634318, accuracy 0.671875\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 6.116693790753683 minutes\n",
      "Epoch 1309, training loss 0.520156, accuracy 0.812500\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 6.178145968914032 minutes\n",
      "Epoch 1310, training loss 0.636880, accuracy 0.750000\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 6.271476336320242 minutes\n",
      "Epoch 1311, training loss 0.662262, accuracy 0.734375\n",
      "Validation accuracy 0.732500\n",
      "Time elapsed 6.349341360727946 minutes\n",
      "Epoch 1312, training loss 0.600614, accuracy 0.765625\n",
      "Validation accuracy 0.743750\n",
      "Time elapsed 6.411453270912171 minutes\n",
      "Epoch 1313, training loss 0.583203, accuracy 0.812500\n",
      "Validation accuracy 0.740000\n",
      "Time elapsed 6.464019580682119 minutes\n",
      "Epoch 1314, training loss 0.728820, accuracy 0.687500\n",
      "Validation accuracy 0.741250\n",
      "Time elapsed 6.514613628387451 minutes\n",
      "Epoch 1315, training loss 0.573944, accuracy 0.765625\n",
      "Validation accuracy 0.723750\n",
      "Time elapsed 6.565939847628275 minutes\n",
      "Epoch 1316, training loss 0.466178, accuracy 0.843750\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 6.61568656762441 minutes\n",
      "Epoch 1317, training loss 0.544025, accuracy 0.843750\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 6.6662297209103905 minutes\n",
      "Epoch 1318, training loss 0.535335, accuracy 0.828125\n",
      "Validation accuracy 0.733125\n",
      "Time elapsed 6.716027649243673 minutes\n",
      "Epoch 1319, training loss 0.682831, accuracy 0.671875\n",
      "Validation accuracy 0.743125\n",
      "Time elapsed 6.7664684375127155 minutes\n",
      "Epoch 1320, training loss 0.445856, accuracy 0.828125\n",
      "Validation accuracy 0.736250\n",
      "Time elapsed 6.817206752300263 minutes\n",
      "Epoch 1321, training loss 0.687153, accuracy 0.781250\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 6.868336816628774 minutes\n",
      "Epoch 1322, training loss 0.580951, accuracy 0.796875\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 6.92136044104894 minutes\n",
      "Epoch 1323, training loss 0.566059, accuracy 0.765625\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 6.97218824227651 minutes\n",
      "Epoch 1324, training loss 0.736104, accuracy 0.703125\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 7.022308838367462 minutes\n",
      "Epoch 1325, training loss 0.571142, accuracy 0.796875\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 7.07547812461853 minutes\n",
      "Epoch 1326, training loss 0.438484, accuracy 0.859375\n",
      "Validation accuracy 0.736250\n",
      "Time elapsed 7.134872206052145 minutes\n",
      "Epoch 1327, training loss 0.511546, accuracy 0.812500\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 7.185250210762024 minutes\n",
      "Epoch 1328, training loss 0.599078, accuracy 0.812500\n",
      "Validation accuracy 0.698750\n",
      "Time elapsed 7.236648523807526 minutes\n",
      "Epoch 1329, training loss 0.649760, accuracy 0.750000\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 7.305633695920308 minutes\n",
      "Epoch 1330, training loss 0.460248, accuracy 0.796875\n",
      "Validation accuracy 0.731250\n",
      "Time elapsed 7.382604706287384 minutes\n",
      "Epoch 1331, training loss 0.517702, accuracy 0.750000\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 7.4590879281361895 minutes\n",
      "Epoch 1332, training loss 0.414701, accuracy 0.859375\n",
      "Validation accuracy 0.737500\n",
      "Time elapsed 7.54803718328476 minutes\n",
      "Epoch 1333, training loss 0.572409, accuracy 0.718750\n",
      "Validation accuracy 0.742500\n",
      "Time elapsed 7.627372626463572 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1334, training loss 0.685782, accuracy 0.703125\n",
      "Validation accuracy 0.737500\n",
      "Time elapsed 7.683741847674052 minutes\n",
      "Epoch 1335, training loss 0.517610, accuracy 0.781250\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 7.736990952491761 minutes\n",
      "Epoch 1336, training loss 0.484837, accuracy 0.796875\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 7.787915297349294 minutes\n",
      "Epoch 1337, training loss 0.599268, accuracy 0.703125\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 7.843719287713369 minutes\n",
      "Epoch 1338, training loss 0.602028, accuracy 0.796875\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 7.905370422204336 minutes\n",
      "Epoch 1339, training loss 0.524864, accuracy 0.812500\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 7.963917605082194 minutes\n",
      "Epoch 1340, training loss 0.576786, accuracy 0.828125\n",
      "Validation accuracy 0.741250\n",
      "Time elapsed 8.021491142114003 minutes\n",
      "Epoch 1341, training loss 0.572862, accuracy 0.812500\n",
      "Validation accuracy 0.740625\n",
      "Time elapsed 8.076841127872466 minutes\n",
      "Epoch 1342, training loss 0.411607, accuracy 0.937500\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 8.130513640244802 minutes\n",
      "Epoch 1343, training loss 0.514901, accuracy 0.765625\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 8.184718990325928 minutes\n",
      "Epoch 1344, training loss 0.696495, accuracy 0.765625\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 8.237680991490683 minutes\n",
      "Epoch 1345, training loss 0.615171, accuracy 0.718750\n",
      "Validation accuracy 0.740000\n",
      "Time elapsed 8.287891900539398 minutes\n",
      "Epoch 1346, training loss 0.594828, accuracy 0.734375\n",
      "Validation accuracy 0.733750\n",
      "Time elapsed 8.341091748078664 minutes\n",
      "Epoch 1347, training loss 0.521972, accuracy 0.828125\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 8.397536214192709 minutes\n",
      "Epoch 1348, training loss 0.462143, accuracy 0.781250\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 8.449390653769175 minutes\n",
      "Epoch 1349, training loss 0.516167, accuracy 0.781250\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 8.502833620707195 minutes\n",
      "Epoch 1350, training loss 0.505466, accuracy 0.796875\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 8.563565361499787 minutes\n",
      "Epoch 1351, training loss 0.599100, accuracy 0.781250\n",
      "Validation accuracy 0.737500\n",
      "Time elapsed 8.617265582084656 minutes\n",
      "Epoch 1352, training loss 0.575096, accuracy 0.781250\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 8.669090795516968 minutes\n",
      "Epoch 1353, training loss 0.581116, accuracy 0.703125\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 8.725032277901967 minutes\n",
      "Epoch 1354, training loss 0.492159, accuracy 0.812500\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 8.780782024065653 minutes\n",
      "Epoch 1355, training loss 0.534850, accuracy 0.781250\n",
      "Validation accuracy 0.740625\n",
      "Time elapsed 8.839874358971914 minutes\n",
      "Epoch 1356, training loss 0.491019, accuracy 0.843750\n",
      "Validation accuracy 0.736875\n",
      "Time elapsed 8.901586608091991 minutes\n",
      "Epoch 1357, training loss 0.560507, accuracy 0.765625\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 8.955110057195027 minutes\n",
      "Epoch 1358, training loss 0.609861, accuracy 0.781250\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 9.003421354293824 minutes\n",
      "Epoch 1359, training loss 0.533155, accuracy 0.781250\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 9.058777836958567 minutes\n",
      "Epoch 1360, training loss 0.441064, accuracy 0.828125\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 9.110379433631897 minutes\n",
      "Epoch 1361, training loss 0.629858, accuracy 0.765625\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 9.160129702091217 minutes\n",
      "Epoch 1362, training loss 0.680302, accuracy 0.703125\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 9.210176916917165 minutes\n",
      "Epoch 1363, training loss 0.494855, accuracy 0.812500\n",
      "Validation accuracy 0.735625\n",
      "Time elapsed 9.259042696158092 minutes\n",
      "Epoch 1364, training loss 0.631255, accuracy 0.750000\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 9.306763656934102 minutes\n",
      "Epoch 1365, training loss 0.455477, accuracy 0.828125\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 9.352786175409953 minutes\n",
      "Epoch 1366, training loss 0.616185, accuracy 0.765625\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 9.40360556046168 minutes\n",
      "Epoch 1367, training loss 0.440235, accuracy 0.796875\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 9.452041880289714 minutes\n",
      "Epoch 1368, training loss 0.511732, accuracy 0.781250\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 9.499844952424366 minutes\n",
      "Epoch 1369, training loss 0.583415, accuracy 0.750000\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 9.547695895036062 minutes\n",
      "Epoch 1370, training loss 0.603176, accuracy 0.796875\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 9.597516651948293 minutes\n",
      "Epoch 1371, training loss 0.582753, accuracy 0.750000\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 9.646356761455536 minutes\n",
      "Epoch 1372, training loss 0.697836, accuracy 0.718750\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 9.69175789753596 minutes\n",
      "Epoch 1373, training loss 0.547562, accuracy 0.812500\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 9.737869548797608 minutes\n",
      "Epoch 1374, training loss 0.561905, accuracy 0.734375\n",
      "Validation accuracy 0.738750\n",
      "Time elapsed 9.783395071824392 minutes\n",
      "Epoch 1375, training loss 0.566261, accuracy 0.765625\n",
      "Validation accuracy 0.743750\n",
      "Time elapsed 9.82948408126831 minutes\n",
      "Epoch 1376, training loss 0.641830, accuracy 0.671875\n",
      "Validation accuracy 0.711250\n",
      "Time elapsed 9.876995853583018 minutes\n",
      "Epoch 1377, training loss 0.669147, accuracy 0.718750\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 9.924201464653015 minutes\n",
      "Epoch 1378, training loss 0.560420, accuracy 0.781250\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 9.97032854159673 minutes\n",
      "Epoch 1379, training loss 0.510566, accuracy 0.796875\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 10.019121066729229 minutes\n",
      "Epoch 1380, training loss 0.561731, accuracy 0.781250\n",
      "Validation accuracy 0.743750\n",
      "Time elapsed 10.068209628264109 minutes\n",
      "Epoch 1381, training loss 0.516464, accuracy 0.781250\n",
      "Validation accuracy 0.735000\n",
      "Time elapsed 10.116475474834441 minutes\n",
      "Epoch 1382, training loss 0.760042, accuracy 0.656250\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 10.162876085440319 minutes\n",
      "Epoch 1383, training loss 0.479067, accuracy 0.781250\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 10.208428792158763 minutes\n",
      "Epoch 1384, training loss 0.629845, accuracy 0.656250\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 10.257122989495596 minutes\n",
      "Epoch 1385, training loss 0.677841, accuracy 0.703125\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 10.306690808137258 minutes\n",
      "Epoch 1386, training loss 0.385349, accuracy 0.890625\n",
      "Validation accuracy 0.738750\n",
      "Time elapsed 10.355382641156515 minutes\n",
      "Epoch 1387, training loss 0.546024, accuracy 0.765625\n",
      "Validation accuracy 0.732500\n",
      "Time elapsed 10.405936900774638 minutes\n",
      "Epoch 1388, training loss 0.555642, accuracy 0.781250\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 10.455129249890645 minutes\n",
      "Epoch 1389, training loss 0.519035, accuracy 0.796875\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 10.504189844926199 minutes\n",
      "Epoch 1390, training loss 0.553762, accuracy 0.859375\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 10.556250035762787 minutes\n",
      "Epoch 1391, training loss 0.650057, accuracy 0.718750\n",
      "Validation accuracy 0.738750\n",
      "Time elapsed 10.603675480683645 minutes\n",
      "Epoch 1392, training loss 0.413232, accuracy 0.875000\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 10.649623378117878 minutes\n",
      "Epoch 1393, training loss 0.634909, accuracy 0.781250\n",
      "Validation accuracy 0.738750\n",
      "Time elapsed 10.698435366153717 minutes\n",
      "Epoch 1394, training loss 0.489697, accuracy 0.875000\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 10.746667289733887 minutes\n",
      "Epoch 1395, training loss 0.770795, accuracy 0.687500\n",
      "Validation accuracy 0.691250\n",
      "Time elapsed 10.794524776935578 minutes\n",
      "Epoch 1396, training loss 0.534097, accuracy 0.796875\n",
      "Validation accuracy 0.739375\n",
      "Time elapsed 10.842632420857747 minutes\n",
      "Epoch 1397, training loss 0.392537, accuracy 0.843750\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 10.892203732331593 minutes\n",
      "Epoch 1398, training loss 0.461245, accuracy 0.828125\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 10.94807833035787 minutes\n",
      "Epoch 1399, training loss 0.571430, accuracy 0.796875\n",
      "Validation accuracy 0.725625\n",
      "Time elapsed 10.997773059209187 minutes\n",
      "Epoch 1400, training loss 0.617800, accuracy 0.718750\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 11.04642848968506 minutes\n",
      "Epoch 1401, training loss 0.469479, accuracy 0.828125\n",
      "Validation accuracy 0.730625\n",
      "Time elapsed 11.099599957466125 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1402, training loss 0.643589, accuracy 0.781250\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 11.146793540318807 minutes\n",
      "Epoch 1403, training loss 0.456371, accuracy 0.781250\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 11.194048941135406 minutes\n",
      "Epoch 1404, training loss 0.686668, accuracy 0.781250\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 11.241361447175343 minutes\n",
      "Epoch 1405, training loss 0.475331, accuracy 0.812500\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 11.28768057823181 minutes\n",
      "Epoch 1406, training loss 0.456743, accuracy 0.890625\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 11.335357979933422 minutes\n",
      "Epoch 1407, training loss 0.475607, accuracy 0.781250\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 11.385027106602987 minutes\n",
      "Epoch 1408, training loss 0.511314, accuracy 0.812500\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 11.444195814927419 minutes\n",
      "Epoch 1409, training loss 0.578772, accuracy 0.734375\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 11.495386906464894 minutes\n",
      "Epoch 1410, training loss 0.526014, accuracy 0.796875\n",
      "Validation accuracy 0.741250\n",
      "Time elapsed 11.543793487548829 minutes\n",
      "Epoch 1411, training loss 0.499023, accuracy 0.765625\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 11.59345765511195 minutes\n",
      "Epoch 1412, training loss 0.586169, accuracy 0.781250\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 11.641578642527262 minutes\n",
      "Epoch 1413, training loss 0.419381, accuracy 0.875000\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 11.689735345045726 minutes\n",
      "Epoch 1414, training loss 0.543836, accuracy 0.750000\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 11.737053469816844 minutes\n",
      "Epoch 1415, training loss 0.455636, accuracy 0.875000\n",
      "Validation accuracy 0.713750\n",
      "Time elapsed 11.784949644406636 minutes\n",
      "Epoch 1416, training loss 0.630497, accuracy 0.765625\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 11.834177581469218 minutes\n",
      "Epoch 1417, training loss 0.567160, accuracy 0.734375\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 11.882827647527058 minutes\n",
      "Epoch 1418, training loss 0.379320, accuracy 0.859375\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 11.930850378672282 minutes\n",
      "Epoch 1419, training loss 0.695165, accuracy 0.734375\n",
      "Validation accuracy 0.743125\n",
      "Time elapsed 11.97921922604243 minutes\n",
      "Epoch 1420, training loss 0.576576, accuracy 0.750000\n",
      "Validation accuracy 0.738750\n",
      "Time elapsed 12.026957551638285 minutes\n",
      "Epoch 1421, training loss 0.555263, accuracy 0.875000\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 12.080012194315593 minutes\n",
      "Epoch 1422, training loss 0.578420, accuracy 0.796875\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 12.129506003856658 minutes\n",
      "Epoch 1423, training loss 0.465002, accuracy 0.750000\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 12.18047920068105 minutes\n",
      "Epoch 1424, training loss 0.661401, accuracy 0.750000\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 12.228409179051717 minutes\n",
      "Epoch 1425, training loss 0.533880, accuracy 0.828125\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 12.278518712520599 minutes\n",
      "Epoch 1426, training loss 0.508110, accuracy 0.781250\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 12.327059094111124 minutes\n",
      "Epoch 1427, training loss 0.719626, accuracy 0.687500\n",
      "Validation accuracy 0.719375\n",
      "Time elapsed 12.377179078261058 minutes\n",
      "Epoch 1428, training loss 0.637992, accuracy 0.750000\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 12.425797478357952 minutes\n",
      "Epoch 1429, training loss 0.525334, accuracy 0.765625\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 12.474104821681976 minutes\n",
      "Epoch 1430, training loss 0.515796, accuracy 0.781250\n",
      "Validation accuracy 0.726250\n",
      "Time elapsed 12.522517613569896 minutes\n",
      "Epoch 1431, training loss 0.527267, accuracy 0.796875\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 12.569872919718424 minutes\n",
      "Epoch 1432, training loss 0.720189, accuracy 0.750000\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 12.61675077676773 minutes\n",
      "Epoch 1433, training loss 0.383930, accuracy 0.875000\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 12.665920583407084 minutes\n",
      "Epoch 1434, training loss 0.441102, accuracy 0.812500\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 12.716364773114522 minutes\n",
      "Epoch 1435, training loss 0.464481, accuracy 0.796875\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 12.763580898443857 minutes\n",
      "Epoch 1436, training loss 0.606506, accuracy 0.765625\n",
      "Validation accuracy 0.741250\n",
      "Time elapsed 12.81133428812027 minutes\n",
      "Epoch 1437, training loss 0.566591, accuracy 0.812500\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 12.859292038281758 minutes\n",
      "Epoch 1438, training loss 0.684350, accuracy 0.656250\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 12.906348888079325 minutes\n",
      "Epoch 1439, training loss 0.577706, accuracy 0.750000\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 12.953801083564759 minutes\n",
      "Epoch 1440, training loss 0.387158, accuracy 0.890625\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 13.004937819639842 minutes\n",
      "Epoch 1441, training loss 0.443397, accuracy 0.796875\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 13.053712344169616 minutes\n",
      "Epoch 1442, training loss 0.393400, accuracy 0.812500\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 13.105638786156971 minutes\n",
      "Epoch 1443, training loss 0.698101, accuracy 0.750000\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 13.152768544356029 minutes\n",
      "Epoch 1444, training loss 0.471835, accuracy 0.812500\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 13.20096406141917 minutes\n",
      "Epoch 1445, training loss 0.715456, accuracy 0.687500\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 13.250019578138987 minutes\n",
      "Epoch 1446, training loss 0.430824, accuracy 0.843750\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 13.297290547688801 minutes\n",
      "Epoch 1447, training loss 0.434958, accuracy 0.843750\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 13.34866731564204 minutes\n",
      "Epoch 1448, training loss 0.470208, accuracy 0.828125\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 13.39803591966629 minutes\n",
      "Epoch 1449, training loss 0.375991, accuracy 0.859375\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 13.446901027361552 minutes\n",
      "Epoch 1450, training loss 0.558011, accuracy 0.765625\n",
      "Validation accuracy 0.739375\n",
      "Time elapsed 13.4991383711497 minutes\n",
      "Epoch 1451, training loss 0.439051, accuracy 0.828125\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 13.549583125114442 minutes\n",
      "Epoch 1452, training loss 0.595737, accuracy 0.734375\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 13.601968864599863 minutes\n",
      "Epoch 1453, training loss 0.604903, accuracy 0.718750\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 13.65132436354955 minutes\n",
      "Epoch 1454, training loss 0.585764, accuracy 0.718750\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 13.698874374230703 minutes\n",
      "Epoch 1455, training loss 0.629973, accuracy 0.703125\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 13.747142330805461 minutes\n",
      "Epoch 1456, training loss 0.627800, accuracy 0.765625\n",
      "Validation accuracy 0.710625\n",
      "Time elapsed 13.792662040392559 minutes\n",
      "Epoch 1457, training loss 0.467610, accuracy 0.812500\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 13.839666175842286 minutes\n",
      "Epoch 1458, training loss 0.802633, accuracy 0.703125\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 13.892899247010549 minutes\n",
      "Epoch 1459, training loss 0.722895, accuracy 0.671875\n",
      "Validation accuracy 0.735625\n",
      "Time elapsed 13.941497910022736 minutes\n",
      "Epoch 1460, training loss 0.522515, accuracy 0.796875\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 13.987928275267283 minutes\n",
      "Epoch 1461, training loss 0.710622, accuracy 0.750000\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 14.03609071969986 minutes\n",
      "Epoch 1462, training loss 0.572366, accuracy 0.718750\n",
      "Validation accuracy 0.740000\n",
      "Time elapsed 14.08540522257487 minutes\n",
      "Epoch 1463, training loss 0.647749, accuracy 0.671875\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 14.133319294452667 minutes\n",
      "Epoch 1464, training loss 0.586543, accuracy 0.796875\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 14.191021239757537 minutes\n",
      "Epoch 1465, training loss 0.688870, accuracy 0.718750\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 14.261615618069966 minutes\n",
      "Epoch 1466, training loss 0.542684, accuracy 0.687500\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 14.31377238035202 minutes\n",
      "Epoch 1467, training loss 0.573983, accuracy 0.796875\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 14.366378625233969 minutes\n",
      "Epoch 1468, training loss 0.399315, accuracy 0.859375\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 14.427185972531637 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1469, training loss 0.422933, accuracy 0.875000\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 14.491145237286885 minutes\n",
      "Epoch 1470, training loss 0.524739, accuracy 0.796875\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 14.545446729660034 minutes\n",
      "Epoch 1471, training loss 0.530988, accuracy 0.796875\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 14.60259931087494 minutes\n",
      "Epoch 1472, training loss 0.564919, accuracy 0.750000\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 14.66175430615743 minutes\n",
      "Epoch 1473, training loss 0.704349, accuracy 0.703125\n",
      "Validation accuracy 0.727500\n",
      "Time elapsed 14.71601756811142 minutes\n",
      "Epoch 1474, training loss 0.540713, accuracy 0.796875\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 14.767553019523621 minutes\n",
      "Epoch 1475, training loss 0.621957, accuracy 0.703125\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 14.818532148996988 minutes\n",
      "Epoch 1476, training loss 0.525817, accuracy 0.796875\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 14.869989450772604 minutes\n",
      "Epoch 1477, training loss 0.661955, accuracy 0.734375\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 14.92713105281194 minutes\n",
      "Epoch 1478, training loss 0.630203, accuracy 0.765625\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 14.985888238747915 minutes\n",
      "Epoch 1479, training loss 0.589853, accuracy 0.765625\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 15.038615568478901 minutes\n",
      "Epoch 1480, training loss 0.530882, accuracy 0.828125\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 15.091170775890351 minutes\n",
      "Epoch 1481, training loss 0.547339, accuracy 0.781250\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 15.14454452196757 minutes\n",
      "Epoch 1482, training loss 0.533030, accuracy 0.796875\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 15.197034406661988 minutes\n",
      "Epoch 1483, training loss 0.527532, accuracy 0.765625\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 15.253202537695566 minutes\n",
      "Epoch 1484, training loss 0.538989, accuracy 0.843750\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 15.323546183109283 minutes\n",
      "Epoch 1485, training loss 0.609134, accuracy 0.765625\n",
      "Validation accuracy 0.728125\n",
      "Time elapsed 15.398155244191488 minutes\n",
      "Epoch 1486, training loss 0.729550, accuracy 0.734375\n",
      "Validation accuracy 0.735625\n",
      "Time elapsed 15.463420764605205 minutes\n",
      "Epoch 1487, training loss 0.510160, accuracy 0.750000\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 15.520380159219107 minutes\n",
      "Epoch 1488, training loss 0.589567, accuracy 0.796875\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 15.581770586967469 minutes\n",
      "Epoch 1489, training loss 0.430000, accuracy 0.843750\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 15.658688028653463 minutes\n",
      "Epoch 1490, training loss 0.611969, accuracy 0.750000\n",
      "Validation accuracy 0.740625\n",
      "Time elapsed 15.716578320662181 minutes\n",
      "Epoch 1491, training loss 0.608250, accuracy 0.765625\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 15.771438058217367 minutes\n",
      "Epoch 1492, training loss 0.564293, accuracy 0.843750\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 15.820990947882335 minutes\n",
      "Epoch 1493, training loss 0.661273, accuracy 0.750000\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 15.870580248037975 minutes\n",
      "Epoch 1494, training loss 0.529639, accuracy 0.812500\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 15.923954319953918 minutes\n",
      "Epoch 1495, training loss 0.655682, accuracy 0.750000\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 15.976542723178863 minutes\n",
      "Epoch 1496, training loss 0.440648, accuracy 0.859375\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 16.028750157356264 minutes\n",
      "Epoch 1497, training loss 0.512372, accuracy 0.796875\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 16.077814149856568 minutes\n",
      "Epoch 1498, training loss 0.504330, accuracy 0.812500\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 16.12531307140986 minutes\n",
      "Epoch 1499, training loss 0.616093, accuracy 0.781250\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 16.175218192736306 minutes\n",
      "Epoch 1500, training loss 0.632057, accuracy 0.734375\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 16.224306348959605 minutes\n",
      "Epoch 1501, training loss 0.577456, accuracy 0.796875\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 16.27266692320506 minutes\n",
      "Epoch 1502, training loss 0.619942, accuracy 0.765625\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 16.320616034666696 minutes\n",
      "Epoch 1503, training loss 0.505738, accuracy 0.843750\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 16.3684543132782 minutes\n",
      "Epoch 1504, training loss 0.593837, accuracy 0.734375\n",
      "Validation accuracy 0.740000\n",
      "Time elapsed 16.420005039374033 minutes\n",
      "Epoch 1505, training loss 0.434556, accuracy 0.859375\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 16.470779033501945 minutes\n",
      "Epoch 1506, training loss 0.553258, accuracy 0.828125\n",
      "Validation accuracy 0.741875\n",
      "Time elapsed 16.51998170216878 minutes\n",
      "Epoch 1507, training loss 0.558964, accuracy 0.765625\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 16.56991675297419 minutes\n",
      "Epoch 1508, training loss 0.668478, accuracy 0.734375\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 16.61827039718628 minutes\n",
      "Epoch 1509, training loss 0.581122, accuracy 0.750000\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 16.6668053150177 minutes\n",
      "Epoch 1510, training loss 0.592622, accuracy 0.687500\n",
      "Validation accuracy 0.740625\n",
      "Time elapsed 16.71682211558024 minutes\n",
      "Epoch 1511, training loss 0.560584, accuracy 0.750000\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 16.76701437632243 minutes\n",
      "Epoch 1512, training loss 0.488634, accuracy 0.796875\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 16.818565066655477 minutes\n",
      "Epoch 1513, training loss 0.564063, accuracy 0.734375\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 16.867211413383483 minutes\n",
      "Epoch 1514, training loss 0.460188, accuracy 0.781250\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 16.916461785634358 minutes\n",
      "Epoch 1515, training loss 0.434591, accuracy 0.859375\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 16.968230072657267 minutes\n",
      "Epoch 1516, training loss 0.401999, accuracy 0.843750\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 17.01673204898834 minutes\n",
      "Epoch 1517, training loss 0.617788, accuracy 0.796875\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 17.065322077274324 minutes\n",
      "Epoch 1518, training loss 0.687331, accuracy 0.687500\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 17.112902601559956 minutes\n",
      "Epoch 1519, training loss 0.542333, accuracy 0.796875\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 17.16094233592351 minutes\n",
      "Epoch 1520, training loss 0.524149, accuracy 0.812500\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 17.208572669823965 minutes\n",
      "Epoch 1521, training loss 0.419904, accuracy 0.906250\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 17.256320281823477 minutes\n",
      "Epoch 1522, training loss 0.493045, accuracy 0.812500\n",
      "Validation accuracy 0.720625\n",
      "Time elapsed 17.304910039901735 minutes\n",
      "Epoch 1523, training loss 0.411779, accuracy 0.812500\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 17.353323256969453 minutes\n",
      "Epoch 1524, training loss 0.365691, accuracy 0.890625\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 17.40772155125936 minutes\n",
      "Epoch 1525, training loss 0.508171, accuracy 0.750000\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 17.458407406012217 minutes\n",
      "Epoch 1526, training loss 0.491984, accuracy 0.828125\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 17.506769406795502 minutes\n",
      "Epoch 1527, training loss 0.593390, accuracy 0.781250\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 17.554740126927694 minutes\n",
      "Epoch 1528, training loss 0.729775, accuracy 0.687500\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 17.602375066280366 minutes\n",
      "Epoch 1529, training loss 0.493168, accuracy 0.796875\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 17.65094522237778 minutes\n",
      "Epoch 1530, training loss 0.669117, accuracy 0.765625\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 17.6979483405749 minutes\n",
      "Epoch 1531, training loss 0.530574, accuracy 0.781250\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 17.745245146751405 minutes\n",
      "Epoch 1532, training loss 0.628823, accuracy 0.812500\n",
      "Validation accuracy 0.736250\n",
      "Time elapsed 17.79302541812261 minutes\n",
      "Epoch 1533, training loss 0.612768, accuracy 0.734375\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 17.842832986513773 minutes\n",
      "Epoch 1534, training loss 0.459672, accuracy 0.843750\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 17.89426843325297 minutes\n",
      "Epoch 1535, training loss 0.616739, accuracy 0.687500\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 17.942557434240978 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1536, training loss 0.543209, accuracy 0.796875\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 17.992776135603588 minutes\n",
      "Epoch 1537, training loss 0.442535, accuracy 0.843750\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 18.041027116775513 minutes\n",
      "Epoch 1538, training loss 0.565647, accuracy 0.781250\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 18.08903691371282 minutes\n",
      "Epoch 1539, training loss 0.487757, accuracy 0.828125\n",
      "Validation accuracy 0.770625\n",
      "Time elapsed 18.136354879538217 minutes\n",
      "Epoch 1540, training loss 0.476827, accuracy 0.843750\n",
      "Validation accuracy 0.741250\n",
      "Time elapsed 18.183942262331644 minutes\n",
      "Epoch 1541, training loss 0.550840, accuracy 0.796875\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 18.23175938129425 minutes\n",
      "Epoch 1542, training loss 0.553281, accuracy 0.765625\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 18.279296267032624 minutes\n",
      "Epoch 1543, training loss 0.576482, accuracy 0.812500\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 18.32744468053182 minutes\n",
      "Epoch 1544, training loss 0.491052, accuracy 0.765625\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 18.37477376461029 minutes\n",
      "Epoch 1545, training loss 0.477628, accuracy 0.843750\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 18.424926336606344 minutes\n",
      "Epoch 1546, training loss 0.463234, accuracy 0.812500\n",
      "Validation accuracy 0.769375\n",
      "Time elapsed 18.47354161341985 minutes\n",
      "Epoch 1547, training loss 0.542746, accuracy 0.781250\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 18.52221630414327 minutes\n",
      "Epoch 1548, training loss 0.608863, accuracy 0.765625\n",
      "Validation accuracy 0.735000\n",
      "Time elapsed 18.57083310286204 minutes\n",
      "Epoch 1549, training loss 0.488005, accuracy 0.781250\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 18.618179925282796 minutes\n",
      "Epoch 1550, training loss 0.496480, accuracy 0.781250\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 18.666231056054432 minutes\n",
      "Epoch 1551, training loss 0.676848, accuracy 0.718750\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 18.713532467683155 minutes\n",
      "Epoch 1552, training loss 0.550787, accuracy 0.812500\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 18.761169238885245 minutes\n",
      "Epoch 1553, training loss 0.597185, accuracy 0.703125\n",
      "Validation accuracy 0.709375\n",
      "Time elapsed 18.80873103539149 minutes\n",
      "Epoch 1554, training loss 0.612812, accuracy 0.750000\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 18.85638124148051 minutes\n",
      "Epoch 1555, training loss 0.368266, accuracy 0.859375\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 18.906422162055968 minutes\n",
      "Epoch 1556, training loss 0.641963, accuracy 0.734375\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 18.954157169659933 minutes\n",
      "Epoch 1557, training loss 0.524899, accuracy 0.828125\n",
      "Validation accuracy 0.719375\n",
      "Time elapsed 19.004269512494407 minutes\n",
      "Epoch 1558, training loss 0.389071, accuracy 0.796875\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 19.058383297920226 minutes\n",
      "Epoch 1559, training loss 0.591087, accuracy 0.734375\n",
      "Validation accuracy 0.770625\n",
      "Time elapsed 19.108307707309724 minutes\n",
      "Epoch 1560, training loss 0.513523, accuracy 0.781250\n",
      "Validation accuracy 0.743125\n",
      "Time elapsed 19.158459556102752 minutes\n",
      "Epoch 1561, training loss 0.606668, accuracy 0.828125\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 19.207421445846556 minutes\n",
      "Epoch 1562, training loss 0.550846, accuracy 0.796875\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 19.25676185687383 minutes\n",
      "Epoch 1563, training loss 0.648413, accuracy 0.703125\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 19.305306756496428 minutes\n",
      "Epoch 1564, training loss 0.652505, accuracy 0.718750\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 19.354047107696534 minutes\n",
      "Epoch 1565, training loss 0.467593, accuracy 0.859375\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 19.405830438931783 minutes\n",
      "Epoch 1566, training loss 0.513474, accuracy 0.828125\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 19.45469582080841 minutes\n",
      "Epoch 1567, training loss 0.605891, accuracy 0.812500\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 19.50302123228709 minutes\n",
      "Epoch 1568, training loss 0.465072, accuracy 0.890625\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 19.552344036102294 minutes\n",
      "Epoch 1569, training loss 0.632919, accuracy 0.781250\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 19.600322691599526 minutes\n",
      "Epoch 1570, training loss 0.567847, accuracy 0.765625\n",
      "Validation accuracy 0.745000\n",
      "Time elapsed 19.648268314202628 minutes\n",
      "Epoch 1571, training loss 0.553429, accuracy 0.750000\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 19.69579459428787 minutes\n",
      "Epoch 1572, training loss 0.625153, accuracy 0.718750\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 19.7435231089592 minutes\n",
      "Epoch 1573, training loss 0.535955, accuracy 0.828125\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 19.79160728851954 minutes\n",
      "Epoch 1574, training loss 0.579419, accuracy 0.765625\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 19.84016042550405 minutes\n",
      "Epoch 1575, training loss 0.478711, accuracy 0.781250\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 19.888278329372405 minutes\n",
      "Epoch 1576, training loss 0.476304, accuracy 0.812500\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 19.93884143034617 minutes\n",
      "Epoch 1577, training loss 0.456662, accuracy 0.890625\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 19.988242936134338 minutes\n",
      "Epoch 1578, training loss 0.601410, accuracy 0.812500\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 20.03651755253474 minutes\n",
      "Epoch 1579, training loss 0.368717, accuracy 0.875000\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 20.085459113121033 minutes\n",
      "Epoch 1580, training loss 0.524944, accuracy 0.781250\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 20.133470471700033 minutes\n",
      "Epoch 1581, training loss 0.529414, accuracy 0.765625\n",
      "Validation accuracy 0.735625\n",
      "Time elapsed 20.182472217082978 minutes\n",
      "Epoch 1582, training loss 0.511933, accuracy 0.828125\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 20.229859272638958 minutes\n",
      "Epoch 1583, training loss 0.522017, accuracy 0.765625\n",
      "Validation accuracy 0.747500\n",
      "Time elapsed 20.277982437610625 minutes\n",
      "Epoch 1584, training loss 0.535155, accuracy 0.750000\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 20.326905326048532 minutes\n",
      "Epoch 1585, training loss 0.442517, accuracy 0.843750\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 20.375348929564158 minutes\n",
      "Epoch 1586, training loss 0.472053, accuracy 0.828125\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 20.427791714668274 minutes\n",
      "Epoch 1587, training loss 0.527644, accuracy 0.750000\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 20.476280891895293 minutes\n",
      "Epoch 1588, training loss 0.439806, accuracy 0.812500\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 20.52408628066381 minutes\n",
      "Epoch 1589, training loss 0.514687, accuracy 0.750000\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 20.572514176368713 minutes\n",
      "Epoch 1590, training loss 0.461779, accuracy 0.843750\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 20.621299544970196 minutes\n",
      "Epoch 1591, training loss 0.521208, accuracy 0.796875\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 20.66983542442322 minutes\n",
      "Epoch 1592, training loss 0.380507, accuracy 0.875000\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 20.717682389418282 minutes\n",
      "Epoch 1593, training loss 0.522233, accuracy 0.796875\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 20.765859365463257 minutes\n",
      "Epoch 1594, training loss 0.477785, accuracy 0.828125\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 20.81356516679128 minutes\n",
      "Epoch 1595, training loss 0.441163, accuracy 0.859375\n",
      "Validation accuracy 0.773125\n",
      "Time elapsed 20.861605111757914 minutes\n",
      "Epoch 1596, training loss 0.698325, accuracy 0.703125\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 20.91433420976003 minutes\n",
      "Epoch 1597, training loss 0.557793, accuracy 0.796875\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 20.967509885629017 minutes\n",
      "Epoch 1598, training loss 0.526148, accuracy 0.812500\n",
      "Validation accuracy 0.730625\n",
      "Time elapsed 21.017170457045236 minutes\n",
      "Epoch 1599, training loss 0.319614, accuracy 0.906250\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 21.066136356194814 minutes\n",
      "Epoch 1600, training loss 0.473847, accuracy 0.890625\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 21.11432604789734 minutes\n",
      "Epoch 1601, training loss 0.565263, accuracy 0.765625\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 21.163896099726358 minutes\n",
      "Epoch 1602, training loss 0.499371, accuracy 0.843750\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 21.212272787094115 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1603, training loss 0.530571, accuracy 0.750000\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 21.259421292940775 minutes\n",
      "Epoch 1604, training loss 0.489191, accuracy 0.843750\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 21.308299465974173 minutes\n",
      "Epoch 1605, training loss 0.521529, accuracy 0.781250\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 21.35813093582789 minutes\n",
      "Epoch 1606, training loss 0.544684, accuracy 0.765625\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 21.410768095652262 minutes\n",
      "Epoch 1607, training loss 0.475495, accuracy 0.796875\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 21.462184198697408 minutes\n",
      "Epoch 1608, training loss 0.586158, accuracy 0.765625\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 21.510719100634258 minutes\n",
      "Epoch 1609, training loss 0.640572, accuracy 0.734375\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 21.55858826637268 minutes\n",
      "Epoch 1610, training loss 0.442389, accuracy 0.828125\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 21.607284259796142 minutes\n",
      "Epoch 1611, training loss 0.496383, accuracy 0.812500\n",
      "Validation accuracy 0.743125\n",
      "Time elapsed 21.655577369530995 minutes\n",
      "Epoch 1612, training loss 0.531766, accuracy 0.812500\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 21.704066395759583 minutes\n",
      "Epoch 1613, training loss 0.677529, accuracy 0.781250\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 21.752866768836974 minutes\n",
      "Epoch 1614, training loss 0.495588, accuracy 0.843750\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 21.80043923854828 minutes\n",
      "Epoch 1615, training loss 0.591123, accuracy 0.734375\n",
      "Validation accuracy 0.728750\n",
      "Time elapsed 21.84895300467809 minutes\n",
      "Epoch 1616, training loss 0.547787, accuracy 0.796875\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 21.899861788749696 minutes\n",
      "Epoch 1617, training loss 0.628479, accuracy 0.703125\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 21.949732840061188 minutes\n",
      "Epoch 1618, training loss 0.629950, accuracy 0.734375\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 22.000800800323486 minutes\n",
      "Epoch 1619, training loss 0.423508, accuracy 0.859375\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 22.049530144532522 minutes\n",
      "Epoch 1620, training loss 0.565324, accuracy 0.781250\n",
      "Validation accuracy 0.722500\n",
      "Time elapsed 22.097001349925996 minutes\n",
      "Epoch 1621, training loss 0.433233, accuracy 0.843750\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 22.144982171058654 minutes\n",
      "Epoch 1622, training loss 0.605549, accuracy 0.671875\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 22.193116474151612 minutes\n",
      "Epoch 1623, training loss 0.537330, accuracy 0.781250\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 22.24222063223521 minutes\n",
      "Epoch 1624, training loss 0.451389, accuracy 0.812500\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 22.292180061340332 minutes\n",
      "Epoch 1625, training loss 0.770308, accuracy 0.718750\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 22.340089547634125 minutes\n",
      "Epoch 1626, training loss 0.690751, accuracy 0.703125\n",
      "Validation accuracy 0.741875\n",
      "Time elapsed 22.389245057106017 minutes\n",
      "Epoch 1627, training loss 0.604116, accuracy 0.812500\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 22.439340126514434 minutes\n",
      "Epoch 1628, training loss 0.547745, accuracy 0.765625\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 22.488371737798055 minutes\n",
      "Epoch 1629, training loss 0.652743, accuracy 0.750000\n",
      "Validation accuracy 0.693125\n",
      "Time elapsed 22.53718811670939 minutes\n",
      "Epoch 1630, training loss 0.461358, accuracy 0.812500\n",
      "Validation accuracy 0.771250\n",
      "Time elapsed 22.585179018974305 minutes\n",
      "Epoch 1631, training loss 0.552899, accuracy 0.781250\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 22.632467126846315 minutes\n",
      "Epoch 1632, training loss 0.402854, accuracy 0.859375\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 22.680356204509735 minutes\n",
      "Epoch 1633, training loss 0.366404, accuracy 0.921875\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 22.728376730283102 minutes\n",
      "Epoch 1634, training loss 0.501621, accuracy 0.796875\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 22.775420065720876 minutes\n",
      "Epoch 1635, training loss 0.597542, accuracy 0.765625\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 22.823778875668843 minutes\n",
      "Epoch 1636, training loss 0.652953, accuracy 0.703125\n",
      "Validation accuracy 0.740625\n",
      "Time elapsed 22.87134203116099 minutes\n",
      "Epoch 1637, training loss 0.525444, accuracy 0.734375\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 22.921866047382355 minutes\n",
      "Epoch 1638, training loss 0.531399, accuracy 0.781250\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 22.970327671368917 minutes\n",
      "Epoch 1639, training loss 0.421933, accuracy 0.796875\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 23.01843141714732 minutes\n",
      "Epoch 1640, training loss 0.490443, accuracy 0.859375\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 23.066061850388845 minutes\n",
      "Epoch 1641, training loss 0.693527, accuracy 0.687500\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 23.113738663991292 minutes\n",
      "Epoch 1642, training loss 0.405351, accuracy 0.796875\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 23.161837697029114 minutes\n",
      "Epoch 1643, training loss 0.581158, accuracy 0.781250\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 23.20923283894857 minutes\n",
      "Epoch 1644, training loss 0.541213, accuracy 0.796875\n",
      "Validation accuracy 0.743125\n",
      "Time elapsed 23.25650273958842 minutes\n",
      "Epoch 1645, training loss 0.618106, accuracy 0.734375\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 23.306175208091737 minutes\n",
      "Epoch 1646, training loss 0.460386, accuracy 0.828125\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 23.355941061178843 minutes\n",
      "Epoch 1647, training loss 0.493286, accuracy 0.796875\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 23.406999484697977 minutes\n",
      "Epoch 1648, training loss 0.566602, accuracy 0.765625\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 23.45794004201889 minutes\n",
      "Epoch 1649, training loss 0.469411, accuracy 0.828125\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 23.50736798842748 minutes\n",
      "Epoch 1650, training loss 0.339809, accuracy 0.906250\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 23.55693818728129 minutes\n",
      "Epoch 1651, training loss 0.674211, accuracy 0.781250\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 23.604286229610445 minutes\n",
      "Epoch 1652, training loss 0.437171, accuracy 0.812500\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 23.652395248413086 minutes\n",
      "Epoch 1653, training loss 0.588602, accuracy 0.812500\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 23.699364598592123 minutes\n",
      "Epoch 1654, training loss 0.447489, accuracy 0.812500\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 23.746661353111268 minutes\n",
      "Epoch 1655, training loss 0.552844, accuracy 0.750000\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 23.794632736841837 minutes\n",
      "Epoch 1656, training loss 0.510070, accuracy 0.828125\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 23.84243029753367 minutes\n",
      "Epoch 1657, training loss 0.523948, accuracy 0.781250\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 23.89145229260127 minutes\n",
      "Epoch 1658, training loss 0.511661, accuracy 0.781250\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 23.9414604028066 minutes\n",
      "Epoch 1659, training loss 0.410880, accuracy 0.843750\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 23.98961591720581 minutes\n",
      "Epoch 1660, training loss 0.607366, accuracy 0.718750\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 24.03651083310445 minutes\n",
      "Epoch 1661, training loss 0.661462, accuracy 0.781250\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 24.084936451911926 minutes\n",
      "Epoch 1662, training loss 0.544380, accuracy 0.750000\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 24.132537718613943 minutes\n",
      "Epoch 1663, training loss 0.699683, accuracy 0.656250\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 24.18020987510681 minutes\n",
      "Epoch 1664, training loss 0.469040, accuracy 0.828125\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 24.227942605813343 minutes\n",
      "Epoch 1665, training loss 0.636428, accuracy 0.781250\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 24.275926033655804 minutes\n",
      "Epoch 1666, training loss 0.584096, accuracy 0.796875\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 24.323845140139262 minutes\n",
      "Epoch 1667, training loss 0.426543, accuracy 0.828125\n",
      "Validation accuracy 0.773125\n",
      "Time elapsed 24.37258288462957 minutes\n",
      "Epoch 1668, training loss 0.523144, accuracy 0.781250\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 24.423864535490672 minutes\n",
      "Epoch 1669, training loss 0.502484, accuracy 0.750000\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 24.472944951057436 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1670, training loss 0.447967, accuracy 0.812500\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 24.520499420166015 minutes\n",
      "Epoch 1671, training loss 0.727943, accuracy 0.656250\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 24.568670264879863 minutes\n",
      "Epoch 1672, training loss 0.459782, accuracy 0.812500\n",
      "Validation accuracy 0.771250\n",
      "Time elapsed 24.6175199508667 minutes\n",
      "Epoch 1673, training loss 0.504211, accuracy 0.843750\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 24.66624768177668 minutes\n",
      "Epoch 1674, training loss 0.357964, accuracy 0.859375\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 24.714710040887198 minutes\n",
      "Epoch 1675, training loss 0.497994, accuracy 0.781250\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 24.763488765557607 minutes\n",
      "Epoch 1676, training loss 0.705075, accuracy 0.718750\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 24.81247910261154 minutes\n",
      "Epoch 1677, training loss 0.587266, accuracy 0.765625\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 24.859680990378063 minutes\n",
      "Epoch 1678, training loss 0.497115, accuracy 0.828125\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 24.909996672471365 minutes\n",
      "Epoch 1679, training loss 0.564862, accuracy 0.765625\n",
      "Validation accuracy 0.770000\n",
      "Time elapsed 24.95871866941452 minutes\n",
      "Epoch 1680, training loss 0.565875, accuracy 0.687500\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 25.0068385720253 minutes\n",
      "Epoch 1681, training loss 0.469440, accuracy 0.781250\n",
      "Validation accuracy 0.775625\n",
      "Time elapsed 25.055414080619812 minutes\n",
      "Epoch 1682, training loss 0.580460, accuracy 0.750000\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 25.10290952126185 minutes\n",
      "Epoch 1683, training loss 0.377845, accuracy 0.859375\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 25.1507115205129 minutes\n",
      "Epoch 1684, training loss 0.497178, accuracy 0.781250\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 25.197828725973764 minutes\n",
      "Epoch 1685, training loss 0.469400, accuracy 0.812500\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 25.246118366718292 minutes\n",
      "Epoch 1686, training loss 0.472284, accuracy 0.812500\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 25.294290812810264 minutes\n",
      "Epoch 1687, training loss 0.610052, accuracy 0.765625\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 25.342787659168245 minutes\n",
      "Epoch 1688, training loss 0.468916, accuracy 0.781250\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 25.393847262859346 minutes\n",
      "Epoch 1689, training loss 0.594375, accuracy 0.781250\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 25.445407728354137 minutes\n",
      "Epoch 1690, training loss 0.503384, accuracy 0.765625\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 25.4942423303922 minutes\n",
      "Epoch 1691, training loss 0.500262, accuracy 0.750000\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 25.54214142560959 minutes\n",
      "Epoch 1692, training loss 0.586354, accuracy 0.718750\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 25.591098070144653 minutes\n",
      "Epoch 1693, training loss 0.591491, accuracy 0.734375\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 25.638835187753042 minutes\n",
      "Epoch 1694, training loss 0.594734, accuracy 0.750000\n",
      "Validation accuracy 0.772500\n",
      "Time elapsed 25.68728458484014 minutes\n",
      "Epoch 1695, training loss 0.441720, accuracy 0.843750\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 25.734912316004436 minutes\n",
      "Epoch 1696, training loss 0.388433, accuracy 0.890625\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 25.782652695973713 minutes\n",
      "Epoch 1697, training loss 0.505691, accuracy 0.781250\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 25.831082864602408 minutes\n",
      "Epoch 1698, training loss 0.486583, accuracy 0.796875\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 25.879385511080425 minutes\n",
      "Epoch 1699, training loss 0.448128, accuracy 0.796875\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 25.927886120478313 minutes\n",
      "Epoch 1700, training loss 0.761976, accuracy 0.671875\n",
      "Validation accuracy 0.731875\n",
      "Time elapsed 25.975862570603688 minutes\n",
      "Epoch 1701, training loss 0.421677, accuracy 0.781250\n",
      "Validation accuracy 0.739375\n",
      "Time elapsed 26.02561233441035 minutes\n",
      "Epoch 1702, training loss 0.615859, accuracy 0.765625\n",
      "Validation accuracy 0.773125\n",
      "Time elapsed 26.07388486067454 minutes\n",
      "Epoch 1703, training loss 0.559226, accuracy 0.765625\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 26.122352504730223 minutes\n",
      "Epoch 1704, training loss 0.535282, accuracy 0.765625\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 26.17039291461309 minutes\n",
      "Epoch 1705, training loss 0.522759, accuracy 0.812500\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 26.218196626504263 minutes\n",
      "Epoch 1706, training loss 0.515933, accuracy 0.796875\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 26.266677192846934 minutes\n",
      "Epoch 1707, training loss 0.577805, accuracy 0.765625\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 26.31678522825241 minutes\n",
      "Epoch 1708, training loss 0.457547, accuracy 0.843750\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 26.36946703195572 minutes\n",
      "Epoch 1709, training loss 0.564739, accuracy 0.812500\n",
      "Validation accuracy 0.731250\n",
      "Time elapsed 26.421406213442484 minutes\n",
      "Epoch 1710, training loss 0.466734, accuracy 0.875000\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 26.469128076235453 minutes\n",
      "Epoch 1711, training loss 0.595733, accuracy 0.781250\n",
      "Validation accuracy 0.774375\n",
      "Time elapsed 26.51684377193451 minutes\n",
      "Epoch 1712, training loss 0.625809, accuracy 0.750000\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 26.563840572039286 minutes\n",
      "Epoch 1713, training loss 0.570505, accuracy 0.812500\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 26.61269575357437 minutes\n",
      "Epoch 1714, training loss 0.561519, accuracy 0.828125\n",
      "Validation accuracy 0.769375\n",
      "Time elapsed 26.662111230691274 minutes\n",
      "Epoch 1715, training loss 0.495506, accuracy 0.734375\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 26.710061160723367 minutes\n",
      "Epoch 1716, training loss 0.607312, accuracy 0.765625\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 26.757099489370983 minutes\n",
      "Epoch 1717, training loss 0.671893, accuracy 0.765625\n",
      "Validation accuracy 0.728750\n",
      "Time elapsed 26.805514991283417 minutes\n",
      "Epoch 1718, training loss 0.376886, accuracy 0.890625\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 26.854713420073193 minutes\n",
      "Epoch 1719, training loss 0.408637, accuracy 0.890625\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 26.904885617891946 minutes\n",
      "Epoch 1720, training loss 0.507442, accuracy 0.796875\n",
      "Validation accuracy 0.769375\n",
      "Time elapsed 26.954509015878042 minutes\n",
      "Epoch 1721, training loss 0.369011, accuracy 0.937500\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 27.002805860837302 minutes\n",
      "Epoch 1722, training loss 0.446585, accuracy 0.859375\n",
      "Validation accuracy 0.770000\n",
      "Time elapsed 27.050930392742156 minutes\n",
      "Epoch 1723, training loss 0.452033, accuracy 0.828125\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 27.1033784588178 minutes\n",
      "Epoch 1724, training loss 0.612688, accuracy 0.734375\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 27.154029270013172 minutes\n",
      "Epoch 1725, training loss 0.406061, accuracy 0.843750\n",
      "Validation accuracy 0.778125\n",
      "Time elapsed 27.202416932582857 minutes\n",
      "Epoch 1726, training loss 0.485554, accuracy 0.828125\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 27.251699336369832 minutes\n",
      "Epoch 1727, training loss 0.563034, accuracy 0.781250\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 27.3006294409434 minutes\n",
      "Epoch 1728, training loss 0.568305, accuracy 0.703125\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 27.349771745999654 minutes\n",
      "Epoch 1729, training loss 0.527899, accuracy 0.812500\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 27.39991375605265 minutes\n",
      "Epoch 1730, training loss 0.530560, accuracy 0.781250\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 27.45063440402349 minutes\n",
      "Epoch 1731, training loss 0.552044, accuracy 0.796875\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 27.499184854825337 minutes\n",
      "Epoch 1732, training loss 0.496685, accuracy 0.781250\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 27.548111057281496 minutes\n",
      "Epoch 1733, training loss 0.416468, accuracy 0.843750\n",
      "Validation accuracy 0.773125\n",
      "Time elapsed 27.59688311815262 minutes\n",
      "Epoch 1734, training loss 0.537709, accuracy 0.859375\n",
      "Validation accuracy 0.774375\n",
      "Time elapsed 27.64531333843867 minutes\n",
      "Epoch 1735, training loss 0.532321, accuracy 0.828125\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 27.695264728864036 minutes\n",
      "Epoch 1736, training loss 0.591162, accuracy 0.812500\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 27.743821064631145 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1737, training loss 0.429820, accuracy 0.828125\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 27.792495691776274 minutes\n",
      "Epoch 1738, training loss 0.555013, accuracy 0.718750\n",
      "Validation accuracy 0.750625\n",
      "Time elapsed 27.84118834336599 minutes\n",
      "Epoch 1739, training loss 0.654003, accuracy 0.687500\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 27.890002624193826 minutes\n",
      "Epoch 1740, training loss 0.602646, accuracy 0.765625\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 27.940118877092996 minutes\n",
      "Epoch 1741, training loss 0.621495, accuracy 0.718750\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 27.988846429189046 minutes\n",
      "Epoch 1742, training loss 0.613424, accuracy 0.703125\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 28.03714828491211 minutes\n",
      "Epoch 1743, training loss 0.439583, accuracy 0.796875\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 28.085556089878082 minutes\n",
      "Epoch 1744, training loss 0.592835, accuracy 0.781250\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 28.134367589155833 minutes\n",
      "Epoch 1745, training loss 0.558268, accuracy 0.781250\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 28.183999991416933 minutes\n",
      "Epoch 1746, training loss 0.602591, accuracy 0.750000\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 28.234374030431113 minutes\n",
      "Epoch 1747, training loss 0.433756, accuracy 0.828125\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 28.28401252031326 minutes\n",
      "Epoch 1748, training loss 0.367024, accuracy 0.875000\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 28.3340527455012 minutes\n",
      "Epoch 1749, training loss 0.450796, accuracy 0.843750\n",
      "Validation accuracy 0.770625\n",
      "Time elapsed 28.38468234539032 minutes\n",
      "Epoch 1750, training loss 0.673559, accuracy 0.687500\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 28.437437736988066 minutes\n",
      "Epoch 1751, training loss 0.525606, accuracy 0.828125\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 28.486985433101655 minutes\n",
      "Epoch 1752, training loss 0.509156, accuracy 0.828125\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 28.53699870109558 minutes\n",
      "Epoch 1753, training loss 0.450053, accuracy 0.812500\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 28.58621690273285 minutes\n",
      "Epoch 1754, training loss 0.478436, accuracy 0.828125\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 28.637033156553905 minutes\n",
      "Epoch 1755, training loss 0.495588, accuracy 0.781250\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 28.68582497040431 minutes\n",
      "Epoch 1756, training loss 0.475295, accuracy 0.859375\n",
      "Validation accuracy 0.776250\n",
      "Time elapsed 28.73448138634364 minutes\n",
      "Epoch 1757, training loss 0.530980, accuracy 0.843750\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 28.785782965024314 minutes\n",
      "Epoch 1758, training loss 0.493783, accuracy 0.843750\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 28.83504751523336 minutes\n",
      "Epoch 1759, training loss 0.390793, accuracy 0.828125\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 28.884689311186474 minutes\n",
      "Epoch 1760, training loss 0.549056, accuracy 0.828125\n",
      "Validation accuracy 0.740000\n",
      "Time elapsed 28.932662896315257 minutes\n",
      "Epoch 1761, training loss 0.463709, accuracy 0.812500\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 28.980435637633004 minutes\n",
      "Epoch 1762, training loss 0.467470, accuracy 0.781250\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 29.030913313229878 minutes\n",
      "Epoch 1763, training loss 0.579571, accuracy 0.765625\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 29.079736880461375 minutes\n",
      "Epoch 1764, training loss 0.603046, accuracy 0.718750\n",
      "Validation accuracy 0.746250\n",
      "Time elapsed 29.127440905570985 minutes\n",
      "Epoch 1765, training loss 0.490979, accuracy 0.812500\n",
      "Validation accuracy 0.777500\n",
      "Time elapsed 29.176879052321116 minutes\n",
      "Epoch 1766, training loss 0.451657, accuracy 0.765625\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 29.22588795820872 minutes\n",
      "Epoch 1767, training loss 0.488202, accuracy 0.843750\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 29.275461558500925 minutes\n",
      "Epoch 1768, training loss 0.605796, accuracy 0.718750\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 29.327789866924284 minutes\n",
      "Epoch 1769, training loss 0.545651, accuracy 0.796875\n",
      "Validation accuracy 0.778125\n",
      "Time elapsed 29.3773508032163 minutes\n",
      "Epoch 1770, training loss 0.566853, accuracy 0.718750\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 29.43155737320582 minutes\n",
      "Epoch 1771, training loss 0.479307, accuracy 0.796875\n",
      "Validation accuracy 0.730000\n",
      "Time elapsed 29.480369261900584 minutes\n",
      "Epoch 1772, training loss 0.474567, accuracy 0.812500\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 29.529800728956857 minutes\n",
      "Epoch 1773, training loss 0.603528, accuracy 0.781250\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 29.579273986816407 minutes\n",
      "Epoch 1774, training loss 0.610581, accuracy 0.812500\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 29.627680174509685 minutes\n",
      "Epoch 1775, training loss 0.352197, accuracy 0.875000\n",
      "Validation accuracy 0.772500\n",
      "Time elapsed 29.677251529693603 minutes\n",
      "Epoch 1776, training loss 0.531488, accuracy 0.750000\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 29.725485932826995 minutes\n",
      "Epoch 1777, training loss 0.456634, accuracy 0.859375\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 29.77384985287984 minutes\n",
      "Epoch 1778, training loss 0.520580, accuracy 0.828125\n",
      "Validation accuracy 0.769375\n",
      "Time elapsed 29.822996425628663 minutes\n",
      "Epoch 1779, training loss 0.509870, accuracy 0.828125\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 29.872129066785178 minutes\n",
      "Epoch 1780, training loss 0.498391, accuracy 0.796875\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 29.925227439403535 minutes\n",
      "Epoch 1781, training loss 0.426221, accuracy 0.828125\n",
      "Validation accuracy 0.751875\n",
      "Time elapsed 29.974158505598705 minutes\n",
      "Epoch 1782, training loss 0.613736, accuracy 0.718750\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 30.02335718472799 minutes\n",
      "Epoch 1783, training loss 0.543092, accuracy 0.796875\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 30.072018790245057 minutes\n",
      "Epoch 1784, training loss 0.519565, accuracy 0.812500\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 30.120283003648122 minutes\n",
      "Epoch 1785, training loss 0.434196, accuracy 0.843750\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 30.168871776262918 minutes\n",
      "Epoch 1786, training loss 0.467240, accuracy 0.890625\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 30.218125236034393 minutes\n",
      "Epoch 1787, training loss 0.683904, accuracy 0.734375\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 30.268045111497244 minutes\n",
      "Epoch 1788, training loss 0.518062, accuracy 0.765625\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 30.318948038419087 minutes\n",
      "Epoch 1789, training loss 0.412299, accuracy 0.875000\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 30.3685445189476 minutes\n",
      "Epoch 1790, training loss 0.429366, accuracy 0.812500\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 30.42050317923228 minutes\n",
      "Epoch 1791, training loss 0.424265, accuracy 0.796875\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 30.46867907444636 minutes\n",
      "Epoch 1792, training loss 0.641800, accuracy 0.750000\n",
      "Validation accuracy 0.734375\n",
      "Time elapsed 30.51741755803426 minutes\n",
      "Epoch 1793, training loss 0.439039, accuracy 0.843750\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 30.566743874549864 minutes\n",
      "Epoch 1794, training loss 0.560230, accuracy 0.796875\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 30.615608274936676 minutes\n",
      "Epoch 1795, training loss 0.546984, accuracy 0.781250\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 30.664561076958975 minutes\n",
      "Epoch 1796, training loss 0.443187, accuracy 0.828125\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 30.713672224680582 minutes\n",
      "Epoch 1797, training loss 0.492459, accuracy 0.828125\n",
      "Validation accuracy 0.770000\n",
      "Time elapsed 30.76272939046224 minutes\n",
      "Epoch 1798, training loss 0.541987, accuracy 0.796875\n",
      "Validation accuracy 0.771250\n",
      "Time elapsed 30.811575623353324 minutes\n",
      "Epoch 1799, training loss 0.655820, accuracy 0.687500\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 30.86059314807256 minutes\n",
      "Epoch 1800, training loss 0.567194, accuracy 0.781250\n",
      "Validation accuracy 0.772500\n",
      "Time elapsed 30.913861056168873 minutes\n",
      "Epoch 1801, training loss 0.378507, accuracy 0.859375\n",
      "Validation accuracy 0.770000\n",
      "Time elapsed 30.964501690864562 minutes\n",
      "Epoch 1802, training loss 0.434238, accuracy 0.781250\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 31.014393627643585 minutes\n",
      "Epoch 1803, training loss 0.350417, accuracy 0.859375\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 31.063711376984916 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1804, training loss 0.430017, accuracy 0.843750\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 31.113100643952688 minutes\n",
      "Epoch 1805, training loss 0.593148, accuracy 0.734375\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 31.162369108200075 minutes\n",
      "Epoch 1806, training loss 0.746317, accuracy 0.750000\n",
      "Validation accuracy 0.703125\n",
      "Time elapsed 31.210172247886657 minutes\n",
      "Epoch 1807, training loss 0.464083, accuracy 0.796875\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 31.258322779337565 minutes\n",
      "Epoch 1808, training loss 0.573560, accuracy 0.765625\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 31.307035410404204 minutes\n",
      "Epoch 1809, training loss 0.464422, accuracy 0.859375\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 31.355474515755972 minutes\n",
      "Epoch 1810, training loss 0.381605, accuracy 0.890625\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 31.40588068564733 minutes\n",
      "Epoch 1811, training loss 0.605695, accuracy 0.718750\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 31.45835030078888 minutes\n",
      "Epoch 1812, training loss 0.501951, accuracy 0.781250\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 31.506923619906107 minutes\n",
      "Epoch 1813, training loss 0.438485, accuracy 0.859375\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 31.557528511683145 minutes\n",
      "Epoch 1814, training loss 0.501984, accuracy 0.734375\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 31.607704981168112 minutes\n",
      "Epoch 1815, training loss 0.525819, accuracy 0.765625\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 31.656548277537027 minutes\n",
      "Epoch 1816, training loss 0.632146, accuracy 0.750000\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 31.704956781864166 minutes\n",
      "Epoch 1817, training loss 0.605395, accuracy 0.765625\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 31.752585081259408 minutes\n",
      "Epoch 1818, training loss 0.552963, accuracy 0.828125\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 31.800806240240732 minutes\n",
      "Epoch 1819, training loss 0.550267, accuracy 0.781250\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 31.848765472571056 minutes\n",
      "Epoch 1820, training loss 0.563681, accuracy 0.765625\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 31.89989562431971 minutes\n",
      "Epoch 1821, training loss 0.383981, accuracy 0.859375\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 31.948770872751872 minutes\n",
      "Epoch 1822, training loss 0.594437, accuracy 0.718750\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 31.9997433980306 minutes\n",
      "Epoch 1823, training loss 0.499992, accuracy 0.781250\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 32.04785041014353 minutes\n",
      "Epoch 1824, training loss 0.511701, accuracy 0.796875\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 32.09763783613841 minutes\n",
      "Epoch 1825, training loss 0.492355, accuracy 0.812500\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 32.14630079269409 minutes\n",
      "Epoch 1826, training loss 0.619902, accuracy 0.734375\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 32.193895856539406 minutes\n",
      "Epoch 1827, training loss 0.476402, accuracy 0.796875\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 32.24237375259399 minutes\n",
      "Epoch 1828, training loss 0.445383, accuracy 0.812500\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 32.29128734668096 minutes\n",
      "Epoch 1829, training loss 0.692777, accuracy 0.765625\n",
      "Validation accuracy 0.749375\n",
      "Time elapsed 32.33935540517171 minutes\n",
      "Epoch 1830, training loss 0.572584, accuracy 0.812500\n",
      "Validation accuracy 0.750000\n",
      "Time elapsed 32.38905379374822 minutes\n",
      "Epoch 1831, training loss 0.334936, accuracy 0.859375\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 32.43938736518224 minutes\n",
      "Epoch 1832, training loss 0.476019, accuracy 0.796875\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 32.49146597782771 minutes\n",
      "Epoch 1833, training loss 0.563409, accuracy 0.828125\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 32.54034490982691 minutes\n",
      "Epoch 1834, training loss 0.619651, accuracy 0.812500\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 32.58853458563487 minutes\n",
      "Epoch 1835, training loss 0.681481, accuracy 0.750000\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 32.6367492278417 minutes\n",
      "Epoch 1836, training loss 0.577513, accuracy 0.734375\n",
      "Validation accuracy 0.753125\n",
      "Time elapsed 32.68452347914378 minutes\n",
      "Epoch 1837, training loss 0.518061, accuracy 0.687500\n",
      "Validation accuracy 0.776250\n",
      "Time elapsed 32.732601809501645 minutes\n",
      "Epoch 1838, training loss 0.435585, accuracy 0.812500\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 32.780803334712985 minutes\n",
      "Epoch 1839, training loss 0.542948, accuracy 0.734375\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 32.830186482270555 minutes\n",
      "Epoch 1840, training loss 0.443697, accuracy 0.812500\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 32.881471916039786 minutes\n",
      "Epoch 1841, training loss 0.430908, accuracy 0.843750\n",
      "Validation accuracy 0.780625\n",
      "Time elapsed 32.93105464378993 minutes\n",
      "Epoch 1842, training loss 0.662201, accuracy 0.796875\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 32.982749644915266 minutes\n",
      "Epoch 1843, training loss 0.546212, accuracy 0.765625\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 33.03925664027532 minutes\n",
      "Epoch 1844, training loss 0.550387, accuracy 0.765625\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 33.100882256031035 minutes\n",
      "Epoch 1845, training loss 0.511461, accuracy 0.796875\n",
      "Validation accuracy 0.769375\n",
      "Time elapsed 33.156319133440654 minutes\n",
      "Epoch 1846, training loss 0.502820, accuracy 0.843750\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 33.21571720838547 minutes\n",
      "Epoch 1847, training loss 0.467036, accuracy 0.843750\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 33.27670798301697 minutes\n",
      "Epoch 1848, training loss 0.659304, accuracy 0.734375\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 33.33319503068924 minutes\n",
      "Epoch 1849, training loss 0.598375, accuracy 0.703125\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 33.38511878649394 minutes\n",
      "Epoch 1850, training loss 0.629341, accuracy 0.718750\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 33.445248210430144 minutes\n",
      "Epoch 1851, training loss 0.482236, accuracy 0.843750\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 33.51743530432383 minutes\n",
      "Epoch 1852, training loss 0.627704, accuracy 0.734375\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 33.56974751551946 minutes\n",
      "Epoch 1853, training loss 0.493874, accuracy 0.718750\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 33.61851504246394 minutes\n",
      "Epoch 1854, training loss 0.577676, accuracy 0.765625\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 33.671993084748586 minutes\n",
      "Epoch 1855, training loss 0.456258, accuracy 0.859375\n",
      "Validation accuracy 0.741250\n",
      "Time elapsed 33.73403953711192 minutes\n",
      "Epoch 1856, training loss 0.522706, accuracy 0.781250\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 33.792047190666196 minutes\n",
      "Epoch 1857, training loss 0.559535, accuracy 0.796875\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 33.84871589342753 minutes\n",
      "Epoch 1858, training loss 0.629146, accuracy 0.750000\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 33.90543886423111 minutes\n",
      "Epoch 1859, training loss 0.709617, accuracy 0.750000\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 33.95870505174001 minutes\n",
      "Epoch 1860, training loss 0.437027, accuracy 0.812500\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 34.01214377880096 minutes\n",
      "Epoch 1861, training loss 0.552964, accuracy 0.796875\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 34.078382603327434 minutes\n",
      "Epoch 1862, training loss 0.453403, accuracy 0.843750\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 34.13899170557658 minutes\n",
      "Epoch 1863, training loss 0.539608, accuracy 0.718750\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 34.20457505385081 minutes\n",
      "Epoch 1864, training loss 0.563148, accuracy 0.781250\n",
      "Validation accuracy 0.741875\n",
      "Time elapsed 34.27079664071401 minutes\n",
      "Epoch 1865, training loss 0.569504, accuracy 0.781250\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 34.32416135072708 minutes\n",
      "Epoch 1866, training loss 1.474706, accuracy 0.484375\n",
      "Validation accuracy 0.611250\n",
      "Time elapsed 34.37704744736354 minutes\n",
      "Epoch 1867, training loss 0.485789, accuracy 0.859375\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 34.432133893171944 minutes\n",
      "Epoch 1868, training loss 0.463746, accuracy 0.781250\n",
      "Validation accuracy 0.770625\n",
      "Time elapsed 34.48792250553767 minutes\n",
      "Epoch 1869, training loss 0.450540, accuracy 0.859375\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 34.567840441068014 minutes\n",
      "Epoch 1870, training loss 0.548561, accuracy 0.796875\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 34.64011697769165 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1871, training loss 0.551199, accuracy 0.765625\n",
      "Validation accuracy 0.776875\n",
      "Time elapsed 34.70696199337642 minutes\n",
      "Epoch 1872, training loss 0.504882, accuracy 0.796875\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 34.76845311323802 minutes\n",
      "Epoch 1873, training loss 0.412661, accuracy 0.859375\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 34.83626219828923 minutes\n",
      "Epoch 1874, training loss 0.345035, accuracy 0.890625\n",
      "Validation accuracy 0.771250\n",
      "Time elapsed 34.897167865435286 minutes\n",
      "Epoch 1875, training loss 0.636030, accuracy 0.734375\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 34.950175221761064 minutes\n",
      "Epoch 1876, training loss 0.691046, accuracy 0.656250\n",
      "Validation accuracy 0.773750\n",
      "Time elapsed 35.01043285131455 minutes\n",
      "Epoch 1877, training loss 0.594122, accuracy 0.765625\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 35.05905292034149 minutes\n",
      "Epoch 1878, training loss 0.752618, accuracy 0.718750\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 35.104807968934374 minutes\n",
      "Epoch 1879, training loss 0.635674, accuracy 0.718750\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 35.1586998462677 minutes\n",
      "Epoch 1880, training loss 0.556801, accuracy 0.750000\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 35.21353144645691 minutes\n",
      "Epoch 1881, training loss 0.641652, accuracy 0.671875\n",
      "Validation accuracy 0.744375\n",
      "Time elapsed 35.26962377627691 minutes\n",
      "Epoch 1882, training loss 0.545375, accuracy 0.765625\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 35.318096391359965 minutes\n",
      "Epoch 1883, training loss 0.395776, accuracy 0.812500\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 35.374482639630635 minutes\n",
      "Epoch 1884, training loss 0.557067, accuracy 0.828125\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 35.43630309899648 minutes\n",
      "Epoch 1885, training loss 0.393683, accuracy 0.828125\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 35.50446945428848 minutes\n",
      "Epoch 1886, training loss 0.568646, accuracy 0.734375\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 35.56107108592987 minutes\n",
      "Epoch 1887, training loss 0.529264, accuracy 0.828125\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 35.61587697664897 minutes\n",
      "Epoch 1888, training loss 0.386484, accuracy 0.890625\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 35.66471083958944 minutes\n",
      "Epoch 1889, training loss 0.471699, accuracy 0.859375\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 35.71148227850596 minutes\n",
      "Epoch 1890, training loss 0.361950, accuracy 0.843750\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 35.75726311604182 minutes\n",
      "Epoch 1891, training loss 0.425034, accuracy 0.875000\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 35.802784486611685 minutes\n",
      "Epoch 1892, training loss 0.465856, accuracy 0.812500\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 35.848905829588574 minutes\n",
      "Epoch 1893, training loss 0.390102, accuracy 0.843750\n",
      "Validation accuracy 0.773750\n",
      "Time elapsed 35.89539775451024 minutes\n",
      "Epoch 1894, training loss 0.576045, accuracy 0.703125\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 35.94156202872594 minutes\n",
      "Epoch 1895, training loss 0.457153, accuracy 0.828125\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 35.989817849795024 minutes\n",
      "Epoch 1896, training loss 0.477397, accuracy 0.796875\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 36.035857729117076 minutes\n",
      "Epoch 1897, training loss 0.439444, accuracy 0.843750\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 36.085796348253886 minutes\n",
      "Epoch 1898, training loss 0.625935, accuracy 0.781250\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 36.1383774638176 minutes\n",
      "Epoch 1899, training loss 0.447775, accuracy 0.828125\n",
      "Validation accuracy 0.730000\n",
      "Time elapsed 36.193784495194755 minutes\n",
      "Epoch 1900, training loss 0.501374, accuracy 0.828125\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 36.24517999490102 minutes\n",
      "Epoch 1901, training loss 0.650869, accuracy 0.765625\n",
      "Validation accuracy 0.773750\n",
      "Time elapsed 36.29539096752803 minutes\n",
      "Epoch 1902, training loss 0.616216, accuracy 0.750000\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 36.34549292723338 minutes\n",
      "Epoch 1903, training loss 0.542373, accuracy 0.812500\n",
      "Validation accuracy 0.770000\n",
      "Time elapsed 36.397230207920074 minutes\n",
      "Epoch 1904, training loss 0.466676, accuracy 0.750000\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 36.44733188549677 minutes\n",
      "Epoch 1905, training loss 0.729018, accuracy 0.718750\n",
      "Validation accuracy 0.756875\n",
      "Time elapsed 36.49872575998306 minutes\n",
      "Epoch 1906, training loss 0.439297, accuracy 0.812500\n",
      "Validation accuracy 0.774375\n",
      "Time elapsed 36.54903569618861 minutes\n",
      "Epoch 1907, training loss 0.556849, accuracy 0.718750\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 36.59883605241775 minutes\n",
      "Epoch 1908, training loss 0.646129, accuracy 0.703125\n",
      "Validation accuracy 0.736875\n",
      "Time elapsed 36.64831583499908 minutes\n",
      "Epoch 1909, training loss 0.410737, accuracy 0.859375\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 36.69807459910711 minutes\n",
      "Epoch 1910, training loss 0.600217, accuracy 0.781250\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 36.747626117865245 minutes\n",
      "Epoch 1911, training loss 0.359666, accuracy 0.875000\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 36.80015734036763 minutes\n",
      "Epoch 1912, training loss 0.496059, accuracy 0.859375\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 36.85109873612722 minutes\n",
      "Epoch 1913, training loss 0.497102, accuracy 0.765625\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 36.90440958738327 minutes\n",
      "Epoch 1914, training loss 0.645425, accuracy 0.718750\n",
      "Validation accuracy 0.698750\n",
      "Time elapsed 36.95508482058843 minutes\n",
      "Epoch 1915, training loss 0.411393, accuracy 0.828125\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 37.00586924155553 minutes\n",
      "Epoch 1916, training loss 0.337989, accuracy 0.906250\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 37.05773546695709 minutes\n",
      "Epoch 1917, training loss 0.495114, accuracy 0.796875\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 37.11117394367854 minutes\n",
      "Epoch 1918, training loss 0.352605, accuracy 0.828125\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 37.164751847585045 minutes\n",
      "Epoch 1919, training loss 0.552699, accuracy 0.781250\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 37.22140882809957 minutes\n",
      "Epoch 1920, training loss 0.419805, accuracy 0.828125\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 37.274125138918556 minutes\n",
      "Epoch 1921, training loss 0.447090, accuracy 0.781250\n",
      "Validation accuracy 0.760000\n",
      "Time elapsed 37.34002559185028 minutes\n",
      "Epoch 1922, training loss 0.620028, accuracy 0.765625\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 37.41835923989614 minutes\n",
      "Epoch 1923, training loss 0.369816, accuracy 0.875000\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 37.473171766599016 minutes\n",
      "Epoch 1924, training loss 0.426087, accuracy 0.781250\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 37.52505020300547 minutes\n",
      "Epoch 1925, training loss 0.522233, accuracy 0.796875\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 37.57100639343262 minutes\n",
      "Epoch 1926, training loss 0.457449, accuracy 0.828125\n",
      "Validation accuracy 0.736875\n",
      "Time elapsed 37.62284245093664 minutes\n",
      "Epoch 1927, training loss 0.574904, accuracy 0.765625\n",
      "Validation accuracy 0.773125\n",
      "Time elapsed 37.682432452837624 minutes\n",
      "Epoch 1928, training loss 0.486275, accuracy 0.812500\n",
      "Validation accuracy 0.751250\n",
      "Time elapsed 37.74736663897832 minutes\n",
      "Epoch 1929, training loss 0.607345, accuracy 0.843750\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 37.811979138851164 minutes\n",
      "Epoch 1930, training loss 0.543817, accuracy 0.828125\n",
      "Validation accuracy 0.771875\n",
      "Time elapsed 37.86901364723841 minutes\n",
      "Epoch 1931, training loss 0.265253, accuracy 0.921875\n",
      "Validation accuracy 0.770625\n",
      "Time elapsed 37.9224298675855 minutes\n",
      "Epoch 1932, training loss 0.498513, accuracy 0.781250\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 37.97506572008133 minutes\n",
      "Epoch 1933, training loss 0.611540, accuracy 0.671875\n",
      "Validation accuracy 0.745625\n",
      "Time elapsed 38.02894908984502 minutes\n",
      "Epoch 1934, training loss 0.451033, accuracy 0.828125\n",
      "Validation accuracy 0.760625\n",
      "Time elapsed 38.08557650645574 minutes\n",
      "Epoch 1935, training loss 0.542205, accuracy 0.703125\n",
      "Validation accuracy 0.773750\n",
      "Time elapsed 38.14335606495539 minutes\n",
      "Epoch 1936, training loss 0.411698, accuracy 0.796875\n",
      "Validation accuracy 0.755000\n",
      "Time elapsed 38.19911957184474 minutes\n",
      "Epoch 1937, training loss 0.503525, accuracy 0.765625\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 38.25599482456843 minutes\n",
      "Epoch 1938, training loss 0.436882, accuracy 0.843750\n",
      "Validation accuracy 0.757500\n",
      "Time elapsed 38.31631517012914 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1939, training loss 0.570874, accuracy 0.781250\n",
      "Validation accuracy 0.731250\n",
      "Time elapsed 38.37215404113134 minutes\n",
      "Epoch 1940, training loss 0.743400, accuracy 0.703125\n",
      "Validation accuracy 0.746875\n",
      "Time elapsed 38.42940081357956 minutes\n",
      "Epoch 1941, training loss 0.563615, accuracy 0.796875\n",
      "Validation accuracy 0.753750\n",
      "Time elapsed 38.48857179880142 minutes\n",
      "Epoch 1942, training loss 0.373536, accuracy 0.828125\n",
      "Validation accuracy 0.775000\n",
      "Time elapsed 38.54399878184001 minutes\n",
      "Epoch 1943, training loss 0.434319, accuracy 0.859375\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 38.60392245054245 minutes\n",
      "Epoch 1944, training loss 0.671552, accuracy 0.734375\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 38.66173034509023 minutes\n",
      "Epoch 1945, training loss 0.648583, accuracy 0.656250\n",
      "Validation accuracy 0.731250\n",
      "Time elapsed 38.71501845121384 minutes\n",
      "Epoch 1946, training loss 0.443097, accuracy 0.812500\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 38.76593673626582 minutes\n",
      "Epoch 1947, training loss 0.623955, accuracy 0.750000\n",
      "Validation accuracy 0.755625\n",
      "Time elapsed 38.8164781888326 minutes\n",
      "Epoch 1948, training loss 0.526186, accuracy 0.781250\n",
      "Validation accuracy 0.770000\n",
      "Time elapsed 38.86559944152832 minutes\n",
      "Epoch 1949, training loss 0.596869, accuracy 0.781250\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 38.919675135612486 minutes\n",
      "Epoch 1950, training loss 0.357853, accuracy 0.828125\n",
      "Validation accuracy 0.772500\n",
      "Time elapsed 38.97002944151561 minutes\n",
      "Epoch 1951, training loss 0.426971, accuracy 0.875000\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 39.02026364803314 minutes\n",
      "Epoch 1952, training loss 0.578065, accuracy 0.812500\n",
      "Validation accuracy 0.759375\n",
      "Time elapsed 39.071181837717695 minutes\n",
      "Epoch 1953, training loss 0.480672, accuracy 0.812500\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 39.12125145991643 minutes\n",
      "Epoch 1954, training loss 0.753218, accuracy 0.703125\n",
      "Validation accuracy 0.742500\n",
      "Time elapsed 39.17123887936274 minutes\n",
      "Epoch 1955, training loss 0.625996, accuracy 0.718750\n",
      "Validation accuracy 0.666250\n",
      "Time elapsed 39.22150832414627 minutes\n",
      "Epoch 1956, training loss 0.554646, accuracy 0.781250\n",
      "Validation accuracy 0.748750\n",
      "Time elapsed 39.271515889962515 minutes\n",
      "Epoch 1957, training loss 0.487033, accuracy 0.796875\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 39.32354669968287 minutes\n",
      "Epoch 1958, training loss 0.405021, accuracy 0.875000\n",
      "Validation accuracy 0.773125\n",
      "Time elapsed 39.380550805727644 minutes\n",
      "Epoch 1959, training loss 0.482010, accuracy 0.765625\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 39.436357748508456 minutes\n",
      "Epoch 1960, training loss 0.316450, accuracy 0.890625\n",
      "Validation accuracy 0.770000\n",
      "Time elapsed 39.48847356239955 minutes\n",
      "Epoch 1961, training loss 0.627700, accuracy 0.687500\n",
      "Validation accuracy 0.756250\n",
      "Time elapsed 39.53939274946848 minutes\n",
      "Epoch 1962, training loss 0.578864, accuracy 0.734375\n",
      "Validation accuracy 0.780000\n",
      "Time elapsed 39.59071838061015 minutes\n",
      "Epoch 1963, training loss 0.593579, accuracy 0.765625\n",
      "Validation accuracy 0.758750\n",
      "Time elapsed 39.64561810890834 minutes\n",
      "Epoch 1964, training loss 0.531579, accuracy 0.781250\n",
      "Validation accuracy 0.773125\n",
      "Time elapsed 39.70383661985397 minutes\n",
      "Epoch 1965, training loss 0.471633, accuracy 0.781250\n",
      "Validation accuracy 0.758125\n",
      "Time elapsed 39.770028471946716 minutes\n",
      "Epoch 1966, training loss 0.495798, accuracy 0.796875\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 39.83124390045802 minutes\n",
      "Epoch 1967, training loss 0.489644, accuracy 0.812500\n",
      "Validation accuracy 0.770000\n",
      "Time elapsed 39.882402102152504 minutes\n",
      "Epoch 1968, training loss 0.413878, accuracy 0.828125\n",
      "Validation accuracy 0.754375\n",
      "Time elapsed 39.9420595963796 minutes\n",
      "Epoch 1969, training loss 0.512057, accuracy 0.875000\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 40.0007173260053 minutes\n",
      "Epoch 1970, training loss 0.596669, accuracy 0.843750\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 40.06170679330826 minutes\n",
      "Epoch 1971, training loss 0.433444, accuracy 0.828125\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 40.12023423910141 minutes\n",
      "Epoch 1972, training loss 0.553267, accuracy 0.781250\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 40.172222928206125 minutes\n",
      "Epoch 1973, training loss 0.639797, accuracy 0.718750\n",
      "Validation accuracy 0.765625\n",
      "Time elapsed 40.22204640309016 minutes\n",
      "Epoch 1974, training loss 0.438404, accuracy 0.843750\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 40.276159099737804 minutes\n",
      "Epoch 1975, training loss 0.483754, accuracy 0.812500\n",
      "Validation accuracy 0.736250\n",
      "Time elapsed 40.334832394123076 minutes\n",
      "Epoch 1976, training loss 0.482601, accuracy 0.843750\n",
      "Validation accuracy 0.770625\n",
      "Time elapsed 40.39494456450144 minutes\n",
      "Epoch 1977, training loss 0.466696, accuracy 0.828125\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 40.4579758365949 minutes\n",
      "Epoch 1978, training loss 0.353629, accuracy 0.859375\n",
      "Validation accuracy 0.752500\n",
      "Time elapsed 40.50789407491684 minutes\n",
      "Epoch 1979, training loss 0.568852, accuracy 0.765625\n",
      "Validation accuracy 0.768750\n",
      "Time elapsed 40.56365784406662 minutes\n",
      "Epoch 1980, training loss 0.651518, accuracy 0.703125\n",
      "Validation accuracy 0.761875\n",
      "Time elapsed 40.62846105098724 minutes\n",
      "Epoch 1981, training loss 0.462240, accuracy 0.859375\n",
      "Validation accuracy 0.768125\n",
      "Time elapsed 40.69507389465968 minutes\n",
      "Epoch 1982, training loss 0.609966, accuracy 0.781250\n",
      "Validation accuracy 0.762500\n",
      "Time elapsed 40.74757527510325 minutes\n",
      "Epoch 1983, training loss 0.609078, accuracy 0.781250\n",
      "Validation accuracy 0.764375\n",
      "Time elapsed 40.80280711253484 minutes\n",
      "Epoch 1984, training loss 0.377144, accuracy 0.843750\n",
      "Validation accuracy 0.770625\n",
      "Time elapsed 40.853292711575826 minutes\n",
      "Epoch 1985, training loss 0.560384, accuracy 0.828125\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 40.91094400882721 minutes\n",
      "Epoch 1986, training loss 0.524642, accuracy 0.765625\n",
      "Validation accuracy 0.765000\n",
      "Time elapsed 40.9653582016627 minutes\n",
      "Epoch 1987, training loss 0.506628, accuracy 0.812500\n",
      "Validation accuracy 0.766875\n",
      "Time elapsed 41.02029685576757 minutes\n",
      "Epoch 1988, training loss 0.524451, accuracy 0.781250\n",
      "Validation accuracy 0.767500\n",
      "Time elapsed 41.07117981513341 minutes\n",
      "Epoch 1989, training loss 0.513303, accuracy 0.781250\n",
      "Validation accuracy 0.740000\n",
      "Time elapsed 41.12037029266357 minutes\n",
      "Epoch 1990, training loss 0.685236, accuracy 0.671875\n",
      "Validation accuracy 0.730625\n",
      "Time elapsed 41.18064664999644 minutes\n",
      "Epoch 1991, training loss 0.714969, accuracy 0.734375\n",
      "Validation accuracy 0.761250\n",
      "Time elapsed 41.23090546925862 minutes\n",
      "Epoch 1992, training loss 0.390973, accuracy 0.828125\n",
      "Validation accuracy 0.763125\n",
      "Time elapsed 41.28188816706339 minutes\n",
      "Epoch 1993, training loss 0.578322, accuracy 0.765625\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 41.329401369889574 minutes\n",
      "Epoch 1994, training loss 0.598142, accuracy 0.765625\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 41.39009305636088 minutes\n",
      "Epoch 1995, training loss 0.478872, accuracy 0.734375\n",
      "Validation accuracy 0.763750\n",
      "Time elapsed 41.46106815338135 minutes\n",
      "Epoch 1996, training loss 0.407334, accuracy 0.828125\n",
      "Validation accuracy 0.773125\n",
      "Time elapsed 41.51827750205994 minutes\n",
      "Epoch 1997, training loss 0.441824, accuracy 0.781250\n",
      "Validation accuracy 0.773750\n",
      "Time elapsed 41.56935589313507 minutes\n",
      "Epoch 1998, training loss 0.467814, accuracy 0.812500\n",
      "Validation accuracy 0.748125\n",
      "Time elapsed 41.61663864453634 minutes\n",
      "Epoch 1999, training loss 0.557600, accuracy 0.750000\n",
      "Validation accuracy 0.766250\n",
      "Time elapsed 41.66460400422414 minutes\n",
      "Testing set accuracy 0.753906\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/trainavg',\n",
    "                                     sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/validationavg')\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#cifar10.reset()\n",
    "print(\"Trainable variables\")\n",
    "for n in tf.trainable_variables():\n",
    "    print(n.name)\n",
    "\n",
    "epochs = 2000\n",
    "mean_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "std_gradients = np.zeros([len(tf.trainable_variables()), epochs])\n",
    "\n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "while cifar10.getEpoch() < epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    # just a training iteration\n",
    "    _ = sess.run(train_step,\n",
    "                feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "    \n",
    "    step = batch_idx+epoch*n_batches\n",
    "    \n",
    "    # Write training summary\n",
    "    if step%50==0:\n",
    "        summary = sess.run(merged,\n",
    "                          feed_dict={\n",
    "                model_input: batch_data,\n",
    "                target: batch_labels,\n",
    "                keep_prob: 0.5 # set to 1.0 at inference time\n",
    "            })\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "    # gradient (by layer) statistics over last training batch & validation summary\n",
    "    if batch_idx==0:\n",
    "        loss, acc, grads = sess.run((cross_entropy, accuracy, grads_vars), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "\n",
    "        for layer in range(len(tf.trainable_variables())):\n",
    "            mean_gradients[layer, epoch] = np.mean(np.abs(grads[layer][0]))\n",
    "            std_gradients[layer, epoch] = np.std(np.abs(grads[layer][0]))\n",
    "        print(\"Epoch %d, training loss %f, accuracy %f\" % (epoch, loss, acc))\n",
    "        \n",
    "        summary, validation_accuracy = validate()\n",
    "        validation_writer.add_summary(summary, step)\n",
    "        print(\"Validation accuracy %f\" % validation_accuracy)\n",
    "        print(\"Time elapsed\", (time.time()-t_i)/60.0, \"minutes\")\n",
    "train_writer.flush()\n",
    "validation_writer.flush()\n",
    "test_acc = test()\n",
    "print(\"Testing set accuracy %f\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzpOLc865xP0P9OkYSAmoYL8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6de8bf49e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDCPj/D9kxm0cXNCQZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6de8ad16d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting gradients\n",
    "n_layers = len(tf.trainable_variables()) // 2\n",
    "x = np.arange(epochs)\n",
    "i = 0\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Weights Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()\n",
    "i = 1\n",
    "plt.figure()\n",
    "while i < n_layers*2:\n",
    "    plt.errorbar(x,mean_gradients[i,:],std_gradients[i,:])\n",
    "    i = i + 2\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Biases Gradient by Layer')\n",
    "plt.legend([\"conv1\",\"conv2\",\"fc1\",\"fc2\"][-n_layers:])\n",
    "plt.xlim(-0.2, epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00033464  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.        ]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(targets, outputs):\n",
    "    '''Returns a confusion matrix. Both targets and outputs\n",
    "    should be 1-D arrays of zeros and ones.'''\n",
    "    encoded_data = 2*targets+outputs  # Map targets and outputs to {0, 1, 2, 3}\n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return ((TP, FP), (FN, TN))\n",
    "\n",
    "def roc_curve(targets, outputs):\n",
    "    '''Returns a ROC curve. Outputs should be in range 0-1\n",
    "    in order to move the threshold.'''\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        outputs_with_threshold = (outputs > threshold).astype(np.float)\n",
    "        ((TP, FP), (FN, TN)) = confusion_matrix(\n",
    "            targets, \n",
    "            outputs_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "    fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "accs = sess.run(model_output,\n",
    "             feed_dict={\n",
    "            model_input: cifar10.test_data,\n",
    "                       target: cifar10.test_labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "matrix = np.zeros([9,9])\n",
    "print(accs[0]/sum(sum(accs)))\n",
    "print(np.argmax(cifar10.test_labels[0]))\n",
    "total = sum(sum(accs))\n",
    "for i in range(len(accs)):\n",
    "    matrix[np.argmax(accs[i]),np.argmax(cifar10.test_labels[i])] += 1\n",
    "    \n",
    "\n",
    "confusion = tf.confusion_matrix(cifar10.test_labels.reshape(-1),accs.reshape(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 169.    0.    1.    1.    0.    3.    0.    0.    3.]\n",
      " [   7.  106.    8.    1.   24.   11.    0.    7.   24.]\n",
      " [   0.    0.   85.    1.   19.    0.    0.    0.    1.]\n",
      " [   0.    1.    1.  191.    5.    0.    0.    0.    3.]\n",
      " [   2.    7.   58.    6.  104.    1.    0.    0.    5.]\n",
      " [  16.   13.    1.    3.    2.  193.    0.    0.   64.]\n",
      " [   0.    0.    4.    0.    1.    0.  182.    0.    0.]\n",
      " [   2.    0.   41.    0.   43.    0.    0.  247.    3.]\n",
      " [   3.    9.    0.    2.    1.   50.    0.    0.   96.]]\n",
      "[ 199.  136.  199.  205.  199.  258.  182.  254.  199.]\n"
     ]
    }
   ],
   "source": [
    "print(matrix)\n",
    "print(sum(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199 136 199 205 199 258 182 254 199]\n",
      "[[ 164.    0.    0.    1.    0.    1.    0.    0.    2.]\n",
      " [   9.  113.    8.    1.   25.   23.    0.   11.   55.]\n",
      " [   0.    0.   75.    1.   19.    0.    0.    0.    1.]\n",
      " [   0.    0.    0.  188.    3.    0.    0.    0.    2.]\n",
      " [   1.    8.   43.    7.   91.    7.    0.    5.    5.]\n",
      " [  20.    7.    2.    3.    3.  172.    0.    0.   53.]\n",
      " [   0.    1.    6.    0.    5.    0.  182.    3.    0.]\n",
      " [   2.    4.   65.    1.   53.    4.    0.  235.    4.]\n",
      " [   3.    3.    0.    3.    0.   51.    0.    0.   77.]]\n"
     ]
    }
   ],
   "source": [
    "print(sum(cifar10.test_labels))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RRL/RRab',\n",
       " 'RRL/RRe',\n",
       " 'EB/ED',\n",
       " 'CEPH/1O',\n",
       " 'EB/ESD',\n",
       " 'RRL/RRd',\n",
       " 'EB/ED_VAR',\n",
       " 'EB/ED_ESD',\n",
       " 'RRL/RRc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
